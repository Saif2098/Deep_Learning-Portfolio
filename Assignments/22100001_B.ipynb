{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlM2iatsQ9nb"
      },
      "source": [
        "# Assignment 2  - Part B\n",
        "\n",
        "\n",
        "#### Roll Number:2022-10-0001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_kQzTmZRTLu"
      },
      "source": [
        "### Task Explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m73XPqSORiIJ"
      },
      "source": [
        "In this task, we will perform feature extraction and fine-tuning on a previously trained over a large dataset model. This is a very useful domain of Deep Learning known as Transfer Learning. We don't always have the luxury of time and computational power to adequately train a large model over our dataset. So, we use models which have been trained on large datasets (usually ImageNet) and then fine-tune them on our dataset. The intuition behind transfer learning for image classification is that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. You can then take advantage of these learned feature maps without having to start from scratch by training a large model on a large dataset.\n",
        "\n",
        "Here, we will fine-tune a previously trained model on imagenet to learn and classify Marvel Superheroes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5dOkrpMXxwA"
      },
      "source": [
        "### Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNFej_smE6As"
      },
      "source": [
        "use this [link](https://drive.google.com/drive/folders/1O3crt73u20vAOK11FU59uvKai9DCggOa?usp=sharing) to add the dataset folder shortcut to your drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpIdfa5ds8Ml",
        "outputId": "63d088e9-1d8b-4867-f170-c3c3ecf35ca3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKNgVC1AtKi6"
      },
      "outputs": [],
      "source": [
        "!unzip '/content/gdrive/MyDrive/Deep Learning/A2/marvel.zip' -d '/content/gdrive/MyDrive/Deep Learning/A2/marvel_images'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay5FPjLZn6_x"
      },
      "source": [
        "### Task 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_484MVejoBJB"
      },
      "source": [
        "the Marvel Superheroes dataset consists of 8 classes, you have to add another class for \"Thor\" \n",
        "\n",
        "you are welcome to scrape images from any online source, make another folder for Thor inside train and val directories.\n",
        "\n",
        "make sure there are different images in the validation and training datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4P4FtYzeMo0C",
        "outputId": "e6b95cd5-f384-49c7-c9a3-499e9cb39813"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of thor images to scrape is approximately 377\n"
          ]
        }
      ],
      "source": [
        "# find the number of images to scrape\n",
        "\n",
        "import glob\n",
        "img_paths = []\n",
        "files = glob.glob(\"/content/gdrive/MyDrive/Deep Learning/A2/marvel_images/marvel/**/hulk/*\", recursive = False)\n",
        "for file in files:\n",
        "  img_paths.append(file)\n",
        "print(\"Number of thor images to scrape is approximately {}\".format(len(img_paths)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XiIJN3ENTAu"
      },
      "source": [
        "Used https://towardsdatascience.com/image-scraping-with-python-a96feda8af2d to scrape 300 thor images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aIvy7yq-dlL"
      },
      "source": [
        "### Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRknlAP8-jO1"
      },
      "source": [
        "In this task we will be downloding pre-trained models and further train them through feature extraction and finetuning on our Marvel dataset.\n",
        "\n",
        "Finally, we will compare these models with a third model trained from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuuKM6L7XvVB"
      },
      "source": [
        "### Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxOeWlzGYZcw"
      },
      "source": [
        "You can use anyone of the following models for this task. See the [documentation](https://pytorch.org/vision/stable/models.html) for more details.\n",
        "- VGG\n",
        "- ResNet\n",
        "- MobileNet\n",
        "- EfficientNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfMOftyqbQbp"
      },
      "source": [
        "### Tutuorial Example\n",
        "\n",
        "You are recommended to follow this [tutorial](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#create-the-optimizer) for this task. All the hints and information required are available here and this task can be easily completed by following it. In this tutorial, they use MobileNet but as mentioned above you are free to use any of the abovementioned models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_POqcmrGQloX",
        "outputId": "d9e0e90c-ce15-4a78-8c7f-27a58e752bc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version:  1.10.0+cu111\n",
            "Torchvision Version:  0.11.1+cu111\n"
          ]
        }
      ],
      "source": [
        "# make any other necessary imports here\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\n",
        "\n",
        "import cv2\n",
        "import glob\n",
        "import re\n",
        "from google.colab.patches import cv2_imshow\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FnuTryHnnIbQ"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0uPopOtAZZp"
      },
      "source": [
        "Define hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "-FvcGUc6c4sB"
      },
      "outputs": [],
      "source": [
        "# define hyperparameters\n",
        "data_dir = \"/content/marvel_dataset/marvel\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 9\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 64\n",
        "\n",
        "# Number of epochs to train for\n",
        "num_epochs = 30\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "#   when True we only update the reshaped layer params i.e  perform feature extraction\n",
        "feature_extract = True\n",
        "\n",
        "input_shape = (224,224)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21nBaKllBLun"
      },
      "source": [
        "### Data Load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idL_FIkrBeLC"
      },
      "source": [
        "Load the data and initialize dataloaders here.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP2AsNi04qgO"
      },
      "source": [
        "##### Extract the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J26dqCemDV7C",
        "outputId": "3a57d409-f813-4c51-e33f-31cf55cb2383"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The labels are: {'thanos': 0, 'black widow': 1, 'captain america': 2, 'doctor strange': 3, 'hulk': 4, 'ironman': 5, 'loki': 6, 'spider-man': 7, 'thor': 8}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Labels \n",
        "my_labels={}\n",
        "files = glob.glob(\"/content/gdrive/MyDrive/Deep Learning/A2/marvel_images/marvel/train/*\", recursive = False)\n",
        "for i, file in enumerate(files):\n",
        "  label = re.split(\"/\", file)[9]\n",
        "  my_labels[label] = i\n",
        "print(\"The labels are:\", my_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCbCyCAL404_"
      },
      "source": [
        "##### Find the paths for training and validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dmt3s4m2i3vw",
        "outputId": "31ddb89f-f20d-4fa2-dd20-1d65e5e4fdcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of training paths 2789\n"
          ]
        }
      ],
      "source": [
        "training_paths = []\n",
        "files = glob.glob(\"/content/gdrive/MyDrive/Deep Learning/A2/marvel_images/marvel/train/**/*\", recursive = False)\n",
        "for file in files:\n",
        "  training_paths.append(file)\n",
        "print(\"Length of training paths {}\".format(len(training_paths)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6SnYRUvjKA5",
        "outputId": "176853c0-cf39-452f-f308-bf365982156a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of training paths 520\n"
          ]
        }
      ],
      "source": [
        "valid_paths = []\n",
        "files = glob.glob(\"/content/gdrive/MyDrive/Deep Learning/A2/marvel_images/marvel/valid/**/*\", recursive = False)\n",
        "for file in files:\n",
        "  valid_paths.append(file)\n",
        "print(\"Length of training paths {}\".format(len(valid_paths)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faxISkRp4_ir"
      },
      "source": [
        "##### Data Generator Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7gFoRUIdhN0w"
      },
      "outputs": [],
      "source": [
        "def preprocessing_norm(images):\n",
        "    images = images / 255\n",
        "    return images\n",
        "\n",
        "# Data generator function \n",
        "def data_generator(img_path_list):\n",
        "  \n",
        "  return_list = []\n",
        "  num_samples = len(img_path_list)\n",
        "  \n",
        "  for img_path in img_path_list:\n",
        " \n",
        "    im_cv = cv2.imread(img_path)\n",
        "    im_cv = im_cv.astype(np.float32)\n",
        "    im_cv = cv2.cvtColor(im_cv, cv2.COLOR_BGR2RGB) # convert to RGB\n",
        "    im_cv = cv2.resize(im_cv, input_shape)\n",
        "    im_cv = np.transpose(preprocessing_norm(im_cv))\n",
        "    \n",
        "    label = re.split(\"/\", img_path)[9]\n",
        "    label = my_labels[label] # convert to numeric label\n",
        "\n",
        "    return_list.append((im_cv, label))\n",
        "    \n",
        "  return return_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMGkj5RI5LAS"
      },
      "source": [
        "##### Create training and validation loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0QH-uPmIlxq6"
      },
      "outputs": [],
      "source": [
        "# create a training loader\n",
        "training_loader = DataLoader(data_generator(training_paths), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# create a validation loader\n",
        "validation_loader = DataLoader(data_generator(valid_paths), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# create dictionary\n",
        "dataloaders_dict = {'train': training_loader, 'val': validation_loader}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziqVSbfTDn7S"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHKnZp0cNUZw"
      },
      "source": [
        "Define the train_model function here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZGs4cstCEkhx"
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output. In train\n",
        "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                    #   but in testing we only consider the final output.\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQUOvUfSNrP-"
      },
      "source": [
        "next, define the set_parameter_requires_grad() function.\n",
        "\n",
        "When performing feature extracting, we do not update the model's weights from prior training. We only update the new top layer added.\n",
        "\n",
        "When performing finetuning, we update all the weights for all the layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "vEAZQJsoEU28"
      },
      "outputs": [],
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdIqYIsdOd62"
      },
      "source": [
        "Define the initialize_model function here, you do not need to include all the categories , simply include the one or more relevant to the model you are using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "UHtbeczWAU53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d2a84bf-570b-42a1-af21-f01134aa6b7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=9, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "\n",
        "    return model_ft, input_size\n",
        "\n",
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(\"resnet\", num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "# Print the model we just instantiated\n",
        "print(model_ft)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWsN7rEOPSAx"
      },
      "source": [
        "### Creating Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK6povE-xd4u"
      },
      "source": [
        "We have to send the model to our device, which could be the GPU or CPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "tsmP0js4xTvD"
      },
      "outputs": [],
      "source": [
        "# Send the model to GPU\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#Code here\n",
        "model_ft = model_ft.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C23FYAM5PUKI"
      },
      "source": [
        "\n",
        "We also have to define an optimizer which updates only relevant parameters.\n",
        "For instance, when performing **feature extraction**, recall that only top new layer(s) are to be updated. So you'll only see `fc.weight` and `fc.bias` as layers to be updated.\n",
        "\n",
        "When performing **finetuning**, all the layers will be updated so you'll see a long list of layers printed out if `feature_extract=None`\n",
        "\n",
        " this piece of code from the referenced tutorial will help make sure that only relevant layers have `.requires_grad=True` "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpNW0yTOEwCO",
        "outputId": "f205c5b3-7045-41bf-d85f-a1de55fc46b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params to learn:\n",
            "feature extraction\n",
            "\t fc.weight\n",
            "\t fc.bias\n"
          ]
        }
      ],
      "source": [
        "\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    print('feature extraction')\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    print('finetuning ')\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "2-GnxYOcxqEx"
      },
      "outputs": [],
      "source": [
        "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
        "# Observe that all parameters are being optimized\n",
        "#Code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRqlh8WLQ86C"
      },
      "source": [
        "### Training the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ynfgemwQ_cJ"
      },
      "source": [
        "Now we use the function defined above tp train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6L7cApWRD5o",
        "outputId": "217a3889-6094-4675-8162-01e2ccf89822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/29\n",
            "----------\n",
            "train Loss: 1.9344 Acc: 0.3105\n",
            "val Loss: 1.5345 Acc: 0.5192\n",
            "\n",
            "Epoch 1/29\n",
            "----------\n",
            "train Loss: 1.4976 Acc: 0.5048\n",
            "val Loss: 1.3806 Acc: 0.5635\n",
            "\n",
            "Epoch 2/29\n",
            "----------\n",
            "train Loss: 1.3694 Acc: 0.5507\n",
            "val Loss: 1.3146 Acc: 0.5712\n",
            "\n",
            "Epoch 3/29\n",
            "----------\n",
            "train Loss: 1.2779 Acc: 0.5780\n",
            "val Loss: 1.2756 Acc: 0.5788\n",
            "\n",
            "Epoch 4/29\n",
            "----------\n",
            "train Loss: 1.2310 Acc: 0.5859\n",
            "val Loss: 1.2556 Acc: 0.5750\n",
            "\n",
            "Epoch 5/29\n",
            "----------\n",
            "train Loss: 1.2113 Acc: 0.5973\n",
            "val Loss: 1.2576 Acc: 0.5904\n",
            "\n",
            "Epoch 6/29\n",
            "----------\n",
            "train Loss: 1.1635 Acc: 0.6210\n",
            "val Loss: 1.2532 Acc: 0.5731\n",
            "\n",
            "Epoch 7/29\n",
            "----------\n",
            "train Loss: 1.1615 Acc: 0.6210\n",
            "val Loss: 1.2644 Acc: 0.5981\n",
            "\n",
            "Epoch 8/29\n",
            "----------\n",
            "train Loss: 1.1140 Acc: 0.6307\n",
            "val Loss: 1.2616 Acc: 0.5923\n",
            "\n",
            "Epoch 9/29\n",
            "----------\n",
            "train Loss: 1.1205 Acc: 0.6285\n",
            "val Loss: 1.2351 Acc: 0.5981\n",
            "\n",
            "Epoch 10/29\n",
            "----------\n",
            "train Loss: 1.0885 Acc: 0.6361\n",
            "val Loss: 1.2449 Acc: 0.5923\n",
            "\n",
            "Epoch 11/29\n",
            "----------\n",
            "train Loss: 1.0805 Acc: 0.6346\n",
            "val Loss: 1.2314 Acc: 0.5923\n",
            "\n",
            "Epoch 12/29\n",
            "----------\n",
            "train Loss: 1.0872 Acc: 0.6418\n",
            "val Loss: 1.2670 Acc: 0.6019\n",
            "\n",
            "Epoch 13/29\n",
            "----------\n",
            "train Loss: 1.0766 Acc: 0.6518\n",
            "val Loss: 1.2489 Acc: 0.5865\n",
            "\n",
            "Epoch 14/29\n",
            "----------\n",
            "train Loss: 1.0627 Acc: 0.6407\n",
            "val Loss: 1.2352 Acc: 0.5885\n",
            "\n",
            "Epoch 15/29\n",
            "----------\n",
            "train Loss: 1.0503 Acc: 0.6397\n",
            "val Loss: 1.2616 Acc: 0.5904\n",
            "\n",
            "Epoch 16/29\n",
            "----------\n",
            "train Loss: 1.0462 Acc: 0.6508\n",
            "val Loss: 1.2388 Acc: 0.6038\n",
            "\n",
            "Epoch 17/29\n",
            "----------\n",
            "train Loss: 1.0297 Acc: 0.6486\n",
            "val Loss: 1.2802 Acc: 0.5808\n",
            "\n",
            "Epoch 18/29\n",
            "----------\n",
            "train Loss: 1.0132 Acc: 0.6572\n",
            "val Loss: 1.2598 Acc: 0.5885\n",
            "\n",
            "Epoch 19/29\n",
            "----------\n",
            "train Loss: 1.0101 Acc: 0.6662\n",
            "val Loss: 1.2527 Acc: 0.5885\n",
            "\n",
            "Epoch 20/29\n",
            "----------\n",
            "train Loss: 1.0195 Acc: 0.6579\n",
            "val Loss: 1.2424 Acc: 0.5962\n",
            "\n",
            "Epoch 21/29\n",
            "----------\n",
            "train Loss: 1.0043 Acc: 0.6669\n",
            "val Loss: 1.2605 Acc: 0.5923\n",
            "\n",
            "Epoch 22/29\n",
            "----------\n",
            "train Loss: 1.0080 Acc: 0.6662\n",
            "val Loss: 1.2317 Acc: 0.6058\n",
            "\n",
            "Epoch 23/29\n",
            "----------\n",
            "train Loss: 1.0048 Acc: 0.6665\n",
            "val Loss: 1.2646 Acc: 0.5942\n",
            "\n",
            "Epoch 24/29\n",
            "----------\n",
            "train Loss: 0.9810 Acc: 0.6676\n",
            "val Loss: 1.2542 Acc: 0.5904\n",
            "\n",
            "Epoch 25/29\n",
            "----------\n",
            "train Loss: 0.9868 Acc: 0.6665\n",
            "val Loss: 1.2624 Acc: 0.5808\n",
            "\n",
            "Epoch 26/29\n",
            "----------\n",
            "train Loss: 0.9642 Acc: 0.6805\n",
            "val Loss: 1.2549 Acc: 0.5885\n",
            "\n",
            "Epoch 27/29\n",
            "----------\n",
            "train Loss: 0.9999 Acc: 0.6651\n",
            "val Loss: 1.2577 Acc: 0.5904\n",
            "\n",
            "Epoch 28/29\n",
            "----------\n",
            "train Loss: 0.9812 Acc: 0.6698\n",
            "val Loss: 1.2869 Acc: 0.5981\n",
            "\n",
            "Epoch 29/29\n",
            "----------\n",
            "train Loss: 0.9709 Acc: 0.6644\n",
            "val Loss: 1.3089 Acc: 0.5788\n",
            "\n",
            "Training complete in 4m 21s\n",
            "Best val Acc: 0.605769\n"
          ]
        }
      ],
      "source": [
        "# Setup the loss fxn\n",
        "#Code here\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate\n",
        "#Code here\n",
        "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHVA-aKtRW3K"
      },
      "source": [
        "### Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8OIEWauRamV"
      },
      "source": [
        "We will now perform finetuning, let's initialize a new model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQ3BC59yR1p5",
        "outputId": "45603b1f-6cac-43d3-b81f-cf07d5d6948d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=9, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#Code here\n",
        "# Initialize the model for this run\n",
        "feature_extract=False\n",
        "model_ft2, input_size = initialize_model(\"resnet\", num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "# Print the model we just instantiated\n",
        "print(model_ft2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-Y_JmJK5yZZm"
      },
      "outputs": [],
      "source": [
        "# Send the model to GPU\n",
        "#Code here\n",
        "model_ft2 = model_ft2.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5frEFuwSMWJ"
      },
      "source": [
        "Now set `feature_extraction=False`, and run this code again, you should see a long list of layers printed.\n",
        "\n",
        "Recall the difference between finetuning and feature extraction\n",
        "\n",
        "\n",
        "tip: set the learning rate much lower for finetuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "eJvftJGVStbb"
      },
      "outputs": [],
      "source": [
        "feature_extract=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0oMLdyOSlnf",
        "outputId": "2f6901cf-bcd7-4e61-9262-be467a61ab78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params to learn:\n",
            "finetuning \n",
            "\t conv1.weight\n",
            "\t bn1.weight\n",
            "\t bn1.bias\n",
            "\t layer1.0.conv1.weight\n",
            "\t layer1.0.bn1.weight\n",
            "\t layer1.0.bn1.bias\n",
            "\t layer1.0.conv2.weight\n",
            "\t layer1.0.bn2.weight\n",
            "\t layer1.0.bn2.bias\n",
            "\t layer1.1.conv1.weight\n",
            "\t layer1.1.bn1.weight\n",
            "\t layer1.1.bn1.bias\n",
            "\t layer1.1.conv2.weight\n",
            "\t layer1.1.bn2.weight\n",
            "\t layer1.1.bn2.bias\n",
            "\t layer2.0.conv1.weight\n",
            "\t layer2.0.bn1.weight\n",
            "\t layer2.0.bn1.bias\n",
            "\t layer2.0.conv2.weight\n",
            "\t layer2.0.bn2.weight\n",
            "\t layer2.0.bn2.bias\n",
            "\t layer2.0.downsample.0.weight\n",
            "\t layer2.0.downsample.1.weight\n",
            "\t layer2.0.downsample.1.bias\n",
            "\t layer2.1.conv1.weight\n",
            "\t layer2.1.bn1.weight\n",
            "\t layer2.1.bn1.bias\n",
            "\t layer2.1.conv2.weight\n",
            "\t layer2.1.bn2.weight\n",
            "\t layer2.1.bn2.bias\n",
            "\t layer3.0.conv1.weight\n",
            "\t layer3.0.bn1.weight\n",
            "\t layer3.0.bn1.bias\n",
            "\t layer3.0.conv2.weight\n",
            "\t layer3.0.bn2.weight\n",
            "\t layer3.0.bn2.bias\n",
            "\t layer3.0.downsample.0.weight\n",
            "\t layer3.0.downsample.1.weight\n",
            "\t layer3.0.downsample.1.bias\n",
            "\t layer3.1.conv1.weight\n",
            "\t layer3.1.bn1.weight\n",
            "\t layer3.1.bn1.bias\n",
            "\t layer3.1.conv2.weight\n",
            "\t layer3.1.bn2.weight\n",
            "\t layer3.1.bn2.bias\n",
            "\t layer4.0.conv1.weight\n",
            "\t layer4.0.bn1.weight\n",
            "\t layer4.0.bn1.bias\n",
            "\t layer4.0.conv2.weight\n",
            "\t layer4.0.bn2.weight\n",
            "\t layer4.0.bn2.bias\n",
            "\t layer4.0.downsample.0.weight\n",
            "\t layer4.0.downsample.1.weight\n",
            "\t layer4.0.downsample.1.bias\n",
            "\t layer4.1.conv1.weight\n",
            "\t layer4.1.bn1.weight\n",
            "\t layer4.1.bn1.bias\n",
            "\t layer4.1.conv2.weight\n",
            "\t layer4.1.bn2.weight\n",
            "\t layer4.1.bn2.bias\n",
            "\t fc.weight\n",
            "\t fc.bias\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "params_to_update = model_ft2.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    print('feature extraction')\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft2.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    print('finetuning ')\n",
        "    for name,param in model_ft2.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKyHB6dtTD85"
      },
      "source": [
        "now repeat the above process to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GggukfATVPF",
        "outputId": "f9ec4421-9ccf-4511-d092-a0b274d87fc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/29\n",
            "----------\n",
            "train Loss: 1.6858 Acc: 0.4059\n",
            "val Loss: 1.1907 Acc: 0.5904\n",
            "\n",
            "Epoch 1/29\n",
            "----------\n",
            "train Loss: 0.9511 Acc: 0.6942\n",
            "val Loss: 1.0875 Acc: 0.6481\n",
            "\n",
            "Epoch 2/29\n",
            "----------\n",
            "train Loss: 0.5847 Acc: 0.8211\n",
            "val Loss: 1.1626 Acc: 0.6442\n",
            "\n",
            "Epoch 3/29\n",
            "----------\n",
            "train Loss: 0.3527 Acc: 0.9043\n",
            "val Loss: 1.2030 Acc: 0.6692\n",
            "\n",
            "Epoch 4/29\n",
            "----------\n",
            "train Loss: 0.2202 Acc: 0.9484\n",
            "val Loss: 1.2220 Acc: 0.6788\n",
            "\n",
            "Epoch 5/29\n",
            "----------\n",
            "train Loss: 0.1617 Acc: 0.9624\n",
            "val Loss: 1.2658 Acc: 0.6673\n",
            "\n",
            "Epoch 6/29\n",
            "----------\n",
            "train Loss: 0.1264 Acc: 0.9688\n",
            "val Loss: 1.2798 Acc: 0.6885\n",
            "\n",
            "Epoch 7/29\n",
            "----------\n",
            "train Loss: 0.1056 Acc: 0.9702\n",
            "val Loss: 1.3024 Acc: 0.6827\n",
            "\n",
            "Epoch 8/29\n",
            "----------\n",
            "train Loss: 0.0974 Acc: 0.9720\n",
            "val Loss: 1.3047 Acc: 0.6846\n",
            "\n",
            "Epoch 9/29\n",
            "----------\n",
            "train Loss: 0.0878 Acc: 0.9745\n",
            "val Loss: 1.3295 Acc: 0.6923\n",
            "\n",
            "Epoch 10/29\n",
            "----------\n",
            "train Loss: 0.0796 Acc: 0.9738\n",
            "val Loss: 1.3499 Acc: 0.6731\n",
            "\n",
            "Epoch 11/29\n",
            "----------\n",
            "train Loss: 0.0773 Acc: 0.9760\n",
            "val Loss: 1.3104 Acc: 0.6885\n",
            "\n",
            "Epoch 12/29\n",
            "----------\n",
            "train Loss: 0.0643 Acc: 0.9753\n",
            "val Loss: 1.3699 Acc: 0.6923\n",
            "\n",
            "Epoch 13/29\n",
            "----------\n",
            "train Loss: 0.0668 Acc: 0.9738\n",
            "val Loss: 1.3530 Acc: 0.6904\n",
            "\n",
            "Epoch 14/29\n",
            "----------\n",
            "train Loss: 0.0608 Acc: 0.9767\n",
            "val Loss: 1.4365 Acc: 0.6904\n",
            "\n",
            "Epoch 15/29\n",
            "----------\n",
            "train Loss: 0.0569 Acc: 0.9763\n",
            "val Loss: 1.4144 Acc: 0.6981\n",
            "\n",
            "Epoch 16/29\n",
            "----------\n",
            "train Loss: 0.0565 Acc: 0.9760\n",
            "val Loss: 1.4386 Acc: 0.6942\n",
            "\n",
            "Epoch 17/29\n",
            "----------\n",
            "train Loss: 0.0574 Acc: 0.9756\n",
            "val Loss: 1.4448 Acc: 0.7058\n",
            "\n",
            "Epoch 18/29\n",
            "----------\n",
            "train Loss: 0.0533 Acc: 0.9742\n",
            "val Loss: 1.4289 Acc: 0.7000\n",
            "\n",
            "Epoch 19/29\n",
            "----------\n",
            "train Loss: 0.0570 Acc: 0.9763\n",
            "val Loss: 1.4739 Acc: 0.6846\n",
            "\n",
            "Epoch 20/29\n",
            "----------\n",
            "train Loss: 0.0513 Acc: 0.9753\n",
            "val Loss: 1.4549 Acc: 0.6865\n",
            "\n",
            "Epoch 21/29\n",
            "----------\n",
            "train Loss: 0.0499 Acc: 0.9749\n",
            "val Loss: 1.5350 Acc: 0.6596\n",
            "\n",
            "Epoch 22/29\n",
            "----------\n",
            "train Loss: 0.0508 Acc: 0.9749\n",
            "val Loss: 1.5031 Acc: 0.6731\n",
            "\n",
            "Epoch 23/29\n",
            "----------\n",
            "train Loss: 0.0473 Acc: 0.9753\n",
            "val Loss: 1.5110 Acc: 0.6904\n",
            "\n",
            "Epoch 24/29\n",
            "----------\n",
            "train Loss: 0.0419 Acc: 0.9781\n",
            "val Loss: 1.5301 Acc: 0.6846\n",
            "\n",
            "Epoch 25/29\n",
            "----------\n",
            "train Loss: 0.0434 Acc: 0.9760\n",
            "val Loss: 1.5285 Acc: 0.7019\n",
            "\n",
            "Epoch 26/29\n",
            "----------\n",
            "train Loss: 0.0453 Acc: 0.9753\n",
            "val Loss: 1.5253 Acc: 0.6846\n",
            "\n",
            "Epoch 27/29\n",
            "----------\n",
            "train Loss: 0.0463 Acc: 0.9753\n",
            "val Loss: 1.5449 Acc: 0.6885\n",
            "\n",
            "Epoch 28/29\n",
            "----------\n",
            "train Loss: 0.0426 Acc: 0.9774\n",
            "val Loss: 1.5288 Acc: 0.6846\n",
            "\n",
            "Epoch 29/29\n",
            "----------\n",
            "train Loss: 0.0420 Acc: 0.9763\n",
            "val Loss: 1.5933 Acc: 0.6942\n",
            "\n",
            "Training complete in 11m 20s\n",
            "Best val Acc: 0.705769\n"
          ]
        }
      ],
      "source": [
        "#Code here\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
        "model_ft2, hist2 = train_model(model_ft2, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50iMjgPkUSe8"
      },
      "source": [
        "### Training from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8_edbnPUWqB"
      },
      "source": [
        "Train a third model from scratch, similar as above.\n",
        "\n",
        "\n",
        "We won't have to worry about updating layers, simply set `use_pretrained=False` in initialize_model function, send model to device, define optimizer and loss and run training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-v_8ai1TzsQ",
        "outputId": "6729c6e2-1a49-49ea-8bb1-f82fadaa41aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=9, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#Code here\n",
        "# Initialize the model for this run\n",
        "feature_extract=False\n",
        "model_ft3, input_size = initialize_model(\"resnet\", num_classes, feature_extract, use_pretrained=False)\n",
        "\n",
        "# Print the model we just instantiated\n",
        "print(model_ft3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params_to_update = model_ft3.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    print('feature extraction')\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft2.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    print('finetuning ')\n",
        "    for name,param in model_ft2.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev7i9ffkCON2",
        "outputId": "2d905f41-6ee1-4c10-ec75-721ca66319f8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params to learn:\n",
            "finetuning \n",
            "\t conv1.weight\n",
            "\t bn1.weight\n",
            "\t bn1.bias\n",
            "\t layer1.0.conv1.weight\n",
            "\t layer1.0.bn1.weight\n",
            "\t layer1.0.bn1.bias\n",
            "\t layer1.0.conv2.weight\n",
            "\t layer1.0.bn2.weight\n",
            "\t layer1.0.bn2.bias\n",
            "\t layer1.1.conv1.weight\n",
            "\t layer1.1.bn1.weight\n",
            "\t layer1.1.bn1.bias\n",
            "\t layer1.1.conv2.weight\n",
            "\t layer1.1.bn2.weight\n",
            "\t layer1.1.bn2.bias\n",
            "\t layer2.0.conv1.weight\n",
            "\t layer2.0.bn1.weight\n",
            "\t layer2.0.bn1.bias\n",
            "\t layer2.0.conv2.weight\n",
            "\t layer2.0.bn2.weight\n",
            "\t layer2.0.bn2.bias\n",
            "\t layer2.0.downsample.0.weight\n",
            "\t layer2.0.downsample.1.weight\n",
            "\t layer2.0.downsample.1.bias\n",
            "\t layer2.1.conv1.weight\n",
            "\t layer2.1.bn1.weight\n",
            "\t layer2.1.bn1.bias\n",
            "\t layer2.1.conv2.weight\n",
            "\t layer2.1.bn2.weight\n",
            "\t layer2.1.bn2.bias\n",
            "\t layer3.0.conv1.weight\n",
            "\t layer3.0.bn1.weight\n",
            "\t layer3.0.bn1.bias\n",
            "\t layer3.0.conv2.weight\n",
            "\t layer3.0.bn2.weight\n",
            "\t layer3.0.bn2.bias\n",
            "\t layer3.0.downsample.0.weight\n",
            "\t layer3.0.downsample.1.weight\n",
            "\t layer3.0.downsample.1.bias\n",
            "\t layer3.1.conv1.weight\n",
            "\t layer3.1.bn1.weight\n",
            "\t layer3.1.bn1.bias\n",
            "\t layer3.1.conv2.weight\n",
            "\t layer3.1.bn2.weight\n",
            "\t layer3.1.bn2.bias\n",
            "\t layer4.0.conv1.weight\n",
            "\t layer4.0.bn1.weight\n",
            "\t layer4.0.bn1.bias\n",
            "\t layer4.0.conv2.weight\n",
            "\t layer4.0.bn2.weight\n",
            "\t layer4.0.bn2.bias\n",
            "\t layer4.0.downsample.0.weight\n",
            "\t layer4.0.downsample.1.weight\n",
            "\t layer4.0.downsample.1.bias\n",
            "\t layer4.1.conv1.weight\n",
            "\t layer4.1.bn1.weight\n",
            "\t layer4.1.bn1.bias\n",
            "\t layer4.1.conv2.weight\n",
            "\t layer4.1.bn2.weight\n",
            "\t layer4.1.bn2.bias\n",
            "\t fc.weight\n",
            "\t fc.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Send the model to GPU\n",
        "model_ft3 = model_ft3.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
        "model_ft3, hist3 = train_model(model_ft3, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DnUlz0gCCzf",
        "outputId": "e393fc6d-f013-407b-d20a-e00d5097bc72"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/29\n",
            "----------\n",
            "train Loss: 2.1673 Acc: 0.1599\n",
            "val Loss: 2.1398 Acc: 0.2058\n",
            "\n",
            "Epoch 1/29\n",
            "----------\n",
            "train Loss: 2.0509 Acc: 0.2363\n",
            "val Loss: 2.1651 Acc: 0.1923\n",
            "\n",
            "Epoch 2/29\n",
            "----------\n",
            "train Loss: 1.9513 Acc: 0.2729\n",
            "val Loss: 1.9522 Acc: 0.2846\n",
            "\n",
            "Epoch 3/29\n",
            "----------\n",
            "train Loss: 1.8851 Acc: 0.3076\n",
            "val Loss: 1.9981 Acc: 0.2846\n",
            "\n",
            "Epoch 4/29\n",
            "----------\n",
            "train Loss: 1.7950 Acc: 0.3564\n",
            "val Loss: 1.9451 Acc: 0.3250\n",
            "\n",
            "Epoch 5/29\n",
            "----------\n",
            "train Loss: 1.7369 Acc: 0.3733\n",
            "val Loss: 1.8493 Acc: 0.3462\n",
            "\n",
            "Epoch 6/29\n",
            "----------\n",
            "train Loss: 1.6773 Acc: 0.4087\n",
            "val Loss: 1.9162 Acc: 0.3154\n",
            "\n",
            "Epoch 7/29\n",
            "----------\n",
            "train Loss: 1.5866 Acc: 0.4493\n",
            "val Loss: 1.7684 Acc: 0.3769\n",
            "\n",
            "Epoch 8/29\n",
            "----------\n",
            "train Loss: 1.4972 Acc: 0.4808\n",
            "val Loss: 2.1461 Acc: 0.3115\n",
            "\n",
            "Epoch 9/29\n",
            "----------\n",
            "train Loss: 1.4209 Acc: 0.5041\n",
            "val Loss: 1.8385 Acc: 0.3558\n",
            "\n",
            "Epoch 10/29\n",
            "----------\n",
            "train Loss: 1.2846 Acc: 0.5579\n",
            "val Loss: 2.1462 Acc: 0.3269\n",
            "\n",
            "Epoch 11/29\n",
            "----------\n",
            "train Loss: 1.1404 Acc: 0.6260\n",
            "val Loss: 2.0715 Acc: 0.3692\n",
            "\n",
            "Epoch 12/29\n",
            "----------\n",
            "train Loss: 1.0082 Acc: 0.6597\n",
            "val Loss: 2.1196 Acc: 0.3462\n",
            "\n",
            "Epoch 13/29\n",
            "----------\n",
            "train Loss: 0.8628 Acc: 0.7221\n",
            "val Loss: 2.3027 Acc: 0.3558\n",
            "\n",
            "Epoch 14/29\n",
            "----------\n",
            "train Loss: 0.7129 Acc: 0.7806\n",
            "val Loss: 2.5185 Acc: 0.3558\n",
            "\n",
            "Epoch 15/29\n",
            "----------\n",
            "train Loss: 0.6372 Acc: 0.8003\n",
            "val Loss: 2.1532 Acc: 0.3827\n",
            "\n",
            "Epoch 16/29\n",
            "----------\n",
            "train Loss: 0.5065 Acc: 0.8534\n",
            "val Loss: 2.3201 Acc: 0.3596\n",
            "\n",
            "Epoch 17/29\n",
            "----------\n",
            "train Loss: 0.4209 Acc: 0.8824\n",
            "val Loss: 2.7834 Acc: 0.3000\n",
            "\n",
            "Epoch 18/29\n",
            "----------\n",
            "train Loss: 0.3727 Acc: 0.8978\n",
            "val Loss: 2.4585 Acc: 0.3827\n",
            "\n",
            "Epoch 19/29\n",
            "----------\n",
            "train Loss: 0.2898 Acc: 0.9258\n",
            "val Loss: 2.3772 Acc: 0.3885\n",
            "\n",
            "Epoch 20/29\n",
            "----------\n",
            "train Loss: 0.3139 Acc: 0.9157\n",
            "val Loss: 2.4060 Acc: 0.3923\n",
            "\n",
            "Epoch 21/29\n",
            "----------\n",
            "train Loss: 0.2464 Acc: 0.9405\n",
            "val Loss: 2.8165 Acc: 0.3538\n",
            "\n",
            "Epoch 22/29\n",
            "----------\n",
            "train Loss: 0.2553 Acc: 0.9387\n",
            "val Loss: 2.7023 Acc: 0.3654\n",
            "\n",
            "Epoch 23/29\n",
            "----------\n",
            "train Loss: 0.2234 Acc: 0.9380\n",
            "val Loss: 2.5826 Acc: 0.3942\n",
            "\n",
            "Epoch 24/29\n",
            "----------\n",
            "train Loss: 0.2465 Acc: 0.9358\n",
            "val Loss: 2.6678 Acc: 0.3750\n",
            "\n",
            "Epoch 25/29\n",
            "----------\n",
            "train Loss: 0.2019 Acc: 0.9516\n",
            "val Loss: 2.5354 Acc: 0.3981\n",
            "\n",
            "Epoch 26/29\n",
            "----------\n",
            "train Loss: 0.2030 Acc: 0.9530\n",
            "val Loss: 3.1523 Acc: 0.3269\n",
            "\n",
            "Epoch 27/29\n",
            "----------\n",
            "train Loss: 0.1761 Acc: 0.9627\n",
            "val Loss: 2.8183 Acc: 0.3423\n",
            "\n",
            "Epoch 28/29\n",
            "----------\n",
            "train Loss: 0.1706 Acc: 0.9588\n",
            "val Loss: 2.5987 Acc: 0.3808\n",
            "\n",
            "Epoch 29/29\n",
            "----------\n",
            "train Loss: 0.1396 Acc: 0.9674\n",
            "val Loss: 2.4719 Acc: 0.3846\n",
            "\n",
            "Training complete in 11m 19s\n",
            "Best val Acc: 0.398077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmA_IR-5zLfE"
      },
      "source": [
        "### Comparing models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTzpVkf0zOde"
      },
      "source": [
        "using data from all three models you have trained, plot a graph comparing validation Accuracy vs. Number of Epochs for your models for performance comparison."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss Plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plot\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.plot(range(num_epochs), [x.cpu() for x in hist], linewidth=2.0, label=\"Feature Extract\")\n",
        "ax.plot(range(num_epochs), [x.cpu() for x in hist2], linewidth=2.0, label=\"Fine Tuning\")\n",
        "ax.plot(range(num_epochs), [x.cpu() for x in hist3], linewidth=2.0, label=\"Use pre-trained = False\")\n",
        "ax.set_xlabel('Epochs')\n",
        "ax.set_ylabel('Validation accuracy')\n",
        "ax.set_title('Validation accuracy vs epochs for different models')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "cqjZKpluHOW5",
        "outputId": "9ba06671-f94a-4f78-ed76-fe883b666fb3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "p_Xrku9gV2hj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epKpMl-CV9ZQ"
      },
      "source": [
        "Question: Which of the three trained models performed best in terms of validation accuracy?\n",
        "Explain why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14HQHUufWMet"
      },
      "source": [
        "Answer: Fine tuning performed the best in terms of validation accuracy. The pretrained model parameters were being updated based on the new marvel classification dataset. This made the model more specific to this dataset thereby improving the validation accuracy. The model that performed worst was the one where no pre-trained weights were imported. Essentially, in this case, no transfer learning was used."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "22100001_B.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}