{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Name: Saif Ur Rahman\n",
        "## Roll Number: 2022-10-0001"
      ],
      "metadata": {
        "id": "g0WnANjoDGTF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM1cxLfnctDV"
      },
      "source": [
        "#knowledge Distillation \n",
        "In this assignment we will impliment [TCN](https://www.bmvc2021-virtualconference.com/conference/papers/paper_0831.html) paper. It is a varient of knowledge distillation which uses dense feature vactors instead of logits to transfer knowledge from teacher to student.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOqQ-lRNesRP"
      },
      "source": [
        "# Training base teacher network\n",
        "This section is not graded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "83fd4851e4ab407281e0da49be9acf05",
            "b39e20a0010a41f9a75c59c1e18c885a",
            "306d6e54b9ce41dd997fb43c4a7d782b",
            "982ea5ce6c3c465faca21c14ceb4796f",
            "d7c4edd1f29e46d494369e8c94804163",
            "f0c90c561e1e4a46b936989fb9cb3f70",
            "909ff3d5f7ec441daf830e2a247e5ce2",
            "e0c57949b101443e8f8d211e17a111c9",
            "c820ba4ff0cb44e296e227b4664687f8",
            "7f5dd42c07b0486d8b4f765e2a4e212d",
            "e206c15e80df4c5cb5e2fac5708c9d5d"
          ]
        },
        "id": "x8H1PHYKE8kJ",
        "outputId": "c78ec61d-9225-4d93-afc7-a9298fee1a87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83fd4851e4ab407281e0da49be9acf05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from torch.autograd import Variable\n",
        "import tqdm\n",
        "\n",
        "batch_size = 100\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Up5ob6ujFKex"
      },
      "outputs": [],
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "teacher = VGG('VGG16')\n",
        "teacher = teacher.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2wyVCEgqFxxo"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(teacher.parameters(), lr=0.0001)\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    teacher.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        inputs = Variable(inputs, requires_grad=False)\n",
        "        targets = Variable(targets)\n",
        "        teacher.zero_grad()\n",
        "        outputs = teacher(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if(batch_idx % 200 == 0):\n",
        "          print(\"Accuracy : \",100.*correct/total,\" Loss : \", train_loss/(batch_idx+1))\n",
        "def test(epoch):\n",
        "    teacher.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = teacher(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\"Accuracy : \",100.*correct/total,\" Loss : \", test_loss/(batch_idx+1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xfNOBETGFNR",
        "outputId": "700e37c3-acbd-4a51-9e6f-ab7b6db0ab05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Accuracy :  8.0  Loss :  2.449977397918701\n",
            "Accuracy :  41.82089552238806  Loss :  1.5786146643149912\n",
            "Accuracy :  48.96758104738154  Loss :  1.401194090706452\n",
            "Validation: \n",
            "Accuracy :  69.0  Loss :  0.938983678817749\n",
            "Accuracy :  64.76190476190476  Loss :  1.0078013056800479\n",
            "Accuracy :  64.51219512195122  Loss :  1.003554972206674\n",
            "Accuracy :  64.29508196721312  Loss :  1.0004605142796625\n",
            "Accuracy :  64.14814814814815  Loss :  1.0051931863949624\n",
            "\n",
            "Epoch: 2\n",
            "Accuracy :  66.0  Loss :  1.010141372680664\n",
            "Accuracy :  64.33333333333333  Loss :  0.9950507594578302\n",
            "Accuracy :  66.07231920199501  Loss :  0.9523365898917143\n",
            "Validation: \n",
            "Accuracy :  77.0  Loss :  0.7040292620658875\n",
            "Accuracy :  71.33333333333333  Loss :  0.8100343119530451\n",
            "Accuracy :  71.41463414634147  Loss :  0.805798401192921\n",
            "Accuracy :  72.04918032786885  Loss :  0.7971230403321689\n",
            "Accuracy :  72.07407407407408  Loss :  0.8013275854381514\n",
            "\n",
            "Epoch: 3\n",
            "Accuracy :  72.0  Loss :  0.7292823195457458\n",
            "Accuracy :  72.43781094527363  Loss :  0.7892893994625528\n",
            "Accuracy :  73.3715710723192  Loss :  0.7666718797790737\n",
            "Validation: \n",
            "Accuracy :  78.0  Loss :  0.6459400057792664\n",
            "Accuracy :  75.0952380952381  Loss :  0.701491687979017\n",
            "Accuracy :  74.82926829268293  Loss :  0.7179880898173262\n",
            "Accuracy :  75.24590163934427  Loss :  0.7130336282683201\n",
            "Accuracy :  75.14814814814815  Loss :  0.7164570012210328\n",
            "\n",
            "Epoch: 4\n",
            "Accuracy :  74.0  Loss :  0.6738881468772888\n",
            "Accuracy :  76.98009950248756  Loss :  0.669839092155001\n",
            "Accuracy :  77.27680798004988  Loss :  0.6590547509621504\n",
            "Validation: \n",
            "Accuracy :  78.0  Loss :  0.6032878160476685\n",
            "Accuracy :  75.9047619047619  Loss :  0.6808278432914189\n",
            "Accuracy :  75.90243902439025  Loss :  0.6962265597610939\n",
            "Accuracy :  76.26229508196721  Loss :  0.6905729423780911\n",
            "Accuracy :  76.18518518518519  Loss :  0.6910273734434151\n",
            "\n",
            "Epoch: 5\n",
            "Accuracy :  78.0  Loss :  0.497842013835907\n",
            "Accuracy :  79.65174129353234  Loss :  0.5895447946009944\n",
            "Accuracy :  79.7780548628429  Loss :  0.5831858014822601\n",
            "Validation: \n",
            "Accuracy :  81.0  Loss :  0.5598539710044861\n",
            "Accuracy :  79.76190476190476  Loss :  0.5991009842781794\n",
            "Accuracy :  79.3170731707317  Loss :  0.6052172772768067\n",
            "Accuracy :  79.47540983606558  Loss :  0.6029800147306724\n",
            "Accuracy :  79.29629629629629  Loss :  0.6044202196745225\n",
            "\n",
            "Epoch: 6\n",
            "Accuracy :  81.0  Loss :  0.4801638126373291\n",
            "Accuracy :  81.44278606965175  Loss :  0.5365079659134594\n",
            "Accuracy :  81.70822942643392  Loss :  0.5306172665217868\n",
            "Validation: \n",
            "Accuracy :  83.0  Loss :  0.548690915107727\n",
            "Accuracy :  80.14285714285714  Loss :  0.5904044849531991\n",
            "Accuracy :  79.41463414634147  Loss :  0.6094341547023959\n",
            "Accuracy :  79.59016393442623  Loss :  0.5999371223762388\n",
            "Accuracy :  79.5679012345679  Loss :  0.5966470513814761\n",
            "\n",
            "Epoch: 7\n",
            "Accuracy :  83.0  Loss :  0.4283503592014313\n",
            "Accuracy :  83.363184079602  Loss :  0.48055048212779694\n",
            "Accuracy :  83.41147132169576  Loss :  0.4776970486985775\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4787822663784027\n",
            "Accuracy :  81.47619047619048  Loss :  0.5455009766987392\n",
            "Accuracy :  81.7560975609756  Loss :  0.5551755224786153\n",
            "Accuracy :  81.8360655737705  Loss :  0.5461597735764551\n",
            "Accuracy :  81.87654320987654  Loss :  0.5370061938409452\n",
            "\n",
            "Epoch: 8\n",
            "Accuracy :  81.0  Loss :  0.41130581498146057\n",
            "Accuracy :  84.45273631840796  Loss :  0.4466825822989146\n",
            "Accuracy :  84.53366583541147  Loss :  0.4432038131200167\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.46392619609832764\n",
            "Accuracy :  82.66666666666667  Loss :  0.5337523860590798\n",
            "Accuracy :  82.21951219512195  Loss :  0.545755980218329\n",
            "Accuracy :  82.27868852459017  Loss :  0.5401684798178126\n",
            "Accuracy :  82.34567901234568  Loss :  0.5363238472261547\n",
            "\n",
            "Epoch: 9\n",
            "Accuracy :  88.0  Loss :  0.27472424507141113\n",
            "Accuracy :  86.22885572139303  Loss :  0.40529578649879094\n",
            "Accuracy :  86.11221945137157  Loss :  0.40458075011014344\n",
            "Validation: \n",
            "Accuracy :  83.0  Loss :  0.47291260957717896\n",
            "Accuracy :  82.33333333333333  Loss :  0.5205648364055724\n",
            "Accuracy :  82.5609756097561  Loss :  0.5237443894147873\n",
            "Accuracy :  82.62295081967213  Loss :  0.5181200575144564\n",
            "Accuracy :  82.66666666666667  Loss :  0.5131550502997858\n",
            "\n",
            "Epoch: 10\n",
            "Accuracy :  93.0  Loss :  0.2960856258869171\n",
            "Accuracy :  86.93532338308458  Loss :  0.379097887532628\n",
            "Accuracy :  86.84538653366583  Loss :  0.381337571136672\n",
            "Validation: \n",
            "Accuracy :  83.0  Loss :  0.5219026803970337\n",
            "Accuracy :  81.9047619047619  Loss :  0.5333429333709535\n",
            "Accuracy :  81.8048780487805  Loss :  0.5449666926046697\n",
            "Accuracy :  81.80327868852459  Loss :  0.544399838955676\n",
            "Accuracy :  82.0246913580247  Loss :  0.5386721024542679\n"
          ]
        }
      ],
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+10):\n",
        "    train(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test(epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mERdZFH7fBXB"
      },
      "source": [
        "# Creating Dense Feature Dataset\n",
        "1.1 In this cell you need to remove the head of teacher network(i.e: last fullyconnected layer) and add a flatten layer at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIK5Vcfo8Y19",
        "outputId": "7b71d8b3-15d2-4067-c001-c95c3a540f40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "            Conv2d-4           [-1, 64, 32, 32]          36,928\n",
            "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
            "              ReLU-6           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 64, 16, 16]               0\n",
            "            Conv2d-8          [-1, 128, 16, 16]          73,856\n",
            "       BatchNorm2d-9          [-1, 128, 16, 16]             256\n",
            "             ReLU-10          [-1, 128, 16, 16]               0\n",
            "           Conv2d-11          [-1, 128, 16, 16]         147,584\n",
            "      BatchNorm2d-12          [-1, 128, 16, 16]             256\n",
            "             ReLU-13          [-1, 128, 16, 16]               0\n",
            "        MaxPool2d-14            [-1, 128, 8, 8]               0\n",
            "           Conv2d-15            [-1, 256, 8, 8]         295,168\n",
            "      BatchNorm2d-16            [-1, 256, 8, 8]             512\n",
            "             ReLU-17            [-1, 256, 8, 8]               0\n",
            "           Conv2d-18            [-1, 256, 8, 8]         590,080\n",
            "      BatchNorm2d-19            [-1, 256, 8, 8]             512\n",
            "             ReLU-20            [-1, 256, 8, 8]               0\n",
            "           Conv2d-21            [-1, 256, 8, 8]         590,080\n",
            "      BatchNorm2d-22            [-1, 256, 8, 8]             512\n",
            "             ReLU-23            [-1, 256, 8, 8]               0\n",
            "        MaxPool2d-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 512, 4, 4]       1,180,160\n",
            "      BatchNorm2d-26            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-27            [-1, 512, 4, 4]               0\n",
            "           Conv2d-28            [-1, 512, 4, 4]       2,359,808\n",
            "      BatchNorm2d-29            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-30            [-1, 512, 4, 4]               0\n",
            "           Conv2d-31            [-1, 512, 4, 4]       2,359,808\n",
            "      BatchNorm2d-32            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-33            [-1, 512, 4, 4]               0\n",
            "        MaxPool2d-34            [-1, 512, 2, 2]               0\n",
            "           Conv2d-35            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-36            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-37            [-1, 512, 2, 2]               0\n",
            "           Conv2d-38            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-39            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-40            [-1, 512, 2, 2]               0\n",
            "           Conv2d-41            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-42            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-43            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-44            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-45            [-1, 512, 1, 1]               0\n",
            "          Flatten-46                  [-1, 512]               0\n",
            "================================================================\n",
            "Total params: 14,723,136\n",
            "Trainable params: 14,723,136\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 6.57\n",
            "Params size (MB): 56.16\n",
            "Estimated Total Size (MB): 62.75\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "teacher_WOH = nn.Sequential(*(list(teacher.children())[:-1]), nn.Flatten())\n",
        "from torchsummary import summary\n",
        "summary(teacher_WOH, (3, 32, 32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG2u-KMYbvI6"
      },
      "source": [
        "The summery of your new teacher without head should look something like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFtzu7DD8uev",
        "outputId": "26ae9fdc-7384-4e61-9f0a-9d0df372dc5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "            Conv2d-4           [-1, 64, 32, 32]          36,928\n",
            "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
            "              ReLU-6           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 64, 16, 16]               0\n",
            "            Conv2d-8          [-1, 128, 16, 16]          73,856\n",
            "       BatchNorm2d-9          [-1, 128, 16, 16]             256\n",
            "             ReLU-10          [-1, 128, 16, 16]               0\n",
            "           Conv2d-11          [-1, 128, 16, 16]         147,584\n",
            "      BatchNorm2d-12          [-1, 128, 16, 16]             256\n",
            "             ReLU-13          [-1, 128, 16, 16]               0\n",
            "        MaxPool2d-14            [-1, 128, 8, 8]               0\n",
            "           Conv2d-15            [-1, 256, 8, 8]         295,168\n",
            "      BatchNorm2d-16            [-1, 256, 8, 8]             512\n",
            "             ReLU-17            [-1, 256, 8, 8]               0\n",
            "           Conv2d-18            [-1, 256, 8, 8]         590,080\n",
            "      BatchNorm2d-19            [-1, 256, 8, 8]             512\n",
            "             ReLU-20            [-1, 256, 8, 8]               0\n",
            "           Conv2d-21            [-1, 256, 8, 8]         590,080\n",
            "      BatchNorm2d-22            [-1, 256, 8, 8]             512\n",
            "             ReLU-23            [-1, 256, 8, 8]               0\n",
            "        MaxPool2d-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 512, 4, 4]       1,180,160\n",
            "      BatchNorm2d-26            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-27            [-1, 512, 4, 4]               0\n",
            "           Conv2d-28            [-1, 512, 4, 4]       2,359,808\n",
            "      BatchNorm2d-29            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-30            [-1, 512, 4, 4]               0\n",
            "           Conv2d-31            [-1, 512, 4, 4]       2,359,808\n",
            "      BatchNorm2d-32            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-33            [-1, 512, 4, 4]               0\n",
            "        MaxPool2d-34            [-1, 512, 2, 2]               0\n",
            "           Conv2d-35            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-36            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-37            [-1, 512, 2, 2]               0\n",
            "           Conv2d-38            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-39            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-40            [-1, 512, 2, 2]               0\n",
            "           Conv2d-41            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-42            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-43            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-44            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-45            [-1, 512, 1, 1]               0\n",
            "          Flatten-46                  [-1, 512]               0\n",
            "================================================================\n",
            "Total params: 14,723,136\n",
            "Trainable params: 14,723,136\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 6.57\n",
            "Params size (MB): 56.16\n",
            "Estimated Total Size (MB): 62.75\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "summary(teacher_WOH, (3, 32, 32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJsum1ioc2tj"
      },
      "source": [
        "1.2 In this cell you have to create dense feature labels dataset(i.e: the outputs of teacher network without head). For that you have to do forward pass on whole dataset and append the outputs in a variable. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jb8FytxoHM62"
      },
      "outputs": [],
      "source": [
        "teacher_WOH.eval()\n",
        "DenseTrain = {}\n",
        "DenseTest = {}\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs = inputs.to(device)\n",
        "        #code here\n",
        "        outputs = teacher_WOH(inputs)\n",
        "        DenseTrain[batch_idx] = outputs\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "        inputs = inputs.to(device)\n",
        "        #code here\n",
        "        outputs = teacher_WOH(inputs)\n",
        "        DenseTest[batch_idx] = outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqvK9fTEF08d",
        "outputId": "b477fcba-78c1-45f9-f212-1a527f8ba803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcs4ea2tG81q"
      },
      "outputs": [],
      "source": [
        "# def load_dict(filepath):\n",
        "#   with open(file_path, 'r') as file:\n",
        "#       data = json.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40sZZvY_cMpK"
      },
      "source": [
        "#Creating ad-hoc student network\n",
        "1.3 Now you need to create an ad-hoc student network that must have less than 2M parameters. for that, you can add a CFG in the following code to design your student network. The output shape(i.e fully connected layer) should match the shape of dense features so that Mean Squared Error can be applied between them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY0dC6EYILB3",
        "outputId": "545f7d73-df27-44d0-bf75-59c8cbaa40a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 64, 16, 16]               0\n",
            "            Conv2d-5          [-1, 128, 16, 16]          73,856\n",
            "       BatchNorm2d-6          [-1, 128, 16, 16]             256\n",
            "              ReLU-7          [-1, 128, 16, 16]               0\n",
            "         MaxPool2d-8            [-1, 128, 8, 8]               0\n",
            "            Conv2d-9            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-10            [-1, 128, 8, 8]             256\n",
            "             ReLU-11            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-12            [-1, 128, 4, 4]               0\n",
            "           Conv2d-13            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-14            [-1, 256, 4, 4]             512\n",
            "             ReLU-15            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-16            [-1, 256, 2, 2]               0\n",
            "           Conv2d-17            [-1, 512, 2, 2]       1,180,160\n",
            "      BatchNorm2d-18            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-19            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-20            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-21            [-1, 512, 1, 1]               0\n",
            "           Linear-22                  [-1, 512]         262,656\n",
            "================================================================\n",
            "Total params: 1,963,392\n",
            "Trainable params: 1,963,392\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.80\n",
            "Params size (MB): 7.49\n",
            "Estimated Total Size (MB): 10.30\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "    'VGGS': [64, 'M', 128, 'M', 128, 'M', 256, 'M', 512, 'M'],\n",
        "\n",
        "}\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(512, 512)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "s1 = VGG('VGGS')\n",
        "s1 = s1.to(device)\n",
        "summary(s1, (3, 32, 32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvwpr56o6hqO"
      },
      "source": [
        "# Training Student\n",
        "1.4 Here you will train the student network using Dense Features that you created in 1.2 as targets. Dataset datagen will provide data in batches so you need to extract the corresponding batch of targets from your Dense feature variable from 1.2, for this you can use the following formula:\n",
        "\n",
        "batch_index * batch_size --> (batch_index * batch_size) + batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHh5huK8SaF7"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(s1.parameters(), lr=0.0001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    s1.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        #code here\n",
        "        batch_size=100\n",
        "        targets = DenseTrain[batch_idx]\n",
        "        targets.to(device)\n",
        "        targets = Variable(targets)\n",
        "        s1.zero_grad()\n",
        "        \n",
        "        outputs = s1(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        if(batch_idx % 10 == 0):\n",
        "          print(\"Loss : \", train_loss/(batch_idx+1))\n",
        "def test(epoch):\n",
        "    s1.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            #code here\n",
        "            batch_size=100\n",
        "            targets = DenseTest[batch_idx]\n",
        "            targets.to(device)\n",
        "            targets = Variable(targets)\n",
        "            s1.zero_grad()\n",
        "            \n",
        "            outputs = s1(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\" Loss : \", test_loss/(batch_idx+1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnEeSdIfX5qp",
        "outputId": "7225258e-cafe-4133-f10b-56ea4bcd3f1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Loss :  0.09991147411383199\n",
            "Loss :  0.0999928440923437\n",
            "Loss :  0.10012034052041503\n",
            "Loss :  0.10014550903547088\n",
            "Loss :  0.1001330200977732\n",
            "Loss :  0.09999764778949093\n",
            "Validation: \n",
            " Loss :  0.07969352602958679\n",
            " Loss :  0.0883280641975857\n",
            " Loss :  0.08759218690598883\n",
            " Loss :  0.08810015914381528\n",
            " Loss :  0.08800661343115347\n",
            "\n",
            "Epoch: 15\n",
            "Loss :  0.1078830435872078\n",
            "Loss :  0.10051297735084187\n",
            "Loss :  0.09872171921389443\n",
            "Loss :  0.0975584342114387\n",
            "Loss :  0.09764004171621508\n",
            "Loss :  0.09823545871996413\n",
            "Loss :  0.09787525151108133\n",
            "Loss :  0.0980628091894405\n",
            "Loss :  0.09755374114086598\n",
            "Loss :  0.09759268508507656\n",
            "Loss :  0.09734466111305917\n",
            "Loss :  0.09747878717141109\n",
            "Loss :  0.09724893488667229\n",
            "Loss :  0.09713373373027977\n",
            "Loss :  0.09735549679884674\n",
            "Loss :  0.09741633661732768\n",
            "Loss :  0.09738623304581791\n",
            "Loss :  0.09741798500742829\n",
            "Loss :  0.09742673745800777\n",
            "Loss :  0.09728172417554556\n",
            "Loss :  0.09716998307562585\n",
            "Loss :  0.09726009101240556\n",
            "Loss :  0.09713373066882743\n",
            "Loss :  0.09701148468952675\n",
            "Loss :  0.0970838737376498\n",
            "Loss :  0.09713520356027254\n",
            "Loss :  0.09707203987001002\n",
            "Loss :  0.09723414890330656\n",
            "Loss :  0.09724761261952729\n",
            "Loss :  0.09712226583888031\n",
            "Loss :  0.0970326617261104\n",
            "Loss :  0.09692229280230316\n",
            "Loss :  0.09668615585732683\n",
            "Loss :  0.09663956020353784\n",
            "Loss :  0.09670688784367178\n",
            "Loss :  0.09668932029409626\n",
            "Loss :  0.09670281492772195\n",
            "Loss :  0.09672515030016475\n",
            "Loss :  0.09682037935560457\n",
            "Loss :  0.09663916947058095\n",
            "Loss :  0.09653500623610846\n",
            "Loss :  0.09659125512678837\n",
            "Loss :  0.09661603446456996\n",
            "Loss :  0.09662636519861333\n",
            "Loss :  0.09660722533253585\n",
            "Loss :  0.09661749096178426\n",
            "Loss :  0.09672365038617314\n",
            "Loss :  0.09672465284538877\n",
            "Loss :  0.09675591191172352\n",
            "Loss :  0.0966720354047181\n",
            "Validation: \n",
            " Loss :  0.08198745548725128\n",
            " Loss :  0.08822784821192424\n",
            " Loss :  0.08760136511267685\n",
            " Loss :  0.088041325695202\n",
            " Loss :  0.08772338466879762\n",
            "\n",
            "Epoch: 16\n",
            "Loss :  0.10787764936685562\n",
            "Loss :  0.09527980536222458\n",
            "Loss :  0.09539679473354704\n",
            "Loss :  0.09487272510605474\n",
            "Loss :  0.09605588527714334\n",
            "Loss :  0.09638125551681892\n",
            "Loss :  0.09640511955882682\n",
            "Loss :  0.09638072392890151\n",
            "Loss :  0.09617014560434553\n",
            "Loss :  0.09622775419400288\n",
            "Loss :  0.09568299087557462\n",
            "Loss :  0.09565777955828486\n",
            "Loss :  0.09534560550342906\n",
            "Loss :  0.09510703856935938\n",
            "Loss :  0.09522104569783447\n",
            "Loss :  0.09517317018564174\n",
            "Loss :  0.09506117816297163\n",
            "Loss :  0.09513195669441893\n",
            "Loss :  0.09505789331803664\n",
            "Loss :  0.09489861930384062\n",
            "Loss :  0.09487175930347015\n",
            "Loss :  0.0948704348709346\n",
            "Loss :  0.09473072794767526\n",
            "Loss :  0.09463441588016816\n",
            "Loss :  0.09461303837442794\n",
            "Loss :  0.09471747736294431\n",
            "Loss :  0.09475799701337156\n",
            "Loss :  0.0948725699058758\n",
            "Loss :  0.09481456387933887\n",
            "Loss :  0.09466933604982711\n",
            "Loss :  0.09459609996242777\n",
            "Loss :  0.09461877840029081\n",
            "Loss :  0.09449417784670804\n",
            "Loss :  0.09455679073434582\n",
            "Loss :  0.09464568915692242\n",
            "Loss :  0.09465337315431008\n",
            "Loss :  0.09475879759174305\n",
            "Loss :  0.09474520094070152\n",
            "Loss :  0.09482632614574407\n",
            "Loss :  0.09459273450438628\n",
            "Loss :  0.0945849160749716\n",
            "Loss :  0.09468573335458473\n",
            "Loss :  0.09465703999982028\n",
            "Loss :  0.09465384218850434\n",
            "Loss :  0.09483068482000001\n",
            "Loss :  0.09490153717162075\n",
            "Loss :  0.09499831903329899\n",
            "Loss :  0.09503523908919337\n",
            "Loss :  0.09502822402361277\n",
            "Loss :  0.09496545485105146\n",
            "Validation: \n",
            " Loss :  0.07832993566989899\n",
            " Loss :  0.08385288076741355\n",
            " Loss :  0.08293027103674121\n",
            " Loss :  0.08358247052939212\n",
            " Loss :  0.08355012508454146\n",
            "\n",
            "Epoch: 17\n",
            "Loss :  0.11160751432180405\n",
            "Loss :  0.09050907601009715\n",
            "Loss :  0.09152843838646299\n",
            "Loss :  0.09095360458858552\n",
            "Loss :  0.09130955814588361\n",
            "Loss :  0.09184151142835617\n",
            "Loss :  0.0918000258627485\n",
            "Loss :  0.0922946091478979\n",
            "Loss :  0.09199978411197662\n",
            "Loss :  0.0922417530155444\n",
            "Loss :  0.09198953262945213\n",
            "Loss :  0.0924796713767825\n",
            "Loss :  0.09232056110112136\n",
            "Loss :  0.0922270095871605\n",
            "Loss :  0.09262960171657252\n",
            "Loss :  0.092775006167936\n",
            "Loss :  0.09278034696482723\n",
            "Loss :  0.09282660885163915\n",
            "Loss :  0.09284025727056008\n",
            "Loss :  0.0928125613568965\n",
            "Loss :  0.09270032117171075\n",
            "Loss :  0.09258661203757282\n",
            "Loss :  0.09260833111688561\n",
            "Loss :  0.09245780436120508\n",
            "Loss :  0.09248962432640717\n",
            "Loss :  0.09250588308411291\n",
            "Loss :  0.09252375966630219\n",
            "Loss :  0.0926398738833811\n",
            "Loss :  0.0925474611518646\n",
            "Loss :  0.09251016222222154\n",
            "Loss :  0.09250565853427811\n",
            "Loss :  0.0925528217119036\n",
            "Loss :  0.09243940893920412\n",
            "Loss :  0.09238995242515\n",
            "Loss :  0.09249357787395153\n",
            "Loss :  0.09248125393930663\n",
            "Loss :  0.09249840552952152\n",
            "Loss :  0.09247486221019148\n",
            "Loss :  0.09256114040225197\n",
            "Loss :  0.0923781609901077\n",
            "Loss :  0.09226613588389614\n",
            "Loss :  0.09226889542129497\n",
            "Loss :  0.09230071761050304\n",
            "Loss :  0.09228981248256501\n",
            "Loss :  0.09239694106132806\n",
            "Loss :  0.09245278319538036\n",
            "Loss :  0.09250791951076587\n",
            "Loss :  0.09251813971755864\n",
            "Loss :  0.09251917830499938\n",
            "Loss :  0.09240695337960288\n",
            "Validation: \n",
            " Loss :  0.07750246673822403\n",
            " Loss :  0.08182033115909212\n",
            " Loss :  0.08142021934433681\n",
            " Loss :  0.08172885658311062\n",
            " Loss :  0.08161694749637886\n",
            "\n",
            "Epoch: 18\n",
            "Loss :  0.10420437902212143\n",
            "Loss :  0.09038519114255905\n",
            "Loss :  0.08968925050326756\n",
            "Loss :  0.08964054550855391\n",
            "Loss :  0.08993396726323337\n",
            "Loss :  0.09085458984561995\n",
            "Loss :  0.0906968215694193\n",
            "Loss :  0.09059866496794661\n",
            "Loss :  0.09056344729514769\n",
            "Loss :  0.09083803522062826\n",
            "Loss :  0.09052543083925059\n",
            "Loss :  0.09067698910429671\n",
            "Loss :  0.09054172291489672\n",
            "Loss :  0.09059988201119518\n",
            "Loss :  0.09083798760217977\n",
            "Loss :  0.0909373608744697\n",
            "Loss :  0.09081962986947587\n",
            "Loss :  0.0908908381273872\n",
            "Loss :  0.09099346975595253\n",
            "Loss :  0.09104590020410677\n",
            "Loss :  0.09090462745866966\n",
            "Loss :  0.090829098535375\n",
            "Loss :  0.09079650216377698\n",
            "Loss :  0.09068819506343825\n",
            "Loss :  0.09084710367487674\n",
            "Loss :  0.09080039000368688\n",
            "Loss :  0.09091418256476465\n",
            "Loss :  0.09100333766193848\n",
            "Loss :  0.09088280889805525\n",
            "Loss :  0.0908075715463186\n",
            "Loss :  0.09075538457628105\n",
            "Loss :  0.09085187663315193\n",
            "Loss :  0.09069106795149058\n",
            "Loss :  0.09064733862786854\n",
            "Loss :  0.09088429933157136\n",
            "Loss :  0.09091423921011112\n",
            "Loss :  0.091056544550403\n",
            "Loss :  0.0911488414332873\n",
            "Loss :  0.091280083428687\n",
            "Loss :  0.09116607300384576\n",
            "Loss :  0.09106667811733826\n",
            "Loss :  0.0910092232762462\n",
            "Loss :  0.09101101825115517\n",
            "Loss :  0.09099444233555251\n",
            "Loss :  0.0911235991282528\n",
            "Loss :  0.09115319846078192\n",
            "Loss :  0.09120544480915442\n",
            "Loss :  0.09123633053715315\n",
            "Loss :  0.09120034386177321\n",
            "Loss :  0.0911452350360313\n",
            "Validation: \n",
            " Loss :  0.07835517823696136\n",
            " Loss :  0.08338222226926259\n",
            " Loss :  0.08250750165160109\n",
            " Loss :  0.08299717795653422\n",
            " Loss :  0.08261157783829136\n",
            "\n",
            "Epoch: 19\n",
            "Loss :  0.10165289044380188\n",
            "Loss :  0.08559196510098198\n",
            "Loss :  0.0864735586302621\n",
            "Loss :  0.08670476583703872\n",
            "Loss :  0.0873738227457535\n",
            "Loss :  0.08782291573052313\n",
            "Loss :  0.08763545006513596\n",
            "Loss :  0.08795035179232208\n",
            "Loss :  0.08799318582923324\n",
            "Loss :  0.08871364339694872\n",
            "Loss :  0.0885373416661036\n",
            "Loss :  0.08889725327760249\n",
            "Loss :  0.08872840202544346\n",
            "Loss :  0.08852487472632459\n",
            "Loss :  0.08854704371369476\n",
            "Loss :  0.08865339915878725\n",
            "Loss :  0.08848738531518427\n",
            "Loss :  0.08859385253741728\n",
            "Loss :  0.08865263288356981\n",
            "Loss :  0.08861522341429875\n",
            "Loss :  0.08857337200078205\n",
            "Loss :  0.08859637948150319\n",
            "Loss :  0.08859953624765259\n",
            "Loss :  0.08861535078003294\n",
            "Loss :  0.0885265655661025\n",
            "Loss :  0.08875647374239576\n",
            "Loss :  0.0887095583353006\n",
            "Loss :  0.08880677214407832\n",
            "Loss :  0.08879138558466664\n",
            "Loss :  0.08874793499196108\n",
            "Loss :  0.08868824118791624\n",
            "Loss :  0.08859592769594438\n",
            "Loss :  0.08852131715816129\n",
            "Loss :  0.08849860144795968\n",
            "Loss :  0.08858015392620193\n",
            "Loss :  0.08862033940618194\n",
            "Loss :  0.08872641464101971\n",
            "Loss :  0.08865428117487952\n",
            "Loss :  0.08879705717948479\n",
            "Loss :  0.08869268059196984\n",
            "Loss :  0.08863411615242685\n",
            "Loss :  0.08862407869883697\n",
            "Loss :  0.08867767205430889\n",
            "Loss :  0.08862963961891121\n",
            "Loss :  0.08869377866623894\n",
            "Loss :  0.08872218245016762\n",
            "Loss :  0.08879689350239367\n",
            "Loss :  0.08881087071260323\n",
            "Loss :  0.08881885721009387\n",
            "Loss :  0.08876001851561113\n",
            "Validation: \n",
            " Loss :  0.07651372253894806\n",
            " Loss :  0.08182658397016071\n",
            " Loss :  0.08080018784214811\n",
            " Loss :  0.08143241679082151\n",
            " Loss :  0.08111995210250218\n",
            "\n",
            "Epoch: 20\n",
            "Loss :  0.10329113900661469\n",
            "Loss :  0.08849237385121259\n",
            "Loss :  0.08795037475370225\n",
            "Loss :  0.08709466337196288\n",
            "Loss :  0.08760548164931739\n",
            "Loss :  0.0881182259204341\n",
            "Loss :  0.08785511467789041\n",
            "Loss :  0.08806154236827098\n",
            "Loss :  0.08812911127820428\n",
            "Loss :  0.08834692344560728\n",
            "Loss :  0.08783960180117352\n",
            "Loss :  0.08797530433884612\n",
            "Loss :  0.08783248885850276\n",
            "Loss :  0.08769430965185165\n",
            "Loss :  0.08787493270339695\n",
            "Loss :  0.08804522469541096\n",
            "Loss :  0.08792034800378432\n",
            "Loss :  0.0879739173497373\n",
            "Loss :  0.08791718883079719\n",
            "Loss :  0.0878591524602855\n",
            "Loss :  0.08775820964901009\n",
            "Loss :  0.08774989981080683\n",
            "Loss :  0.0877423342064495\n",
            "Loss :  0.08762560926732563\n",
            "Loss :  0.08764050870020855\n",
            "Loss :  0.08760129517886743\n",
            "Loss :  0.0874928800082298\n",
            "Loss :  0.08758821173457641\n",
            "Loss :  0.08759517546120063\n",
            "Loss :  0.08759911601588488\n",
            "Loss :  0.08758578206910643\n",
            "Loss :  0.08756003361135434\n",
            "Loss :  0.08743903544554457\n",
            "Loss :  0.08732217543524917\n",
            "Loss :  0.08736504776037333\n",
            "Loss :  0.08740788969898496\n",
            "Loss :  0.08744437575670491\n",
            "Loss :  0.08745910486362052\n",
            "Loss :  0.0875223054308591\n",
            "Loss :  0.08737048342862093\n",
            "Loss :  0.08732107697876909\n",
            "Loss :  0.08731010164657649\n",
            "Loss :  0.08727692055234999\n",
            "Loss :  0.08727667077926361\n",
            "Loss :  0.08734444160512786\n",
            "Loss :  0.08740330396628962\n",
            "Loss :  0.08744448854905147\n",
            "Loss :  0.08748967141988677\n",
            "Loss :  0.08752557669384811\n",
            "Loss :  0.08747041301304842\n",
            "Validation: \n",
            " Loss :  0.07223153114318848\n",
            " Loss :  0.07983578031971342\n",
            " Loss :  0.07928024959273455\n",
            " Loss :  0.07972806826478145\n",
            " Loss :  0.07947632118507668\n",
            "\n",
            "Epoch: 21\n",
            "Loss :  0.09946149587631226\n",
            "Loss :  0.08341596681963313\n",
            "Loss :  0.08501088335400536\n",
            "Loss :  0.08449419491714047\n",
            "Loss :  0.08546199104407938\n",
            "Loss :  0.0856733829075215\n",
            "Loss :  0.08581592521218002\n",
            "Loss :  0.08614622350309936\n",
            "Loss :  0.0859452454764166\n",
            "Loss :  0.08593534186973677\n",
            "Loss :  0.08581045432256\n",
            "Loss :  0.08598395571246878\n",
            "Loss :  0.08570055897570839\n",
            "Loss :  0.08571464375002694\n",
            "Loss :  0.08580213732330512\n",
            "Loss :  0.08587240210629457\n",
            "Loss :  0.08555218915206304\n",
            "Loss :  0.08551825831333797\n",
            "Loss :  0.08562031816382434\n",
            "Loss :  0.08548832228358504\n",
            "Loss :  0.08544171938848732\n",
            "Loss :  0.08545108256040591\n",
            "Loss :  0.0854324252087606\n",
            "Loss :  0.08542175116864117\n",
            "Loss :  0.08546770744303944\n",
            "Loss :  0.08552399452820242\n",
            "Loss :  0.08549910070110554\n",
            "Loss :  0.08553713450132701\n",
            "Loss :  0.08551582823974806\n",
            "Loss :  0.08547014000591953\n",
            "Loss :  0.08538472273718083\n",
            "Loss :  0.0853904780132211\n",
            "Loss :  0.08524933712690419\n",
            "Loss :  0.0852363495623237\n",
            "Loss :  0.08533708947844519\n",
            "Loss :  0.08535389759262063\n",
            "Loss :  0.08541391457629666\n",
            "Loss :  0.08539736550050284\n",
            "Loss :  0.08550195722520508\n",
            "Loss :  0.08535171606961418\n",
            "Loss :  0.08529154763108775\n",
            "Loss :  0.08530387568792867\n",
            "Loss :  0.08538561762087135\n",
            "Loss :  0.08536724910451199\n",
            "Loss :  0.08543349591309791\n",
            "Loss :  0.08554046178141612\n",
            "Loss :  0.08566634075760583\n",
            "Loss :  0.08569667758835349\n",
            "Loss :  0.08569331553894369\n",
            "Loss :  0.08557294154361407\n",
            "Validation: \n",
            " Loss :  0.07801727205514908\n",
            " Loss :  0.07884420312586285\n",
            " Loss :  0.07828876049053378\n",
            " Loss :  0.07854680008575564\n",
            " Loss :  0.0781325379639496\n",
            "\n",
            "Epoch: 22\n",
            "Loss :  0.09904776513576508\n",
            "Loss :  0.08423142473806035\n",
            "Loss :  0.08306153118610382\n",
            "Loss :  0.08276432247892503\n",
            "Loss :  0.08356756262662934\n",
            "Loss :  0.08385924644329969\n",
            "Loss :  0.08376065482858752\n",
            "Loss :  0.08382562275083971\n",
            "Loss :  0.08376172267728382\n",
            "Loss :  0.08372598436180052\n",
            "Loss :  0.08354375467146977\n",
            "Loss :  0.08384439919714455\n",
            "Loss :  0.08372058282213762\n",
            "Loss :  0.08370666518693662\n",
            "Loss :  0.08405192687790444\n",
            "Loss :  0.08419480663261666\n",
            "Loss :  0.0840981165352075\n",
            "Loss :  0.08419735354986804\n",
            "Loss :  0.08411827173022275\n",
            "Loss :  0.08400105283210414\n",
            "Loss :  0.08381520041185825\n",
            "Loss :  0.08385374139270511\n",
            "Loss :  0.08386925119080695\n",
            "Loss :  0.08385345562324896\n",
            "Loss :  0.08396314841954046\n",
            "Loss :  0.08404385524085793\n",
            "Loss :  0.08400969723974608\n",
            "Loss :  0.08405549307370978\n",
            "Loss :  0.0840191133187759\n",
            "Loss :  0.0839430214613164\n",
            "Loss :  0.08386978026838397\n",
            "Loss :  0.08387204743562404\n",
            "Loss :  0.08370521135419329\n",
            "Loss :  0.08368696551488605\n",
            "Loss :  0.08378244239476419\n",
            "Loss :  0.08382052136452449\n",
            "Loss :  0.0839107701983148\n",
            "Loss :  0.08388876095614986\n",
            "Loss :  0.08397203140214986\n",
            "Loss :  0.08383417720227596\n",
            "Loss :  0.08380146634920577\n",
            "Loss :  0.08379754540113928\n",
            "Loss :  0.08381170283941079\n",
            "Loss :  0.08377843939649533\n",
            "Loss :  0.08383280486544244\n",
            "Loss :  0.08389159904879635\n",
            "Loss :  0.08396725414761234\n",
            "Loss :  0.08402424818201429\n",
            "Loss :  0.08409923683704805\n",
            "Loss :  0.08407284968860766\n",
            "Validation: \n",
            " Loss :  0.07400287687778473\n",
            " Loss :  0.07720398015919186\n",
            " Loss :  0.0762977783636349\n",
            " Loss :  0.07675063133728309\n",
            " Loss :  0.07637303745673027\n",
            "\n",
            "Epoch: 23\n",
            "Loss :  0.09484270215034485\n",
            "Loss :  0.08304568109187213\n",
            "Loss :  0.08311399569114049\n",
            "Loss :  0.08269791377167549\n",
            "Loss :  0.08351688977421784\n",
            "Loss :  0.0836058278001991\n",
            "Loss :  0.08360436729720382\n",
            "Loss :  0.0836154355759352\n",
            "Loss :  0.08347501136638501\n",
            "Loss :  0.0833853908947536\n",
            "Loss :  0.08294030094500815\n",
            "Loss :  0.08315314788807619\n",
            "Loss :  0.08314679895550751\n",
            "Loss :  0.08317956800451716\n",
            "Loss :  0.08334789113372776\n",
            "Loss :  0.08335836340260032\n",
            "Loss :  0.08328421506452265\n",
            "Loss :  0.08336286102993447\n",
            "Loss :  0.0833714422847026\n",
            "Loss :  0.0832391902414292\n",
            "Loss :  0.08318156215237148\n",
            "Loss :  0.08320302834019276\n",
            "Loss :  0.08323244426854595\n",
            "Loss :  0.08321155110994975\n",
            "Loss :  0.08325698458554834\n",
            "Loss :  0.08335481174438598\n",
            "Loss :  0.08335487818580935\n",
            "Loss :  0.08342312850965344\n",
            "Loss :  0.08339921024452325\n",
            "Loss :  0.08327464910409704\n",
            "Loss :  0.08315881661006383\n",
            "Loss :  0.08310582364199628\n",
            "Loss :  0.08295764029026031\n",
            "Loss :  0.08300630872073131\n",
            "Loss :  0.0831032316487206\n",
            "Loss :  0.08307962643879431\n",
            "Loss :  0.08317079132943933\n",
            "Loss :  0.08315708713149125\n",
            "Loss :  0.08326248419801081\n",
            "Loss :  0.08310618981376024\n",
            "Loss :  0.08303741917497202\n",
            "Loss :  0.08304622027923302\n",
            "Loss :  0.08305266511907487\n",
            "Loss :  0.08303823685079052\n",
            "Loss :  0.08307942374695997\n",
            "Loss :  0.08313463180596442\n",
            "Loss :  0.08315510238134939\n",
            "Loss :  0.0831783052905603\n",
            "Loss :  0.08316372000378036\n",
            "Loss :  0.08309950013388927\n",
            "Validation: \n",
            " Loss :  0.07520604133605957\n",
            " Loss :  0.0775635437596412\n",
            " Loss :  0.07660526055388334\n",
            " Loss :  0.07686573889900426\n",
            " Loss :  0.07654967241817051\n",
            "\n",
            "Epoch: 24\n",
            "Loss :  0.08808355033397675\n",
            "Loss :  0.08311368525028229\n",
            "Loss :  0.08360807136410758\n",
            "Loss :  0.08228118333124346\n",
            "Loss :  0.082429570759215\n",
            "Loss :  0.08243135859568913\n",
            "Loss :  0.08218839141677638\n",
            "Loss :  0.08222759703935033\n",
            "Loss :  0.08202171022141422\n",
            "Loss :  0.08215161535766098\n",
            "Loss :  0.08215537379578788\n",
            "Loss :  0.08241842049467671\n",
            "Loss :  0.08217306742983416\n",
            "Loss :  0.08194635034972475\n",
            "Loss :  0.0822171632808151\n",
            "Loss :  0.08222942399662851\n",
            "Loss :  0.08216889359936211\n",
            "Loss :  0.08208624826885803\n",
            "Loss :  0.08210576844314185\n",
            "Loss :  0.08204338356306416\n",
            "Loss :  0.08200095293682012\n",
            "Loss :  0.08207915899877864\n",
            "Loss :  0.08216098224010943\n",
            "Loss :  0.08203251827588845\n",
            "Loss :  0.08207248584116149\n",
            "Loss :  0.08209453146177459\n",
            "Loss :  0.0819557210380547\n",
            "Loss :  0.08194619046036168\n",
            "Loss :  0.08187687802675356\n",
            "Loss :  0.0818110438659019\n",
            "Loss :  0.08182614697669431\n",
            "Loss :  0.08173353661007436\n",
            "Loss :  0.08160673406822287\n",
            "Loss :  0.08158432786828441\n",
            "Loss :  0.08179613639794486\n",
            "Loss :  0.08185313735422585\n",
            "Loss :  0.08189818413716604\n",
            "Loss :  0.08192572017724944\n",
            "Loss :  0.08203493748984625\n",
            "Loss :  0.08186465977212352\n",
            "Loss :  0.08179509820895005\n",
            "Loss :  0.08180072866470854\n",
            "Loss :  0.08177610144302284\n",
            "Loss :  0.08173590978897088\n",
            "Loss :  0.08172636414094576\n",
            "Loss :  0.08175709910542103\n",
            "Loss :  0.08183681383858275\n",
            "Loss :  0.0819126065511992\n",
            "Loss :  0.08188652474115643\n",
            "Loss :  0.08175894693147141\n",
            "Validation: \n",
            " Loss :  0.07480458170175552\n",
            " Loss :  0.07580731205997013\n",
            " Loss :  0.07488296289996403\n",
            " Loss :  0.07514737375447007\n",
            " Loss :  0.07488597497160052\n",
            "\n",
            "Epoch: 25\n",
            "Loss :  0.09669330716133118\n",
            "Loss :  0.08183135701851411\n",
            "Loss :  0.07973043797981172\n",
            "Loss :  0.07892909477795323\n",
            "Loss :  0.07928134410119639\n",
            "Loss :  0.07960041159508276\n",
            "Loss :  0.079658535416009\n",
            "Loss :  0.07998136724804489\n",
            "Loss :  0.08000584231850541\n",
            "Loss :  0.0802606558734244\n",
            "Loss :  0.08015336406112898\n",
            "Loss :  0.08047305067648759\n",
            "Loss :  0.08037682185488298\n",
            "Loss :  0.08031011429679302\n",
            "Loss :  0.0805248406761927\n",
            "Loss :  0.0806888397659687\n",
            "Loss :  0.08052581104432574\n",
            "Loss :  0.08047608456067872\n",
            "Loss :  0.08049199359686994\n",
            "Loss :  0.08045624734843589\n",
            "Loss :  0.08040681929878928\n",
            "Loss :  0.08044207000760671\n",
            "Loss :  0.08047152762364478\n",
            "Loss :  0.08047018251764826\n",
            "Loss :  0.08045271258507526\n",
            "Loss :  0.08048651506938782\n",
            "Loss :  0.08048030608458537\n",
            "Loss :  0.08058061244522953\n",
            "Loss :  0.08059348900441173\n",
            "Loss :  0.08051596991589799\n",
            "Loss :  0.08043647265414464\n",
            "Loss :  0.08039158599457173\n",
            "Loss :  0.0802099746985599\n",
            "Loss :  0.08015558143846939\n",
            "Loss :  0.08025498889233709\n",
            "Loss :  0.08022008539095224\n",
            "Loss :  0.08035127263402675\n",
            "Loss :  0.0803550440426464\n",
            "Loss :  0.08042409896772365\n",
            "Loss :  0.0802780102433451\n",
            "Loss :  0.08017614467111311\n",
            "Loss :  0.08017705179696535\n",
            "Loss :  0.08020361893392232\n",
            "Loss :  0.08020887688113476\n",
            "Loss :  0.08024834588068683\n",
            "Loss :  0.08039607494210457\n",
            "Loss :  0.08047210292611877\n",
            "Loss :  0.08043889105003604\n",
            "Loss :  0.08044353830715227\n",
            "Loss :  0.08036055681418984\n",
            "Validation: \n",
            " Loss :  0.07002336531877518\n",
            " Loss :  0.07458736321755818\n",
            " Loss :  0.07402398582638764\n",
            " Loss :  0.07468892890410345\n",
            " Loss :  0.07429857634835774\n",
            "\n",
            "Epoch: 26\n",
            "Loss :  0.09324998408555984\n",
            "Loss :  0.07923781736330553\n",
            "Loss :  0.07843005408843358\n",
            "Loss :  0.07802492043664379\n",
            "Loss :  0.07839388672898455\n",
            "Loss :  0.07850898101049311\n",
            "Loss :  0.0785675265017103\n",
            "Loss :  0.07860012111109747\n",
            "Loss :  0.0786507894963394\n",
            "Loss :  0.07919990991825586\n",
            "Loss :  0.07864103702330354\n",
            "Loss :  0.07876171513989165\n",
            "Loss :  0.07868573894678069\n",
            "Loss :  0.07864360374803761\n",
            "Loss :  0.07875988352383283\n",
            "Loss :  0.07887923816181966\n",
            "Loss :  0.07892706485823815\n",
            "Loss :  0.0789135407373222\n",
            "Loss :  0.07898375330215\n",
            "Loss :  0.0789569819160781\n",
            "Loss :  0.07883677820661175\n",
            "Loss :  0.07892283885541121\n",
            "Loss :  0.07894494849767081\n",
            "Loss :  0.07899221123167963\n",
            "Loss :  0.0791138844309506\n",
            "Loss :  0.07921202955730408\n",
            "Loss :  0.07921480558726979\n",
            "Loss :  0.07936491819985238\n",
            "Loss :  0.07931943837009715\n",
            "Loss :  0.07937147547698922\n",
            "Loss :  0.07932706803479464\n",
            "Loss :  0.07930952760950928\n",
            "Loss :  0.07915342237069228\n",
            "Loss :  0.07919798658603627\n",
            "Loss :  0.07926942331612634\n",
            "Loss :  0.07926455624083169\n",
            "Loss :  0.07932344140933821\n",
            "Loss :  0.07935747831577881\n",
            "Loss :  0.07942396816931997\n",
            "Loss :  0.07932276179647202\n",
            "Loss :  0.07924931344025747\n",
            "Loss :  0.07927474365037143\n",
            "Loss :  0.07931609747316379\n",
            "Loss :  0.07933742075881936\n",
            "Loss :  0.07942555290636291\n",
            "Loss :  0.07946851434900597\n",
            "Loss :  0.07948796806650928\n",
            "Loss :  0.0795280250805705\n",
            "Loss :  0.07958132655915501\n",
            "Loss :  0.07951786660431845\n",
            "Validation: \n",
            " Loss :  0.06699071824550629\n",
            " Loss :  0.0749623669045312\n",
            " Loss :  0.07406661150659002\n",
            " Loss :  0.07442382747521166\n",
            " Loss :  0.07388693204632511\n",
            "\n",
            "Epoch: 27\n",
            "Loss :  0.09447724372148514\n",
            "Loss :  0.0788643942637877\n",
            "Loss :  0.07922939459482829\n",
            "Loss :  0.0782868547304984\n",
            "Loss :  0.07864623716691645\n",
            "Loss :  0.07926647481965084\n",
            "Loss :  0.07918563741640966\n",
            "Loss :  0.07918780611854204\n",
            "Loss :  0.07916675121695907\n",
            "Loss :  0.07946046122482844\n",
            "Loss :  0.07902620481972647\n",
            "Loss :  0.07915909930660918\n",
            "Loss :  0.07907392805026582\n",
            "Loss :  0.0789506803373344\n",
            "Loss :  0.0791808379985762\n",
            "Loss :  0.07910338542516658\n",
            "Loss :  0.07896425903583906\n",
            "Loss :  0.07913105799789318\n",
            "Loss :  0.07920526704900173\n",
            "Loss :  0.07907019106973528\n",
            "Loss :  0.07911482753593531\n",
            "Loss :  0.0792200309334773\n",
            "Loss :  0.07920784820124036\n",
            "Loss :  0.07912000462219312\n",
            "Loss :  0.07913677109969602\n",
            "Loss :  0.07921394459398619\n",
            "Loss :  0.07911152332678609\n",
            "Loss :  0.07914353148303789\n",
            "Loss :  0.07912380871399442\n",
            "Loss :  0.07905949352635551\n",
            "Loss :  0.07897244742839439\n",
            "Loss :  0.07891335074828752\n",
            "Loss :  0.07878328952648186\n",
            "Loss :  0.078749238589379\n",
            "Loss :  0.07885333288275252\n",
            "Loss :  0.07877819008721924\n",
            "Loss :  0.07883481245869745\n",
            "Loss :  0.07882127175835586\n",
            "Loss :  0.07881367067730646\n",
            "Loss :  0.07865561241917599\n",
            "Loss :  0.0785881594157873\n",
            "Loss :  0.07855374389611311\n",
            "Loss :  0.0786202361634678\n",
            "Loss :  0.07858540202445607\n",
            "Loss :  0.07872714462126193\n",
            "Loss :  0.07875534602979863\n",
            "Loss :  0.07885099850481865\n",
            "Loss :  0.07886769339410615\n",
            "Loss :  0.07884178357102023\n",
            "Loss :  0.07880136598273353\n",
            "Validation: \n",
            " Loss :  0.06864816695451736\n",
            " Loss :  0.07227771942104612\n",
            " Loss :  0.07214431691823936\n",
            " Loss :  0.0723807432490294\n",
            " Loss :  0.07206244886289408\n",
            "\n",
            "Epoch: 28\n",
            "Loss :  0.08467710018157959\n",
            "Loss :  0.0771033601327376\n",
            "Loss :  0.077452105425653\n",
            "Loss :  0.07734156616272465\n",
            "Loss :  0.07716999021245212\n",
            "Loss :  0.07692491014798482\n",
            "Loss :  0.07686318408270351\n",
            "Loss :  0.07707316963605478\n",
            "Loss :  0.07702390693220092\n",
            "Loss :  0.07709433817928964\n",
            "Loss :  0.07687126929954727\n",
            "Loss :  0.0770210184425384\n",
            "Loss :  0.07666135939561632\n",
            "Loss :  0.07669333906005357\n",
            "Loss :  0.07702872978774368\n",
            "Loss :  0.07723111844319382\n",
            "Loss :  0.07712491432794873\n",
            "Loss :  0.07719567193709619\n",
            "Loss :  0.07731668504973802\n",
            "Loss :  0.07723819380577322\n",
            "Loss :  0.07715443260411718\n",
            "Loss :  0.07721227744673666\n",
            "Loss :  0.07718695351238704\n",
            "Loss :  0.07717400461886868\n",
            "Loss :  0.07721568188046518\n",
            "Loss :  0.07731992274938351\n",
            "Loss :  0.07725195191315308\n",
            "Loss :  0.07737925798525669\n",
            "Loss :  0.07735171568086138\n",
            "Loss :  0.07737588307650638\n",
            "Loss :  0.07736389421891929\n",
            "Loss :  0.07739677911450625\n",
            "Loss :  0.07727828488403762\n",
            "Loss :  0.07723432312244015\n",
            "Loss :  0.07732889850424532\n",
            "Loss :  0.07729630737944886\n",
            "Loss :  0.07737364360954292\n",
            "Loss :  0.07741468330478732\n",
            "Loss :  0.07748643292255915\n",
            "Loss :  0.07737854569959823\n",
            "Loss :  0.07740090701013728\n",
            "Loss :  0.0774087383889949\n",
            "Loss :  0.0775015223815577\n",
            "Loss :  0.07750099407305851\n",
            "Loss :  0.07755759550161373\n",
            "Loss :  0.07763188632159698\n",
            "Loss :  0.07770148625116545\n",
            "Loss :  0.077724673304778\n",
            "Loss :  0.07773328461759799\n",
            "Loss :  0.07769320270164193\n",
            "Validation: \n",
            " Loss :  0.06696470081806183\n",
            " Loss :  0.07083898268285252\n",
            " Loss :  0.07048958076573\n",
            " Loss :  0.07101070569431195\n",
            " Loss :  0.07068520490034128\n",
            "\n",
            "Epoch: 29\n",
            "Loss :  0.08393335342407227\n",
            "Loss :  0.07701531458984721\n",
            "Loss :  0.07651395953836895\n",
            "Loss :  0.0755191495822322\n",
            "Loss :  0.07595345505127092\n",
            "Loss :  0.0757661209094758\n",
            "Loss :  0.07596174847395694\n",
            "Loss :  0.07615798816714488\n",
            "Loss :  0.07608167652362659\n",
            "Loss :  0.07629313786606212\n",
            "Loss :  0.07609387051941145\n",
            "Loss :  0.07635449980561798\n",
            "Loss :  0.07629870106119754\n",
            "Loss :  0.07616941175843013\n",
            "Loss :  0.07627860662784983\n",
            "Loss :  0.07634508432141993\n",
            "Loss :  0.07630134670075422\n",
            "Loss :  0.07618386093636005\n",
            "Loss :  0.07633360166576027\n",
            "Loss :  0.07628753305417706\n",
            "Loss :  0.07625204202399324\n",
            "Loss :  0.07622942635657098\n",
            "Loss :  0.07618975258385974\n",
            "Loss :  0.0762794270659938\n",
            "Loss :  0.07636035697465121\n",
            "Loss :  0.07635102658514008\n",
            "Loss :  0.07635790126762171\n",
            "Loss :  0.07639425855501111\n",
            "Loss :  0.07635874579597622\n",
            "Loss :  0.07633383899824726\n",
            "Loss :  0.07627437675860634\n",
            "Loss :  0.07628793737752261\n",
            "Loss :  0.07610885342844179\n",
            "Loss :  0.0760998616383335\n",
            "Loss :  0.07613444412427564\n",
            "Loss :  0.07618963258855703\n",
            "Loss :  0.07628417676546898\n",
            "Loss :  0.07628634428439757\n",
            "Loss :  0.07634133478905272\n",
            "Loss :  0.07620936795078276\n",
            "Loss :  0.07619600465284321\n",
            "Loss :  0.07617917849960988\n",
            "Loss :  0.07622547452105763\n",
            "Loss :  0.07614388876959785\n",
            "Loss :  0.07619625394171328\n",
            "Loss :  0.07626024443664466\n",
            "Loss :  0.07631211352904294\n",
            "Loss :  0.07635914111972615\n",
            "Loss :  0.07640632702010584\n",
            "Loss :  0.07633992308815239\n",
            "Validation: \n",
            " Loss :  0.07155962288379669\n",
            " Loss :  0.07475133133786065\n",
            " Loss :  0.07403746392668747\n",
            " Loss :  0.07447844426162907\n",
            " Loss :  0.07411268666202639\n",
            "\n",
            "Epoch: 30\n",
            "Loss :  0.08966238796710968\n",
            "Loss :  0.07676797156984155\n",
            "Loss :  0.07591377003561883\n",
            "Loss :  0.07533761438342833\n",
            "Loss :  0.07549238650173676\n",
            "Loss :  0.07577200197413855\n",
            "Loss :  0.07578324728080484\n",
            "Loss :  0.07596115405920525\n",
            "Loss :  0.0757850313352214\n",
            "Loss :  0.07604722715504877\n",
            "Loss :  0.07605692128291225\n",
            "Loss :  0.07640653509680215\n",
            "Loss :  0.07625672846170496\n",
            "Loss :  0.07598717166608526\n",
            "Loss :  0.07629832015075583\n",
            "Loss :  0.07629237691594275\n",
            "Loss :  0.07609897516436459\n",
            "Loss :  0.07623742814911039\n",
            "Loss :  0.07600606543665432\n",
            "Loss :  0.07591097850684096\n",
            "Loss :  0.07582276216267947\n",
            "Loss :  0.075817091258075\n",
            "Loss :  0.07580177059472956\n",
            "Loss :  0.07572489056042779\n",
            "Loss :  0.07572599687455106\n",
            "Loss :  0.07587051351528719\n",
            "Loss :  0.07582107750106588\n",
            "Loss :  0.07587958230358648\n",
            "Loss :  0.07585034963552213\n",
            "Loss :  0.0758348856876601\n",
            "Loss :  0.07576909372923382\n",
            "Loss :  0.07572539319610673\n",
            "Loss :  0.07559557913619781\n",
            "Loss :  0.0756137223852365\n",
            "Loss :  0.07567936573122953\n",
            "Loss :  0.07572397343453519\n",
            "Loss :  0.07571088262624688\n",
            "Loss :  0.07572897433270663\n",
            "Loss :  0.07582788494002475\n",
            "Loss :  0.07567848417612598\n",
            "Loss :  0.07567743477678655\n",
            "Loss :  0.07563977791880169\n",
            "Loss :  0.0756758550920849\n",
            "Loss :  0.07565181513950056\n",
            "Loss :  0.07573838974613181\n",
            "Loss :  0.0758260431846071\n",
            "Loss :  0.0759038395226131\n",
            "Loss :  0.07591145938052746\n",
            "Loss :  0.07593168879001404\n",
            "Loss :  0.07584502980446864\n",
            "Validation: \n",
            " Loss :  0.07001446187496185\n",
            " Loss :  0.07267856030237108\n",
            " Loss :  0.0715505237259516\n",
            " Loss :  0.07169916468565582\n",
            " Loss :  0.07144259813207167\n",
            "\n",
            "Epoch: 31\n",
            "Loss :  0.0861249640583992\n",
            "Loss :  0.07386549020355399\n",
            "Loss :  0.07394642773128692\n",
            "Loss :  0.07402752844556686\n",
            "Loss :  0.07455900620396544\n",
            "Loss :  0.07495464852043227\n",
            "Loss :  0.07484526516961269\n",
            "Loss :  0.07498553752059668\n",
            "Loss :  0.07521710894358011\n",
            "Loss :  0.07555405992073017\n",
            "Loss :  0.07535580925569677\n",
            "Loss :  0.07538221437517587\n",
            "Loss :  0.07525265829499103\n",
            "Loss :  0.07510434565994575\n",
            "Loss :  0.07531880378617463\n",
            "Loss :  0.0753248001615338\n",
            "Loss :  0.07522001966648961\n",
            "Loss :  0.07528290067586982\n",
            "Loss :  0.07526108468188107\n",
            "Loss :  0.07517878241173884\n",
            "Loss :  0.07516739348792911\n",
            "Loss :  0.0751437781934772\n",
            "Loss :  0.07508152711512815\n",
            "Loss :  0.07507414124035217\n",
            "Loss :  0.0751902587031675\n",
            "Loss :  0.07521703356943757\n",
            "Loss :  0.0751096185910519\n",
            "Loss :  0.07514430017543895\n",
            "Loss :  0.07513109190237055\n",
            "Loss :  0.07512802219728834\n",
            "Loss :  0.07512311990128404\n",
            "Loss :  0.07509990078412068\n",
            "Loss :  0.07499150276137662\n",
            "Loss :  0.07503178752846228\n",
            "Loss :  0.0751217135127164\n",
            "Loss :  0.07511306835333166\n",
            "Loss :  0.07515840064166655\n",
            "Loss :  0.07514158542948271\n",
            "Loss :  0.07521486789809437\n",
            "Loss :  0.07508823120266275\n",
            "Loss :  0.0750073317038894\n",
            "Loss :  0.07499943723462504\n",
            "Loss :  0.07501224239938616\n",
            "Loss :  0.07497323877469846\n",
            "Loss :  0.07505629503098475\n",
            "Loss :  0.07507012036944968\n",
            "Loss :  0.07510214867050115\n",
            "Loss :  0.07510006865150357\n",
            "Loss :  0.07508981697989353\n",
            "Loss :  0.07500979185195415\n",
            "Validation: \n",
            " Loss :  0.06822653114795685\n",
            " Loss :  0.07346713330064501\n",
            " Loss :  0.07202775521976192\n",
            " Loss :  0.07200196226600741\n",
            " Loss :  0.07162407242957457\n",
            "\n",
            "Epoch: 32\n",
            "Loss :  0.08494888246059418\n",
            "Loss :  0.07213907621123573\n",
            "Loss :  0.0718861963777315\n",
            "Loss :  0.0718594228308047\n",
            "Loss :  0.07234205732621797\n",
            "Loss :  0.07288843239931499\n",
            "Loss :  0.07313505175416588\n",
            "Loss :  0.07318716039749938\n",
            "Loss :  0.0732290351556407\n",
            "Loss :  0.0733608416394218\n",
            "Loss :  0.07318901019816351\n",
            "Loss :  0.07354368537932902\n",
            "Loss :  0.07351961898163331\n",
            "Loss :  0.07327386105561075\n",
            "Loss :  0.07330542279684797\n",
            "Loss :  0.07352400220782551\n",
            "Loss :  0.07345035609713993\n",
            "Loss :  0.0734998487720364\n",
            "Loss :  0.0735338015162813\n",
            "Loss :  0.07345700948572284\n",
            "Loss :  0.07344679380604877\n",
            "Loss :  0.07346114825227815\n",
            "Loss :  0.07355320330591224\n",
            "Loss :  0.07358361554868294\n",
            "Loss :  0.07366708261096132\n",
            "Loss :  0.07370395643777582\n",
            "Loss :  0.07369951521299808\n",
            "Loss :  0.07380545834883552\n",
            "Loss :  0.07381839435083586\n",
            "Loss :  0.07379105656417374\n",
            "Loss :  0.07374040023779552\n",
            "Loss :  0.07378790456909459\n",
            "Loss :  0.07367936163165859\n",
            "Loss :  0.0736796761049785\n",
            "Loss :  0.07375567953738649\n",
            "Loss :  0.073746243189288\n",
            "Loss :  0.07376111427791561\n",
            "Loss :  0.07374296083323396\n",
            "Loss :  0.0738519510630704\n",
            "Loss :  0.07368273092695819\n",
            "Loss :  0.07360126432077843\n",
            "Loss :  0.07355794533543343\n",
            "Loss :  0.07355966587106293\n",
            "Loss :  0.07358589229063756\n",
            "Loss :  0.07367080926489668\n",
            "Loss :  0.07376065334829682\n",
            "Loss :  0.07389132995279647\n",
            "Loss :  0.07392086564798517\n",
            "Loss :  0.07394086735288219\n",
            "Loss :  0.07387912183878621\n",
            "Validation: \n",
            " Loss :  0.0634118989109993\n",
            " Loss :  0.06981022851098151\n",
            " Loss :  0.06933642315064989\n",
            " Loss :  0.06974748980070723\n",
            " Loss :  0.06934366057868357\n",
            "\n",
            "Epoch: 33\n",
            "Loss :  0.08402875810861588\n",
            "Loss :  0.07186126505786722\n",
            "Loss :  0.0714864812436558\n",
            "Loss :  0.07180373803261787\n",
            "Loss :  0.07237216339605611\n",
            "Loss :  0.07273393511479977\n",
            "Loss :  0.07297938152170572\n",
            "Loss :  0.07321629085591141\n",
            "Loss :  0.07340123026091376\n",
            "Loss :  0.0736788973048493\n",
            "Loss :  0.0734887878862348\n",
            "Loss :  0.07358008834558565\n",
            "Loss :  0.07332095568460867\n",
            "Loss :  0.07327860306578739\n",
            "Loss :  0.0735620934225566\n",
            "Loss :  0.07353096610761636\n",
            "Loss :  0.07340345811677275\n",
            "Loss :  0.07341226148936483\n",
            "Loss :  0.07338406205259634\n",
            "Loss :  0.07322043648565003\n",
            "Loss :  0.07329405673700778\n",
            "Loss :  0.07334283117858155\n",
            "Loss :  0.07332436598803663\n",
            "Loss :  0.07328846437848492\n",
            "Loss :  0.07335600927781268\n",
            "Loss :  0.07339935262958366\n",
            "Loss :  0.07337842070965017\n",
            "Loss :  0.07344347096952125\n",
            "Loss :  0.07342465721680601\n",
            "Loss :  0.07341548353689643\n",
            "Loss :  0.073376193927273\n",
            "Loss :  0.07337853469409743\n",
            "Loss :  0.07321035923476903\n",
            "Loss :  0.07319246456477217\n",
            "Loss :  0.07324405120483592\n",
            "Loss :  0.07331238853668215\n",
            "Loss :  0.07337031260628119\n",
            "Loss :  0.07338850849842768\n",
            "Loss :  0.07349369932073621\n",
            "Loss :  0.07340377267173794\n",
            "Loss :  0.07336744769218556\n",
            "Loss :  0.07336622107674316\n",
            "Loss :  0.07336417381207054\n",
            "Loss :  0.07336389397005194\n",
            "Loss :  0.07342334976103031\n",
            "Loss :  0.07343126754968235\n",
            "Loss :  0.07350576904131637\n",
            "Loss :  0.07350789990489649\n",
            "Loss :  0.07354048917577322\n",
            "Loss :  0.07346480420948046\n",
            "Validation: \n",
            " Loss :  0.07348266988992691\n",
            " Loss :  0.07331978458733786\n",
            " Loss :  0.0720532152165727\n",
            " Loss :  0.07240966518150001\n",
            " Loss :  0.07180820869994753\n",
            "\n",
            "Epoch: 34\n",
            "Loss :  0.09086000174283981\n",
            "Loss :  0.07239927554672415\n",
            "Loss :  0.0722210162452289\n",
            "Loss :  0.07217429674440815\n",
            "Loss :  0.07231894135475159\n",
            "Loss :  0.07256435412986606\n",
            "Loss :  0.07296232940232167\n",
            "Loss :  0.07292109019529651\n",
            "Loss :  0.07275222444239958\n",
            "Loss :  0.073077818440212\n",
            "Loss :  0.07285481510628568\n",
            "Loss :  0.07308798884083559\n",
            "Loss :  0.07280243249717823\n",
            "Loss :  0.0726303534425852\n",
            "Loss :  0.0728626695613489\n",
            "Loss :  0.07296224925296986\n",
            "Loss :  0.07282557077011707\n",
            "Loss :  0.07282794772358665\n",
            "Loss :  0.07288540533399054\n",
            "Loss :  0.07277872070124013\n",
            "Loss :  0.07278834631787011\n",
            "Loss :  0.07287396610630632\n",
            "Loss :  0.07285097271743403\n",
            "Loss :  0.07273019082618482\n",
            "Loss :  0.07270234894332055\n",
            "Loss :  0.07279424544468344\n",
            "Loss :  0.07280267703693032\n",
            "Loss :  0.07288098896121627\n",
            "Loss :  0.0728079817420223\n",
            "Loss :  0.07282183040253486\n",
            "Loss :  0.07281003636230662\n",
            "Loss :  0.07279518099316064\n",
            "Loss :  0.07261019291768193\n",
            "Loss :  0.07263187457472536\n",
            "Loss :  0.07272925543601562\n",
            "Loss :  0.07271473465269787\n",
            "Loss :  0.0727124535001381\n",
            "Loss :  0.07269652347479548\n",
            "Loss :  0.07276929188470828\n",
            "Loss :  0.07265801453376974\n",
            "Loss :  0.07260206025883444\n",
            "Loss :  0.0726178736219731\n",
            "Loss :  0.07260109001058297\n",
            "Loss :  0.07256986759170855\n",
            "Loss :  0.07263884549778876\n",
            "Loss :  0.0727319194544974\n",
            "Loss :  0.07285215560177143\n",
            "Loss :  0.07289658227417373\n",
            "Loss :  0.07289799626552637\n",
            "Loss :  0.07281312094565079\n",
            "Validation: \n",
            " Loss :  0.06281840056180954\n",
            " Loss :  0.07059850650174278\n",
            " Loss :  0.07038083972363937\n",
            " Loss :  0.07054584772616136\n",
            " Loss :  0.06994424047477452\n",
            "\n",
            "Epoch: 35\n",
            "Loss :  0.07843432575464249\n",
            "Loss :  0.072190867906267\n",
            "Loss :  0.0717659518122673\n",
            "Loss :  0.07184559311116895\n",
            "Loss :  0.07221276812800546\n",
            "Loss :  0.07244890285473243\n",
            "Loss :  0.07230172538366474\n",
            "Loss :  0.07219672245039067\n",
            "Loss :  0.07214834081169999\n",
            "Loss :  0.07238262588833715\n",
            "Loss :  0.07212987859355341\n",
            "Loss :  0.0723419911018363\n",
            "Loss :  0.07229198022814821\n",
            "Loss :  0.07227244802558695\n",
            "Loss :  0.07247635811990034\n",
            "Loss :  0.0724540665647052\n",
            "Loss :  0.07234462010471718\n",
            "Loss :  0.07242278242756052\n",
            "Loss :  0.0723674786765931\n",
            "Loss :  0.07211469942752603\n",
            "Loss :  0.07210054357920713\n",
            "Loss :  0.07217212436252861\n",
            "Loss :  0.0721133993896424\n",
            "Loss :  0.07209796406876989\n",
            "Loss :  0.07209599296334374\n",
            "Loss :  0.0721509428138277\n",
            "Loss :  0.07215044915790302\n",
            "Loss :  0.07225427845307382\n",
            "Loss :  0.07227029367698044\n",
            "Loss :  0.07221318151532989\n",
            "Loss :  0.07215558535781413\n",
            "Loss :  0.07214000706120703\n",
            "Loss :  0.07196626905061745\n",
            "Loss :  0.07203684006662887\n",
            "Loss :  0.07211858420864928\n",
            "Loss :  0.07211350980732176\n",
            "Loss :  0.07212469062233896\n",
            "Loss :  0.07209475667810826\n",
            "Loss :  0.07217642529977589\n",
            "Loss :  0.07207130941817218\n",
            "Loss :  0.07202660548791029\n",
            "Loss :  0.07199214578327472\n",
            "Loss :  0.07196860103879009\n",
            "Loss :  0.07198451604616338\n",
            "Loss :  0.07203582226554282\n",
            "Loss :  0.07210174533321694\n",
            "Loss :  0.07214641831249581\n",
            "Loss :  0.07216078451104985\n",
            "Loss :  0.07214117636458789\n",
            "Loss :  0.07212322632461604\n",
            "Validation: \n",
            " Loss :  0.06380391865968704\n",
            " Loss :  0.06775854865000361\n",
            " Loss :  0.06671619905931193\n",
            " Loss :  0.06713939861195986\n",
            " Loss :  0.06697125351171435\n",
            "\n",
            "Epoch: 36\n",
            "Loss :  0.08283784985542297\n",
            "Loss :  0.07128794660622423\n",
            "Loss :  0.07138461193868093\n",
            "Loss :  0.07072066551735325\n",
            "Loss :  0.07066807078152168\n",
            "Loss :  0.07077656167687155\n",
            "Loss :  0.07081206307792273\n",
            "Loss :  0.07112512029182742\n",
            "Loss :  0.0711348285369667\n",
            "Loss :  0.07118877903609486\n",
            "Loss :  0.07103746410201092\n",
            "Loss :  0.07135320169565913\n",
            "Loss :  0.07129300117862127\n",
            "Loss :  0.07129036774503365\n",
            "Loss :  0.07143599685307936\n",
            "Loss :  0.07146935650943131\n",
            "Loss :  0.07132856705173943\n",
            "Loss :  0.07141710020471037\n",
            "Loss :  0.07149231166306122\n",
            "Loss :  0.07136302423804843\n",
            "Loss :  0.07126008624683565\n",
            "Loss :  0.07131011708158452\n",
            "Loss :  0.07130127954739252\n",
            "Loss :  0.07126411184410512\n",
            "Loss :  0.0712283006656714\n",
            "Loss :  0.07122177710808605\n",
            "Loss :  0.07117551108429715\n",
            "Loss :  0.0712071323691699\n",
            "Loss :  0.07125157788661027\n",
            "Loss :  0.07126572249374029\n",
            "Loss :  0.07123257229692516\n",
            "Loss :  0.07124864725677531\n",
            "Loss :  0.07112148263697683\n",
            "Loss :  0.07105065696504181\n",
            "Loss :  0.0711032480460283\n",
            "Loss :  0.0710989421398042\n",
            "Loss :  0.07114069451709533\n",
            "Loss :  0.07115102261104031\n",
            "Loss :  0.07120835715587058\n",
            "Loss :  0.07108140526258427\n",
            "Loss :  0.07104741913868008\n",
            "Loss :  0.07100777064252944\n",
            "Loss :  0.07098209011122902\n",
            "Loss :  0.0710045615526612\n",
            "Loss :  0.07110271553029548\n",
            "Loss :  0.07115434965081595\n",
            "Loss :  0.07122737901394383\n",
            "Loss :  0.07126299565511651\n",
            "Loss :  0.0712796398854801\n",
            "Loss :  0.07121199465028623\n",
            "Validation: \n",
            " Loss :  0.06478001177310944\n",
            " Loss :  0.06929602225621541\n",
            " Loss :  0.06797036210574754\n",
            " Loss :  0.0686061887345353\n",
            " Loss :  0.06812648916686023\n",
            "\n",
            "Epoch: 37\n",
            "Loss :  0.08874134719371796\n",
            "Loss :  0.07082374021410942\n",
            "Loss :  0.06945892387912386\n",
            "Loss :  0.0695802160809117\n",
            "Loss :  0.07061667122491976\n",
            "Loss :  0.07064426515032263\n",
            "Loss :  0.07023046629839257\n",
            "Loss :  0.0703137197964628\n",
            "Loss :  0.0703468045886652\n",
            "Loss :  0.07045169952479037\n",
            "Loss :  0.0703679873137781\n",
            "Loss :  0.07047307669176711\n",
            "Loss :  0.07018673478448686\n",
            "Loss :  0.0701167771436331\n",
            "Loss :  0.07043510926425034\n",
            "Loss :  0.07058190956514403\n",
            "Loss :  0.07043654869617143\n",
            "Loss :  0.07055046614150555\n",
            "Loss :  0.07056801681772121\n",
            "Loss :  0.07052438760771178\n",
            "Loss :  0.07045208664601715\n",
            "Loss :  0.0704359124939871\n",
            "Loss :  0.07043172902853241\n",
            "Loss :  0.0704289156121093\n",
            "Loss :  0.07046064000221208\n",
            "Loss :  0.07053337179391507\n",
            "Loss :  0.07045189829336272\n",
            "Loss :  0.07044605617978476\n",
            "Loss :  0.07044039577829032\n",
            "Loss :  0.07041145698567436\n",
            "Loss :  0.07036219915975368\n",
            "Loss :  0.07042409447276324\n",
            "Loss :  0.07033789271599036\n",
            "Loss :  0.07037551944169393\n",
            "Loss :  0.07047756608368015\n",
            "Loss :  0.0705239045144486\n",
            "Loss :  0.07057488354702075\n",
            "Loss :  0.07060584098582641\n",
            "Loss :  0.07069494188066543\n",
            "Loss :  0.07054844403358372\n",
            "Loss :  0.07051336708620302\n",
            "Loss :  0.0704913062299545\n",
            "Loss :  0.07045813616267858\n",
            "Loss :  0.07042961901274192\n",
            "Loss :  0.07048947337060288\n",
            "Loss :  0.07051081221938926\n",
            "Loss :  0.07059361834135594\n",
            "Loss :  0.07063621734990688\n",
            "Loss :  0.07060094990161501\n",
            "Loss :  0.07057905313226939\n",
            "Validation: \n",
            " Loss :  0.061953190714120865\n",
            " Loss :  0.06733885052658263\n",
            " Loss :  0.06691496837429883\n",
            " Loss :  0.06762489908542789\n",
            " Loss :  0.06738650679220388\n",
            "\n",
            "Epoch: 38\n",
            "Loss :  0.08498869091272354\n",
            "Loss :  0.06979714706540108\n",
            "Loss :  0.06994943835196041\n",
            "Loss :  0.06977229805723313\n",
            "Loss :  0.06960420292325137\n",
            "Loss :  0.06995383270230948\n",
            "Loss :  0.0703162616882168\n",
            "Loss :  0.07043288999669989\n",
            "Loss :  0.07045897882845667\n",
            "Loss :  0.07068524055264808\n",
            "Loss :  0.07045820543524062\n",
            "Loss :  0.070451852404051\n",
            "Loss :  0.07023381863620655\n",
            "Loss :  0.06996959908545472\n",
            "Loss :  0.0700753777955018\n",
            "Loss :  0.07006622890368203\n",
            "Loss :  0.06989671778142081\n",
            "Loss :  0.0698088662404763\n",
            "Loss :  0.06994125559843706\n",
            "Loss :  0.0698637586301534\n",
            "Loss :  0.06983340837395013\n",
            "Loss :  0.06987673953458032\n",
            "Loss :  0.06982737808273388\n",
            "Loss :  0.06986407829182488\n",
            "Loss :  0.0699651497280944\n",
            "Loss :  0.06997492449335843\n",
            "Loss :  0.06998218670201942\n",
            "Loss :  0.06995218337901844\n",
            "Loss :  0.06992426384704394\n",
            "Loss :  0.06990479414200865\n",
            "Loss :  0.06983807619871492\n",
            "Loss :  0.06984341114615704\n",
            "Loss :  0.06969548246570836\n",
            "Loss :  0.06971698579606332\n",
            "Loss :  0.06977009835524643\n",
            "Loss :  0.06981363273796193\n",
            "Loss :  0.0698871854537907\n",
            "Loss :  0.0699216634577336\n",
            "Loss :  0.06998300242416189\n",
            "Loss :  0.06984238517101464\n",
            "Loss :  0.06979830645749396\n",
            "Loss :  0.0697964434931168\n",
            "Loss :  0.06979952545531289\n",
            "Loss :  0.06973296947552544\n",
            "Loss :  0.06982834751204568\n",
            "Loss :  0.06992277705385257\n",
            "Loss :  0.06998732433110928\n",
            "Loss :  0.07001972710219412\n",
            "Loss :  0.0700190091455305\n",
            "Loss :  0.07000289264072469\n",
            "Validation: \n",
            " Loss :  0.06836593151092529\n",
            " Loss :  0.06848114853103955\n",
            " Loss :  0.06764204427599907\n",
            " Loss :  0.06822392313939626\n",
            " Loss :  0.06784190228323878\n",
            "\n",
            "Epoch: 39\n",
            "Loss :  0.07270607352256775\n",
            "Loss :  0.06693468209017407\n",
            "Loss :  0.06745836599951699\n",
            "Loss :  0.06646197108972457\n",
            "Loss :  0.06777683182097063\n",
            "Loss :  0.06768335372793909\n",
            "Loss :  0.06780719915862943\n",
            "Loss :  0.06808102288296525\n",
            "Loss :  0.0683456101903209\n",
            "Loss :  0.06877249086296165\n",
            "Loss :  0.06878385796110228\n",
            "Loss :  0.06906633105900911\n",
            "Loss :  0.06901455676752674\n",
            "Loss :  0.06892648851143494\n",
            "Loss :  0.06918203968105587\n",
            "Loss :  0.06928251622035014\n",
            "Loss :  0.06924409824221031\n",
            "Loss :  0.06946333621939023\n",
            "Loss :  0.0695113821047775\n",
            "Loss :  0.06935995645548036\n",
            "Loss :  0.06930337570125784\n",
            "Loss :  0.06931527790511954\n",
            "Loss :  0.06933416603175224\n",
            "Loss :  0.06933575857317809\n",
            "Loss :  0.0694108437205746\n",
            "Loss :  0.06942023951337632\n",
            "Loss :  0.069379020410936\n",
            "Loss :  0.06934221472412458\n",
            "Loss :  0.0693382601323289\n",
            "Loss :  0.06937351696116408\n",
            "Loss :  0.06936911436608463\n",
            "Loss :  0.06933792470879493\n",
            "Loss :  0.06920933432157537\n",
            "Loss :  0.06931000886051676\n",
            "Loss :  0.06942301294889268\n",
            "Loss :  0.06944766952696009\n",
            "Loss :  0.06941134378396573\n",
            "Loss :  0.06942841810277209\n",
            "Loss :  0.06953782052427453\n",
            "Loss :  0.06936872041667513\n",
            "Loss :  0.06932220079394945\n",
            "Loss :  0.06935381592951552\n",
            "Loss :  0.06938931860932261\n",
            "Loss :  0.06937949442393265\n",
            "Loss :  0.06947995622614885\n",
            "Loss :  0.06958066561401816\n",
            "Loss :  0.06960700625258774\n",
            "Loss :  0.06965797532136274\n",
            "Loss :  0.06966835739799696\n",
            "Loss :  0.06958978103432054\n",
            "Validation: \n",
            " Loss :  0.0691145658493042\n",
            " Loss :  0.0704716733168988\n",
            " Loss :  0.06993826996625924\n",
            " Loss :  0.07026965708517638\n",
            " Loss :  0.06976559390256434\n",
            "\n",
            "Epoch: 40\n",
            "Loss :  0.07898664474487305\n",
            "Loss :  0.07037306847897443\n",
            "Loss :  0.0690457835083916\n",
            "Loss :  0.06838832615363982\n",
            "Loss :  0.06888709717044016\n",
            "Loss :  0.06832152641579217\n",
            "Loss :  0.06813156012384618\n",
            "Loss :  0.06834204584150247\n",
            "Loss :  0.06862684831391146\n",
            "Loss :  0.06866495986724948\n",
            "Loss :  0.06849892776791412\n",
            "Loss :  0.06861300484554188\n",
            "Loss :  0.06859605186734317\n",
            "Loss :  0.06853332054865269\n",
            "Loss :  0.06866279241780862\n",
            "Loss :  0.0686327007087256\n",
            "Loss :  0.06848014076674207\n",
            "Loss :  0.06860147978652988\n",
            "Loss :  0.06868666031742623\n",
            "Loss :  0.06857536021249457\n",
            "Loss :  0.06851943991892967\n",
            "Loss :  0.06856460906388635\n",
            "Loss :  0.0685557005969108\n",
            "Loss :  0.06852155361018139\n",
            "Loss :  0.06857615939623588\n",
            "Loss :  0.06864199625602756\n",
            "Loss :  0.06853431250571748\n",
            "Loss :  0.06855510598619045\n",
            "Loss :  0.0686127845449804\n",
            "Loss :  0.06855490249405612\n",
            "Loss :  0.06854031233741992\n",
            "Loss :  0.06855623595344676\n",
            "Loss :  0.0684459678294874\n",
            "Loss :  0.06843028794917816\n",
            "Loss :  0.06852443333813522\n",
            "Loss :  0.06855814432741231\n",
            "Loss :  0.06866183813011217\n",
            "Loss :  0.06871275744348201\n",
            "Loss :  0.06879754295069089\n",
            "Loss :  0.0686858265334383\n",
            "Loss :  0.06867747539259549\n",
            "Loss :  0.06868096828968276\n",
            "Loss :  0.0686557639098224\n",
            "Loss :  0.06861152367670685\n",
            "Loss :  0.06866483244191762\n",
            "Loss :  0.06874292052274533\n",
            "Loss :  0.06877207544323162\n",
            "Loss :  0.06881756730962964\n",
            "Loss :  0.06880075616177303\n",
            "Loss :  0.06874247577397984\n",
            "Validation: \n",
            " Loss :  0.0664747804403305\n",
            " Loss :  0.06812814835991178\n",
            " Loss :  0.06761647179359342\n",
            " Loss :  0.06804814837017997\n",
            " Loss :  0.06754716609915097\n",
            "\n",
            "Epoch: 41\n",
            "Loss :  0.07563483715057373\n",
            "Loss :  0.06748543679714203\n",
            "Loss :  0.06676760599726722\n",
            "Loss :  0.06598162831317994\n",
            "Loss :  0.06712796484551779\n",
            "Loss :  0.06768392756873486\n",
            "Loss :  0.06791518103392398\n",
            "Loss :  0.06771883489170545\n",
            "Loss :  0.06802206757811853\n",
            "Loss :  0.06848257685919384\n",
            "Loss :  0.06830632336216398\n",
            "Loss :  0.06853855699971989\n",
            "Loss :  0.06860221669939924\n",
            "Loss :  0.06844291850241996\n",
            "Loss :  0.06848672362612494\n",
            "Loss :  0.06843967434783645\n",
            "Loss :  0.06839310609220718\n",
            "Loss :  0.06830866732879688\n",
            "Loss :  0.06827054800496575\n",
            "Loss :  0.06815744503474361\n",
            "Loss :  0.06812434548984712\n",
            "Loss :  0.06821982707316276\n",
            "Loss :  0.06816315617226908\n",
            "Loss :  0.06815140829844908\n",
            "Loss :  0.06828840831256011\n",
            "Loss :  0.06828650576897351\n",
            "Loss :  0.06823557772284723\n",
            "Loss :  0.06825625185761944\n",
            "Loss :  0.06833081801166738\n",
            "Loss :  0.06829764431540909\n",
            "Loss :  0.06829717989231265\n",
            "Loss :  0.06834181293941004\n",
            "Loss :  0.06821018014091569\n",
            "Loss :  0.0681857456005772\n",
            "Loss :  0.06826449614946793\n",
            "Loss :  0.06831925066235738\n",
            "Loss :  0.06840703886136454\n",
            "Loss :  0.06846969205173199\n",
            "Loss :  0.06860194977108888\n",
            "Loss :  0.06848979570790935\n",
            "Loss :  0.06844708242655692\n",
            "Loss :  0.06840823075015759\n",
            "Loss :  0.06845218011326291\n",
            "Loss :  0.06839529421448431\n",
            "Loss :  0.06843836126582963\n",
            "Loss :  0.06850430991360194\n",
            "Loss :  0.0685570097554661\n",
            "Loss :  0.0685932706574852\n",
            "Loss :  0.0685927043059016\n",
            "Loss :  0.06853368914024165\n",
            "Validation: \n",
            " Loss :  0.0651320368051529\n",
            " Loss :  0.06912419874043692\n",
            " Loss :  0.06805289245960189\n",
            " Loss :  0.06858505723906345\n",
            " Loss :  0.06810527662804097\n",
            "\n",
            "Epoch: 42\n",
            "Loss :  0.07330230623483658\n",
            "Loss :  0.06621151959354227\n",
            "Loss :  0.0651845107121127\n",
            "Loss :  0.06505544531729913\n",
            "Loss :  0.0658834708354822\n",
            "Loss :  0.06579609134910154\n",
            "Loss :  0.06614258940346905\n",
            "Loss :  0.06673473948743981\n",
            "Loss :  0.06681089866676448\n",
            "Loss :  0.06706555998259849\n",
            "Loss :  0.06719621130735567\n",
            "Loss :  0.06733934373200477\n",
            "Loss :  0.06729341446121861\n",
            "Loss :  0.06751457948721092\n",
            "Loss :  0.06780897873513242\n",
            "Loss :  0.0678878023924417\n",
            "Loss :  0.06769372865280009\n",
            "Loss :  0.06767963282546105\n",
            "Loss :  0.06767949651288722\n",
            "Loss :  0.06754468617404943\n",
            "Loss :  0.0675772820316737\n",
            "Loss :  0.06766447160057547\n",
            "Loss :  0.06764477373383164\n",
            "Loss :  0.06760418315083433\n",
            "Loss :  0.06759939948676533\n",
            "Loss :  0.06766465193484408\n",
            "Loss :  0.06764757252116313\n",
            "Loss :  0.06769456189276987\n",
            "Loss :  0.06767948999258547\n",
            "Loss :  0.06766290572244686\n",
            "Loss :  0.06764496251495178\n",
            "Loss :  0.06763842931897694\n",
            "Loss :  0.06751648694091125\n",
            "Loss :  0.06755454858534286\n",
            "Loss :  0.06765544287794846\n",
            "Loss :  0.06770846103331302\n",
            "Loss :  0.06776569562853209\n",
            "Loss :  0.06774994915707734\n",
            "Loss :  0.06778955923056039\n",
            "Loss :  0.06765389935020595\n",
            "Loss :  0.06766352783935028\n",
            "Loss :  0.06763893211772552\n",
            "Loss :  0.06765136175795576\n",
            "Loss :  0.06764600145049272\n",
            "Loss :  0.06777098944702116\n",
            "Loss :  0.06786022478190071\n",
            "Loss :  0.06791228417977853\n",
            "Loss :  0.06792230167744519\n",
            "Loss :  0.06793558211094872\n",
            "Loss :  0.0678966466466424\n",
            "Validation: \n",
            " Loss :  0.06989490985870361\n",
            " Loss :  0.06989369523667154\n",
            " Loss :  0.06902717608140736\n",
            " Loss :  0.06965148497800358\n",
            " Loss :  0.06891176256316679\n",
            "\n",
            "Epoch: 43\n",
            "Loss :  0.06848476082086563\n",
            "Loss :  0.06670919331637296\n",
            "Loss :  0.06616432077827908\n",
            "Loss :  0.0654693448976163\n",
            "Loss :  0.06572961716390238\n",
            "Loss :  0.06577375509283122\n",
            "Loss :  0.06607627435052982\n",
            "Loss :  0.06612257879804558\n",
            "Loss :  0.06633707577431644\n",
            "Loss :  0.06676059805757396\n",
            "Loss :  0.06661335791986768\n",
            "Loss :  0.06680842525921427\n",
            "Loss :  0.06677188247072795\n",
            "Loss :  0.06676583964633577\n",
            "Loss :  0.0668662650946607\n",
            "Loss :  0.06684254553933806\n",
            "Loss :  0.06679750134393295\n",
            "Loss :  0.06669104678763284\n",
            "Loss :  0.06675120725075184\n",
            "Loss :  0.0667144190227486\n",
            "Loss :  0.06671458686613918\n",
            "Loss :  0.06683984610706709\n",
            "Loss :  0.0669183113624877\n",
            "Loss :  0.06691697588214626\n",
            "Loss :  0.06695770954566378\n",
            "Loss :  0.06704338426252761\n",
            "Loss :  0.06702698925617097\n",
            "Loss :  0.0671052978240036\n",
            "Loss :  0.06709330913435098\n",
            "Loss :  0.06705854456756533\n",
            "Loss :  0.06699455390835918\n",
            "Loss :  0.0669861579391734\n",
            "Loss :  0.06691111119411816\n",
            "Loss :  0.06691362456702754\n",
            "Loss :  0.0669878961141508\n",
            "Loss :  0.06701165036513256\n",
            "Loss :  0.06703883221687702\n",
            "Loss :  0.06707497931153306\n",
            "Loss :  0.06716887737939677\n",
            "Loss :  0.06700382323559288\n",
            "Loss :  0.06699944139827517\n",
            "Loss :  0.06702962342803785\n",
            "Loss :  0.06702081655592364\n",
            "Loss :  0.06698248548359716\n",
            "Loss :  0.06703887084950935\n",
            "Loss :  0.06710682902691634\n",
            "Loss :  0.06718153383687882\n",
            "Loss :  0.06723021220755931\n",
            "Loss :  0.06722523932676305\n",
            "Loss :  0.0671493209245977\n",
            "Validation: \n",
            " Loss :  0.06108582019805908\n",
            " Loss :  0.0660037902139482\n",
            " Loss :  0.06573256468627511\n",
            " Loss :  0.0661716954019226\n",
            " Loss :  0.06583079994644647\n",
            "\n",
            "Epoch: 44\n",
            "Loss :  0.07352091372013092\n",
            "Loss :  0.06638894704255191\n",
            "Loss :  0.06564042681739443\n",
            "Loss :  0.06524418246361517\n",
            "Loss :  0.06588113280694659\n",
            "Loss :  0.06593165707354452\n",
            "Loss :  0.06613845671298074\n",
            "Loss :  0.0661503929697292\n",
            "Loss :  0.06659547989199191\n",
            "Loss :  0.06661641495404663\n",
            "Loss :  0.06649539722959594\n",
            "Loss :  0.06678543132436168\n",
            "Loss :  0.06645381687717004\n",
            "Loss :  0.06628724486204504\n",
            "Loss :  0.06645049109843605\n",
            "Loss :  0.06649438703790406\n",
            "Loss :  0.06649640299703764\n",
            "Loss :  0.06649694813970934\n",
            "Loss :  0.06659369265341628\n",
            "Loss :  0.06653200838890375\n",
            "Loss :  0.06656029162492919\n",
            "Loss :  0.06662979847376381\n",
            "Loss :  0.0665302476529622\n",
            "Loss :  0.066590723650151\n",
            "Loss :  0.06670699002708143\n",
            "Loss :  0.06678182281464695\n",
            "Loss :  0.06675325118547655\n",
            "Loss :  0.06680340416097114\n",
            "Loss :  0.06675134918708818\n",
            "Loss :  0.06671714634215299\n",
            "Loss :  0.06662840760031412\n",
            "Loss :  0.06662755779371\n",
            "Loss :  0.06653317370425875\n",
            "Loss :  0.0665325945822854\n",
            "Loss :  0.06661552007247276\n",
            "Loss :  0.06661800726017042\n",
            "Loss :  0.0666450465980329\n",
            "Loss :  0.06672126746603421\n",
            "Loss :  0.06683654035991571\n",
            "Loss :  0.06667958681121507\n",
            "Loss :  0.06667296418413854\n",
            "Loss :  0.06666062668038401\n",
            "Loss :  0.0666623269594905\n",
            "Loss :  0.06664151222965833\n",
            "Loss :  0.06673156853570959\n",
            "Loss :  0.06682328928567617\n",
            "Loss :  0.06690480100261674\n",
            "Loss :  0.06696117375908131\n",
            "Loss :  0.06694394020894213\n",
            "Loss :  0.06689826386189753\n",
            "Validation: \n",
            " Loss :  0.06273797154426575\n",
            " Loss :  0.06719243242627099\n",
            " Loss :  0.06714422146721584\n",
            " Loss :  0.06744140270547788\n",
            " Loss :  0.06698462394652543\n",
            "\n",
            "Epoch: 45\n",
            "Loss :  0.08399604260921478\n",
            "Loss :  0.06800128587267616\n",
            "Loss :  0.06602943085488819\n",
            "Loss :  0.0656834147389858\n",
            "Loss :  0.06622207346485882\n",
            "Loss :  0.06605636825164159\n",
            "Loss :  0.06629434814218615\n",
            "Loss :  0.06632073396738146\n",
            "Loss :  0.06632755816350748\n",
            "Loss :  0.0666833316886818\n",
            "Loss :  0.06648903981883927\n",
            "Loss :  0.06649655422514623\n",
            "Loss :  0.06622051804765197\n",
            "Loss :  0.0662911084999565\n",
            "Loss :  0.06641142138986723\n",
            "Loss :  0.0665220342181771\n",
            "Loss :  0.06654744634624593\n",
            "Loss :  0.06657812866376854\n",
            "Loss :  0.06655992260871671\n",
            "Loss :  0.06634806129710837\n",
            "Loss :  0.06632912564855903\n",
            "Loss :  0.06640280163471733\n",
            "Loss :  0.0663838013332354\n",
            "Loss :  0.06636203744839796\n",
            "Loss :  0.06649738217846982\n",
            "Loss :  0.06654429355584293\n",
            "Loss :  0.06656053556678852\n",
            "Loss :  0.06672368910699752\n",
            "Loss :  0.06682853824939591\n",
            "Loss :  0.06677934111342397\n",
            "Loss :  0.06668103965265411\n",
            "Loss :  0.06661009401610044\n",
            "Loss :  0.06647525168375062\n",
            "Loss :  0.06655903098671458\n",
            "Loss :  0.06659188715855746\n",
            "Loss :  0.06659863673766114\n",
            "Loss :  0.0665928672926908\n",
            "Loss :  0.06661854061797623\n",
            "Loss :  0.06667528469653267\n",
            "Loss :  0.0665579271476592\n",
            "Loss :  0.0665250825428606\n",
            "Loss :  0.06645750745659618\n",
            "Loss :  0.06647187566070545\n",
            "Loss :  0.0664645813388227\n",
            "Loss :  0.06651307237830832\n",
            "Loss :  0.06659639363245266\n",
            "Loss :  0.06663573673095982\n",
            "Loss :  0.06666166872387226\n",
            "Loss :  0.06668888763553635\n",
            "Loss :  0.06663376073184178\n",
            "Validation: \n",
            " Loss :  0.061559323221445084\n",
            " Loss :  0.06557125403057962\n",
            " Loss :  0.0649509549867816\n",
            " Loss :  0.0652931055328885\n",
            " Loss :  0.06480439264833192\n",
            "\n",
            "Epoch: 46\n",
            "Loss :  0.07764524966478348\n",
            "Loss :  0.06553203917362473\n",
            "Loss :  0.06538103911138717\n",
            "Loss :  0.06551469634136846\n",
            "Loss :  0.06633945518150562\n",
            "Loss :  0.06640812236012197\n",
            "Loss :  0.06621291732690374\n",
            "Loss :  0.06632764285928766\n",
            "Loss :  0.06634543040468369\n",
            "Loss :  0.06656808533020071\n",
            "Loss :  0.0663188633942368\n",
            "Loss :  0.06655341118305654\n",
            "Loss :  0.06622984300344444\n",
            "Loss :  0.06614626961127493\n",
            "Loss :  0.06635624566610823\n",
            "Loss :  0.06641394099751055\n",
            "Loss :  0.06623879081991889\n",
            "Loss :  0.06626855072222258\n",
            "Loss :  0.06622773083698684\n",
            "Loss :  0.06611417226142284\n",
            "Loss :  0.06609020843657094\n",
            "Loss :  0.06614313806049632\n",
            "Loss :  0.0660706731528718\n",
            "Loss :  0.06616376785488871\n",
            "Loss :  0.06633389786009472\n",
            "Loss :  0.0662673412417748\n",
            "Loss :  0.0661579874768796\n",
            "Loss :  0.0661298625336142\n",
            "Loss :  0.06611489863847499\n",
            "Loss :  0.06605964554697787\n",
            "Loss :  0.0659594513055098\n",
            "Loss :  0.06594607882033973\n",
            "Loss :  0.06580881126851679\n",
            "Loss :  0.06581269609784071\n",
            "Loss :  0.06589016867828859\n",
            "Loss :  0.0659384430410006\n",
            "Loss :  0.0659827231287626\n",
            "Loss :  0.065999783113639\n",
            "Loss :  0.06607922407116477\n",
            "Loss :  0.06594580661534044\n",
            "Loss :  0.0659142929103755\n",
            "Loss :  0.06589759840944967\n",
            "Loss :  0.06586652399525224\n",
            "Loss :  0.06583356210776106\n",
            "Loss :  0.06588284911211927\n",
            "Loss :  0.06593053246086295\n",
            "Loss :  0.06598668634374333\n",
            "Loss :  0.06604583729098051\n",
            "Loss :  0.06603647709636332\n",
            "Loss :  0.06601124815263476\n",
            "Validation: \n",
            " Loss :  0.062441594898700714\n",
            " Loss :  0.06727426499128342\n",
            " Loss :  0.06706889137262251\n",
            " Loss :  0.06728001812198123\n",
            " Loss :  0.06691467430856493\n",
            "\n",
            "Epoch: 47\n",
            "Loss :  0.0739850252866745\n",
            "Loss :  0.06451959670944647\n",
            "Loss :  0.06455670013314202\n",
            "Loss :  0.06461462294382433\n",
            "Loss :  0.06512195716907339\n",
            "Loss :  0.06541073921264387\n",
            "Loss :  0.06538543868504587\n",
            "Loss :  0.06527174946287988\n",
            "Loss :  0.06508480430937108\n",
            "Loss :  0.06539410315863378\n",
            "Loss :  0.06541451546225217\n",
            "Loss :  0.06548605684761528\n",
            "Loss :  0.06521029290088937\n",
            "Loss :  0.06519354225797508\n",
            "Loss :  0.06541472406569102\n",
            "Loss :  0.06547959067470191\n",
            "Loss :  0.06543556589239873\n",
            "Loss :  0.06546546104881498\n",
            "Loss :  0.06538390724286848\n",
            "Loss :  0.06530744139670702\n",
            "Loss :  0.06530231293012846\n",
            "Loss :  0.06527472940666415\n",
            "Loss :  0.06535267371397752\n",
            "Loss :  0.06533390409373618\n",
            "Loss :  0.06538237957477075\n",
            "Loss :  0.065441371954651\n",
            "Loss :  0.06542005551215332\n",
            "Loss :  0.06553282915886038\n",
            "Loss :  0.06553831602383763\n",
            "Loss :  0.06553440515425607\n",
            "Loss :  0.06545493818596748\n",
            "Loss :  0.06546189584484821\n",
            "Loss :  0.06534955159665268\n",
            "Loss :  0.06536763035368703\n",
            "Loss :  0.0655361294659002\n",
            "Loss :  0.065591859978828\n",
            "Loss :  0.06566472118333436\n",
            "Loss :  0.06570732094488053\n",
            "Loss :  0.06579700959088608\n",
            "Loss :  0.06564557528518655\n",
            "Loss :  0.06560869304988153\n",
            "Loss :  0.06556360485199884\n",
            "Loss :  0.06555679779402433\n",
            "Loss :  0.06555606834504002\n",
            "Loss :  0.06562120859382374\n",
            "Loss :  0.06570040155408652\n",
            "Loss :  0.06574247120485648\n",
            "Loss :  0.06579716020360621\n",
            "Loss :  0.06579069271993489\n",
            "Loss :  0.06572424385947026\n",
            "Validation: \n",
            " Loss :  0.060444898903369904\n",
            " Loss :  0.06578992981286276\n",
            " Loss :  0.06546566217410855\n",
            " Loss :  0.06594184224234252\n",
            " Loss :  0.06563697870314857\n",
            "\n",
            "Epoch: 48\n",
            "Loss :  0.07456988096237183\n",
            "Loss :  0.06305161321705038\n",
            "Loss :  0.06270378739351318\n",
            "Loss :  0.0629814323638716\n",
            "Loss :  0.06362662391691673\n",
            "Loss :  0.06380933822662223\n",
            "Loss :  0.06387618245159993\n",
            "Loss :  0.06404044872648279\n",
            "Loss :  0.06431185670288993\n",
            "Loss :  0.06450524046034603\n",
            "Loss :  0.06430735671431712\n",
            "Loss :  0.06443382239153793\n",
            "Loss :  0.06422952350136663\n",
            "Loss :  0.06416301494677558\n",
            "Loss :  0.06424795183623937\n",
            "Loss :  0.06433476243686202\n",
            "Loss :  0.06416199869991089\n",
            "Loss :  0.06428920932941967\n",
            "Loss :  0.06429242087497238\n",
            "Loss :  0.06422367298010132\n",
            "Loss :  0.06414798441440311\n",
            "Loss :  0.06421981017453976\n",
            "Loss :  0.06423718455285508\n",
            "Loss :  0.06427895616401326\n",
            "Loss :  0.06434251015611704\n",
            "Loss :  0.0644325715493396\n",
            "Loss :  0.06445984423902756\n",
            "Loss :  0.06445266604698452\n",
            "Loss :  0.0644540214824931\n",
            "Loss :  0.06443554541908998\n",
            "Loss :  0.06438172191529971\n",
            "Loss :  0.06442365767823538\n",
            "Loss :  0.06438251135644511\n",
            "Loss :  0.06440942480124735\n",
            "Loss :  0.06451003477414333\n",
            "Loss :  0.06452303112317355\n",
            "Loss :  0.06459385489946917\n",
            "Loss :  0.06458582534301635\n",
            "Loss :  0.06465717084373389\n",
            "Loss :  0.06456996582429428\n",
            "Loss :  0.06457396622682153\n",
            "Loss :  0.06455108442699532\n",
            "Loss :  0.064628559786322\n",
            "Loss :  0.06464577704806339\n",
            "Loss :  0.06471875266017828\n",
            "Loss :  0.0648179238459752\n",
            "Loss :  0.06486775048093268\n",
            "Loss :  0.06487459660905182\n",
            "Loss :  0.06485062781837526\n",
            "Loss :  0.06476493157036678\n",
            "Validation: \n",
            " Loss :  0.06028124690055847\n",
            " Loss :  0.06533824333122798\n",
            " Loss :  0.06478971279248959\n",
            " Loss :  0.06507412252611801\n",
            " Loss :  0.0646369671104131\n",
            "\n",
            "Epoch: 49\n",
            "Loss :  0.07201311737298965\n",
            "Loss :  0.06335549699989232\n",
            "Loss :  0.06288662091607139\n",
            "Loss :  0.06308514805090043\n",
            "Loss :  0.06391255948238256\n",
            "Loss :  0.06415434145167762\n",
            "Loss :  0.0641290690810954\n",
            "Loss :  0.06444774977338145\n",
            "Loss :  0.0645710367847372\n",
            "Loss :  0.06480664796717875\n",
            "Loss :  0.0646679019278819\n",
            "Loss :  0.06472805796845539\n",
            "Loss :  0.06466935507275841\n",
            "Loss :  0.06465302322429554\n",
            "Loss :  0.06491627433198563\n",
            "Loss :  0.06493084860459858\n",
            "Loss :  0.06495268307394864\n",
            "Loss :  0.06488262452403007\n",
            "Loss :  0.06490046568009077\n",
            "Loss :  0.0647547750767925\n",
            "Loss :  0.06472442571574183\n",
            "Loss :  0.06481787608274352\n",
            "Loss :  0.06481674124752235\n",
            "Loss :  0.06477067576019795\n",
            "Loss :  0.06475632926796976\n",
            "Loss :  0.06480608402614099\n",
            "Loss :  0.0647747394882171\n",
            "Loss :  0.06478276322303662\n",
            "Loss :  0.06483126967296905\n",
            "Loss :  0.06480213258530676\n",
            "Loss :  0.06480191249487012\n",
            "Loss :  0.06474704950616673\n",
            "Loss :  0.06461254806719094\n",
            "Loss :  0.06468287634795526\n",
            "Loss :  0.0647888144155926\n",
            "Loss :  0.06479481450597785\n",
            "Loss :  0.0648475962580076\n",
            "Loss :  0.06483454346616635\n",
            "Loss :  0.06491265540790996\n",
            "Loss :  0.06479731927175655\n",
            "Loss :  0.06476199343577585\n",
            "Loss :  0.06471498455589415\n",
            "Loss :  0.06470510475072046\n",
            "Loss :  0.0646420953030376\n",
            "Loss :  0.06475693509489501\n",
            "Loss :  0.06482937140700029\n",
            "Loss :  0.06483982235128606\n",
            "Loss :  0.06483566624185588\n",
            "Loss :  0.06483046046754426\n",
            "Loss :  0.0647624697680386\n",
            "Validation: \n",
            " Loss :  0.062293119728565216\n",
            " Loss :  0.06599150190041178\n",
            " Loss :  0.06516341837804492\n",
            " Loss :  0.06580151348817544\n",
            " Loss :  0.06528063284026252\n",
            "\n",
            "Epoch: 50\n",
            "Loss :  0.07169703394174576\n",
            "Loss :  0.0641271478750489\n",
            "Loss :  0.06410177460029012\n",
            "Loss :  0.06394990942170543\n",
            "Loss :  0.06423131721775706\n",
            "Loss :  0.06454503360916586\n",
            "Loss :  0.06453771492252584\n",
            "Loss :  0.06437805578322478\n",
            "Loss :  0.06430239028032915\n",
            "Loss :  0.06439458030265766\n",
            "Loss :  0.0643359364775738\n",
            "Loss :  0.06448319483850454\n",
            "Loss :  0.06425875914860363\n",
            "Loss :  0.06421681544707931\n",
            "Loss :  0.06425887454591744\n",
            "Loss :  0.06432347347503466\n",
            "Loss :  0.06425497868323918\n",
            "Loss :  0.0642486845976428\n",
            "Loss :  0.0642675320046712\n",
            "Loss :  0.06413985751764313\n",
            "Loss :  0.06404838019713241\n",
            "Loss :  0.06410149645536997\n",
            "Loss :  0.06403314844785232\n",
            "Loss :  0.06396623308975975\n",
            "Loss :  0.06408283927504947\n",
            "Loss :  0.06408895558095073\n",
            "Loss :  0.0640700143901096\n",
            "Loss :  0.06412331148212247\n",
            "Loss :  0.0642070897533376\n",
            "Loss :  0.06424576835226767\n",
            "Loss :  0.0642367086908548\n",
            "Loss :  0.06421221820248285\n",
            "Loss :  0.06408378546854417\n",
            "Loss :  0.06409590702582703\n",
            "Loss :  0.06420460131423564\n",
            "Loss :  0.06420055747117072\n",
            "Loss :  0.06423284438467092\n",
            "Loss :  0.06420721050422147\n",
            "Loss :  0.0642648552326862\n",
            "Loss :  0.0641467285144817\n",
            "Loss :  0.06409582415795385\n",
            "Loss :  0.0640806216664993\n",
            "Loss :  0.0640490390802789\n",
            "Loss :  0.06402188944014485\n",
            "Loss :  0.06407245311911414\n",
            "Loss :  0.06410809383291892\n",
            "Loss :  0.06415624788875435\n",
            "Loss :  0.06417076944545576\n",
            "Loss :  0.06419576773431594\n",
            "Loss :  0.0641695959171064\n",
            "Validation: \n",
            " Loss :  0.06405892968177795\n",
            " Loss :  0.06647015611330669\n",
            " Loss :  0.06596678804333617\n",
            " Loss :  0.06642387422626136\n",
            " Loss :  0.06596455644862151\n",
            "\n",
            "Epoch: 51\n",
            "Loss :  0.06582628190517426\n",
            "Loss :  0.062039846046404404\n",
            "Loss :  0.06214805497300057\n",
            "Loss :  0.06268987408088099\n",
            "Loss :  0.06314145964456767\n",
            "Loss :  0.06318431785878013\n",
            "Loss :  0.06369729413360846\n",
            "Loss :  0.06367820422624199\n",
            "Loss :  0.06361508778767822\n",
            "Loss :  0.06388329202820967\n",
            "Loss :  0.06380962473478648\n",
            "Loss :  0.06383795865081451\n",
            "Loss :  0.0635921227599471\n",
            "Loss :  0.0635467297077634\n",
            "Loss :  0.06376064682366155\n",
            "Loss :  0.06379668866068322\n",
            "Loss :  0.06370168910448595\n",
            "Loss :  0.06377829552481049\n",
            "Loss :  0.06371533627832793\n",
            "Loss :  0.06364358573449844\n",
            "Loss :  0.06367838542111477\n",
            "Loss :  0.0636743487551879\n",
            "Loss :  0.06374043517859813\n",
            "Loss :  0.0637494120827485\n",
            "Loss :  0.06376140855283657\n",
            "Loss :  0.0638319752160534\n",
            "Loss :  0.06376432541800642\n",
            "Loss :  0.06379111468572018\n",
            "Loss :  0.06384600550466585\n",
            "Loss :  0.06386772444618936\n",
            "Loss :  0.06388782792758704\n",
            "Loss :  0.06378429595057604\n",
            "Loss :  0.06365655902250905\n",
            "Loss :  0.06365511441473874\n",
            "Loss :  0.06374696161477797\n",
            "Loss :  0.06374718052496937\n",
            "Loss :  0.06378795726642714\n",
            "Loss :  0.06384262029894922\n",
            "Loss :  0.06391067194735284\n",
            "Loss :  0.0638024905301116\n",
            "Loss :  0.0637512306062658\n",
            "Loss :  0.0637347648994331\n",
            "Loss :  0.0637514100381286\n",
            "Loss :  0.06372397218392233\n",
            "Loss :  0.06380228188032466\n",
            "Loss :  0.06387351198894221\n",
            "Loss :  0.06393414559630665\n",
            "Loss :  0.06393725341957086\n",
            "Loss :  0.06392469046455411\n",
            "Loss :  0.06386561853367297\n",
            "Validation: \n",
            " Loss :  0.05863024666905403\n",
            " Loss :  0.06449641287326813\n",
            " Loss :  0.06435695572233782\n",
            " Loss :  0.06507148471523504\n",
            " Loss :  0.06462227318573881\n",
            "\n",
            "Epoch: 52\n",
            "Loss :  0.07413016259670258\n",
            "Loss :  0.06307892738418146\n",
            "Loss :  0.0629573640014444\n",
            "Loss :  0.06288506619391902\n",
            "Loss :  0.063550059570045\n",
            "Loss :  0.06313519742266804\n",
            "Loss :  0.06283547100229342\n",
            "Loss :  0.06269029165867349\n",
            "Loss :  0.0630159379431495\n",
            "Loss :  0.06319171366292042\n",
            "Loss :  0.06311491696108686\n",
            "Loss :  0.06328760929875546\n",
            "Loss :  0.06329453040745632\n",
            "Loss :  0.06314861234135301\n",
            "Loss :  0.0632106085829701\n",
            "Loss :  0.06326257018064031\n",
            "Loss :  0.0632380429215683\n",
            "Loss :  0.06338443456773173\n",
            "Loss :  0.06335926346110375\n",
            "Loss :  0.06307398841877258\n",
            "Loss :  0.06310378181845394\n",
            "Loss :  0.06315565619494112\n",
            "Loss :  0.06317921212086311\n",
            "Loss :  0.06316819029065954\n",
            "Loss :  0.0632086363138005\n",
            "Loss :  0.0632724368952185\n",
            "Loss :  0.06315583196179621\n",
            "Loss :  0.06319329053218514\n",
            "Loss :  0.0631374924367433\n",
            "Loss :  0.06310131985860593\n",
            "Loss :  0.06304107086603032\n",
            "Loss :  0.06301526877418208\n",
            "Loss :  0.06293920913821441\n",
            "Loss :  0.06287486056636828\n",
            "Loss :  0.06296249984864616\n",
            "Loss :  0.06298304495648441\n",
            "Loss :  0.06302970401179098\n",
            "Loss :  0.0631102008940878\n",
            "Loss :  0.06320973631395442\n",
            "Loss :  0.06312643177330951\n",
            "Loss :  0.06311199087603134\n",
            "Loss :  0.06310764687484778\n",
            "Loss :  0.06310359828368785\n",
            "Loss :  0.06307310606687916\n",
            "Loss :  0.06315162109713705\n",
            "Loss :  0.06322709830300242\n",
            "Loss :  0.06329819366109604\n",
            "Loss :  0.06335604472398251\n",
            "Loss :  0.06335093194072806\n",
            "Loss :  0.06332030421968876\n",
            "Validation: \n",
            " Loss :  0.06319908797740936\n",
            " Loss :  0.06463670304843358\n",
            " Loss :  0.06363419852242237\n",
            " Loss :  0.06395548105728431\n",
            " Loss :  0.06357142991489834\n",
            "\n",
            "Epoch: 53\n",
            "Loss :  0.07289068400859833\n",
            "Loss :  0.06199113008650867\n",
            "Loss :  0.06125881930901891\n",
            "Loss :  0.0609509042434154\n",
            "Loss :  0.06155586042782155\n",
            "Loss :  0.06182352073636709\n",
            "Loss :  0.06197595608527543\n",
            "Loss :  0.062112505849398356\n",
            "Loss :  0.06232066015586441\n",
            "Loss :  0.06258822752387969\n",
            "Loss :  0.062408846498715996\n",
            "Loss :  0.06262509971185848\n",
            "Loss :  0.062439137715692365\n",
            "Loss :  0.062428403901689834\n",
            "Loss :  0.06265469458191952\n",
            "Loss :  0.06277799808623775\n",
            "Loss :  0.06270616126726873\n",
            "Loss :  0.06278322046698882\n",
            "Loss :  0.0628513420895977\n",
            "Loss :  0.06275305267013805\n",
            "Loss :  0.06274043307153147\n",
            "Loss :  0.06278638084423485\n",
            "Loss :  0.06280441674913756\n",
            "Loss :  0.06285038359818004\n",
            "Loss :  0.0629489458015351\n",
            "Loss :  0.0629860168820121\n",
            "Loss :  0.06297493867319205\n",
            "Loss :  0.06313840826334109\n",
            "Loss :  0.06315453260394602\n",
            "Loss :  0.0631287209817634\n",
            "Loss :  0.06309704913253404\n",
            "Loss :  0.06311061881314903\n",
            "Loss :  0.06300167495680746\n",
            "Loss :  0.06304338037562514\n",
            "Loss :  0.06313434578587582\n",
            "Loss :  0.06316370733528055\n",
            "Loss :  0.06318887379897599\n",
            "Loss :  0.06317612938482485\n",
            "Loss :  0.06324172110890779\n",
            "Loss :  0.06315434939416169\n",
            "Loss :  0.06311541830103594\n",
            "Loss :  0.06306596164684516\n",
            "Loss :  0.06306058332996527\n",
            "Loss :  0.06308047992896867\n",
            "Loss :  0.06310672940614542\n",
            "Loss :  0.06319339969866555\n",
            "Loss :  0.0632537154808091\n",
            "Loss :  0.06327454279119549\n",
            "Loss :  0.06325080962135242\n",
            "Loss :  0.06318623012656349\n",
            "Validation: \n",
            " Loss :  0.06007695943117142\n",
            " Loss :  0.06500036588736943\n",
            " Loss :  0.06446268618470286\n",
            " Loss :  0.06503862832657627\n",
            " Loss :  0.06472603102893006\n",
            "\n",
            "Epoch: 54\n",
            "Loss :  0.06602583080530167\n",
            "Loss :  0.06203151765194806\n",
            "Loss :  0.06211198511577788\n",
            "Loss :  0.06200323049579897\n",
            "Loss :  0.062353708395143835\n",
            "Loss :  0.06247447190039298\n",
            "Loss :  0.06253493058144069\n",
            "Loss :  0.06261816799220904\n",
            "Loss :  0.06266259268662076\n",
            "Loss :  0.06283934799196957\n",
            "Loss :  0.06275823712348938\n",
            "Loss :  0.06284599341787733\n",
            "Loss :  0.0627160801683083\n",
            "Loss :  0.06266945856445619\n",
            "Loss :  0.0628356489729374\n",
            "Loss :  0.06298210565617543\n",
            "Loss :  0.06291268590074148\n",
            "Loss :  0.06280884717465841\n",
            "Loss :  0.06271702917652894\n",
            "Loss :  0.06269532408935861\n",
            "Loss :  0.06271099738443076\n",
            "Loss :  0.06278495201002365\n",
            "Loss :  0.06273161302999133\n",
            "Loss :  0.0626995490613954\n",
            "Loss :  0.06271393951105873\n",
            "Loss :  0.06268944506685573\n",
            "Loss :  0.06261866778078207\n",
            "Loss :  0.06265888435979171\n",
            "Loss :  0.06268116243157099\n",
            "Loss :  0.06271160745999657\n",
            "Loss :  0.0626406296097559\n",
            "Loss :  0.06261886350834485\n",
            "Loss :  0.06252473325837067\n",
            "Loss :  0.06255145393469543\n",
            "Loss :  0.06263442365916944\n",
            "Loss :  0.06259248978816546\n",
            "Loss :  0.06260853290681694\n",
            "Loss :  0.06264407387520425\n",
            "Loss :  0.06274983433522577\n",
            "Loss :  0.06262441545419986\n",
            "Loss :  0.06256872658940622\n",
            "Loss :  0.06256621233992044\n",
            "Loss :  0.06257037528195461\n",
            "Loss :  0.06257582182743157\n",
            "Loss :  0.06267532204486886\n",
            "Loss :  0.06273904864876603\n",
            "Loss :  0.06280825903675562\n",
            "Loss :  0.06280073557531504\n",
            "Loss :  0.06280499687437704\n",
            "Loss :  0.0627540765584365\n",
            "Validation: \n",
            " Loss :  0.06395284086465836\n",
            " Loss :  0.06614968216135389\n",
            " Loss :  0.06551544195631655\n",
            " Loss :  0.06598464772105217\n",
            " Loss :  0.06553959818901839\n",
            "\n",
            "Epoch: 55\n",
            "Loss :  0.06576170772314072\n",
            "Loss :  0.061470627446066246\n",
            "Loss :  0.05989354387635276\n",
            "Loss :  0.060419895956593174\n",
            "Loss :  0.06103827958790267\n",
            "Loss :  0.06112362385964861\n",
            "Loss :  0.061066699992926396\n",
            "Loss :  0.06109158605547019\n",
            "Loss :  0.061331699705786176\n",
            "Loss :  0.06167748066913951\n",
            "Loss :  0.06167813272464393\n",
            "Loss :  0.06181711821003003\n",
            "Loss :  0.061555839643990695\n",
            "Loss :  0.061481392992134315\n",
            "Loss :  0.061665865810627635\n",
            "Loss :  0.06166830404803453\n",
            "Loss :  0.06166740823884188\n",
            "Loss :  0.06174038779752993\n",
            "Loss :  0.0617617908968122\n",
            "Loss :  0.061825155743753725\n",
            "Loss :  0.06181995641088011\n",
            "Loss :  0.06179379320469513\n",
            "Loss :  0.06185900529517847\n",
            "Loss :  0.061859003876968896\n",
            "Loss :  0.06195145984531933\n",
            "Loss :  0.06195550354056624\n",
            "Loss :  0.06192509463178244\n",
            "Loss :  0.06195039480814635\n",
            "Loss :  0.061965434391091305\n",
            "Loss :  0.06199629460157398\n",
            "Loss :  0.061958241056762266\n",
            "Loss :  0.06197934160518109\n",
            "Loss :  0.061905018114876524\n",
            "Loss :  0.061956732643604996\n",
            "Loss :  0.062025726267581814\n",
            "Loss :  0.0620310987608555\n",
            "Loss :  0.062068945503796234\n",
            "Loss :  0.06214970076140368\n",
            "Loss :  0.062230709373090526\n",
            "Loss :  0.06210584336382044\n",
            "Loss :  0.06208479447183466\n",
            "Loss :  0.06209954396434074\n",
            "Loss :  0.0620998997338594\n",
            "Loss :  0.06209680385317161\n",
            "Loss :  0.06217581108043524\n",
            "Loss :  0.06222497545463547\n",
            "Loss :  0.062286792189658595\n",
            "Loss :  0.06231877337468911\n",
            "Loss :  0.06232583690860663\n",
            "Loss :  0.062278281366205994\n",
            "Validation: \n",
            " Loss :  0.05792062729597092\n",
            " Loss :  0.06542308965609187\n",
            " Loss :  0.06469887386008007\n",
            " Loss :  0.06511352479946418\n",
            " Loss :  0.06477277026868161\n",
            "\n",
            "Epoch: 56\n",
            "Loss :  0.07065847516059875\n",
            "Loss :  0.0627027726308866\n",
            "Loss :  0.06295219578203701\n",
            "Loss :  0.06211529888453022\n",
            "Loss :  0.061581889485440605\n",
            "Loss :  0.061381957986775565\n",
            "Loss :  0.061724027588230666\n",
            "Loss :  0.06209839366271462\n",
            "Loss :  0.06228416716610944\n",
            "Loss :  0.06254788086964534\n",
            "Loss :  0.06247314910339837\n",
            "Loss :  0.06241838771614942\n",
            "Loss :  0.06219740196569892\n",
            "Loss :  0.06226051783629956\n",
            "Loss :  0.06223221936969892\n",
            "Loss :  0.06222409747590292\n",
            "Loss :  0.062121537554523217\n",
            "Loss :  0.062169000702468974\n",
            "Loss :  0.062172709907616044\n",
            "Loss :  0.06205211554444273\n",
            "Loss :  0.06193128155905809\n",
            "Loss :  0.06188481664770587\n",
            "Loss :  0.06183284814406304\n",
            "Loss :  0.061852063903044825\n",
            "Loss :  0.06190291023909798\n",
            "Loss :  0.06196027485082349\n",
            "Loss :  0.06196856621467291\n",
            "Loss :  0.062120620889857245\n",
            "Loss :  0.062095781092223745\n",
            "Loss :  0.06206828514530077\n",
            "Loss :  0.06201689841343319\n",
            "Loss :  0.06195436830114322\n",
            "Loss :  0.06186973539143337\n",
            "Loss :  0.06191668651570366\n",
            "Loss :  0.06204649588623005\n",
            "Loss :  0.062073302903661025\n",
            "Loss :  0.0620896393469331\n",
            "Loss :  0.06209362867587018\n",
            "Loss :  0.062203425904271484\n",
            "Loss :  0.062078783340999845\n",
            "Loss :  0.06201865637391583\n",
            "Loss :  0.061991754565795844\n",
            "Loss :  0.06203770806521815\n",
            "Loss :  0.06199119453536662\n",
            "Loss :  0.06201410674555502\n",
            "Loss :  0.06209182344426601\n",
            "Loss :  0.06214904257474132\n",
            "Loss :  0.06219084144049895\n",
            "Loss :  0.06218636896376055\n",
            "Loss :  0.06212996412258284\n",
            "Validation: \n",
            " Loss :  0.0597526840865612\n",
            " Loss :  0.06504535852443605\n",
            " Loss :  0.06422653467189975\n",
            " Loss :  0.06490928997270397\n",
            " Loss :  0.06441809296791937\n",
            "\n",
            "Epoch: 57\n",
            "Loss :  0.06392676383256912\n",
            "Loss :  0.060189531269398605\n",
            "Loss :  0.060643830469676425\n",
            "Loss :  0.06173392901978185\n",
            "Loss :  0.061563548245807974\n",
            "Loss :  0.061508333127872614\n",
            "Loss :  0.06142994443901249\n",
            "Loss :  0.06128508303786667\n",
            "Loss :  0.061427194717121714\n",
            "Loss :  0.06162565242458176\n",
            "Loss :  0.06156762261496912\n",
            "Loss :  0.061666586008426304\n",
            "Loss :  0.06152019554302712\n",
            "Loss :  0.061518093101850906\n",
            "Loss :  0.06163641947803768\n",
            "Loss :  0.061687963342429784\n",
            "Loss :  0.061713914600958736\n",
            "Loss :  0.06170141808035081\n",
            "Loss :  0.061716585178236937\n",
            "Loss :  0.061571646481752396\n",
            "Loss :  0.061598650036166554\n",
            "Loss :  0.06159659754043507\n",
            "Loss :  0.06150925528612072\n",
            "Loss :  0.06146920751377102\n",
            "Loss :  0.06147743061308544\n",
            "Loss :  0.0615477799122552\n",
            "Loss :  0.06158196914698429\n",
            "Loss :  0.06163008898716571\n",
            "Loss :  0.06163687639442203\n",
            "Loss :  0.06157537693112986\n",
            "Loss :  0.061500382420925606\n",
            "Loss :  0.06146350426305912\n",
            "Loss :  0.061380446781157706\n",
            "Loss :  0.06142727210217372\n",
            "Loss :  0.06151695041493936\n",
            "Loss :  0.06152770423099526\n",
            "Loss :  0.06155567725609544\n",
            "Loss :  0.061590419755872046\n",
            "Loss :  0.061631385755194765\n",
            "Loss :  0.06148941551937776\n",
            "Loss :  0.061471793215322375\n",
            "Loss :  0.06148585055358798\n",
            "Loss :  0.061463804274018756\n",
            "Loss :  0.06144267379407816\n",
            "Loss :  0.06150432019716218\n",
            "Loss :  0.061555050024370395\n",
            "Loss :  0.061610657034882235\n",
            "Loss :  0.061617619269023274\n",
            "Loss :  0.061629261225152165\n",
            "Loss :  0.06158078897387821\n",
            "Validation: \n",
            " Loss :  0.059825822710990906\n",
            " Loss :  0.06501324226458867\n",
            " Loss :  0.06420507827183096\n",
            " Loss :  0.06466572640127823\n",
            " Loss :  0.06408376076523169\n",
            "\n",
            "Epoch: 58\n",
            "Loss :  0.07318969815969467\n",
            "Loss :  0.06053066829388792\n",
            "Loss :  0.060662001016594115\n",
            "Loss :  0.06010773453500963\n",
            "Loss :  0.06064804697909006\n",
            "Loss :  0.06088292971253395\n",
            "Loss :  0.06080975847654655\n",
            "Loss :  0.06102906561023753\n",
            "Loss :  0.061083411820876746\n",
            "Loss :  0.06153193238999818\n",
            "Loss :  0.06119604220632279\n",
            "Loss :  0.06137837023214177\n",
            "Loss :  0.061190586290822545\n",
            "Loss :  0.06126156224663021\n",
            "Loss :  0.0613903708438924\n",
            "Loss :  0.061416459917430055\n",
            "Loss :  0.061359897047651484\n",
            "Loss :  0.06132384651062781\n",
            "Loss :  0.0613001162258301\n",
            "Loss :  0.06124897104205261\n",
            "Loss :  0.061236955318136596\n",
            "Loss :  0.06131101629180366\n",
            "Loss :  0.06129445341242924\n",
            "Loss :  0.0612874911590056\n",
            "Loss :  0.06132935514650404\n",
            "Loss :  0.06141015562225148\n",
            "Loss :  0.06145320969751512\n",
            "Loss :  0.06151904703231315\n",
            "Loss :  0.06145378865062978\n",
            "Loss :  0.06139998093145\n",
            "Loss :  0.061344014610662016\n",
            "Loss :  0.061326159326018245\n",
            "Loss :  0.06126031201808623\n",
            "Loss :  0.06125376163067414\n",
            "Loss :  0.061313283823627183\n",
            "Loss :  0.061283594671307805\n",
            "Loss :  0.06135481651717606\n",
            "Loss :  0.06135267506270717\n",
            "Loss :  0.06141928765248126\n",
            "Loss :  0.061317731657296495\n",
            "Loss :  0.0612752179050832\n",
            "Loss :  0.06124077656662087\n",
            "Loss :  0.061229362738741265\n",
            "Loss :  0.06122512116233047\n",
            "Loss :  0.06126562563402582\n",
            "Loss :  0.06135082622358117\n",
            "Loss :  0.061450246018468685\n",
            "Loss :  0.06145625612886342\n",
            "Loss :  0.06145451987731482\n",
            "Loss :  0.061396653215778335\n",
            "Validation: \n",
            " Loss :  0.05798383429646492\n",
            " Loss :  0.06301040929697808\n",
            " Loss :  0.06270130932694529\n",
            " Loss :  0.06300213181825935\n",
            " Loss :  0.06246396821038223\n",
            "\n",
            "Epoch: 59\n",
            "Loss :  0.07114367187023163\n",
            "Loss :  0.06066726622256366\n",
            "Loss :  0.06013901141427812\n",
            "Loss :  0.06022738052471992\n",
            "Loss :  0.0609993243181124\n",
            "Loss :  0.06083942745246139\n",
            "Loss :  0.06086456555812085\n",
            "Loss :  0.06105631264582486\n",
            "Loss :  0.061065159232160195\n",
            "Loss :  0.061430757476405785\n",
            "Loss :  0.061271903317163486\n",
            "Loss :  0.06127137425649273\n",
            "Loss :  0.06119261835971154\n",
            "Loss :  0.06114686510831345\n",
            "Loss :  0.06125074178508833\n",
            "Loss :  0.061381254345178604\n",
            "Loss :  0.061255443933772746\n",
            "Loss :  0.061213095590733645\n",
            "Loss :  0.061181424957447945\n",
            "Loss :  0.060996852230027085\n",
            "Loss :  0.06099197123922519\n",
            "Loss :  0.06098469701635329\n",
            "Loss :  0.060904947189588894\n",
            "Loss :  0.06094615387194084\n",
            "Loss :  0.060995661881578414\n",
            "Loss :  0.06101513884041414\n",
            "Loss :  0.06096160231993116\n",
            "Loss :  0.06096179093272044\n",
            "Loss :  0.06090642946682791\n",
            "Loss :  0.06093821453116194\n",
            "Loss :  0.06092269728548107\n",
            "Loss :  0.060920376881621656\n",
            "Loss :  0.06082829945601778\n",
            "Loss :  0.06084248189479563\n",
            "Loss :  0.060876190181701414\n",
            "Loss :  0.060892463643156904\n",
            "Loss :  0.06091951686176897\n",
            "Loss :  0.06092656668868026\n",
            "Loss :  0.060959086389209965\n",
            "Loss :  0.06086041593490659\n",
            "Loss :  0.06084647387303319\n",
            "Loss :  0.060839435334913344\n",
            "Loss :  0.06083200729480832\n",
            "Loss :  0.060800776084379365\n",
            "Loss :  0.06086753949928446\n",
            "Loss :  0.06089883977485337\n",
            "Loss :  0.060936655154261825\n",
            "Loss :  0.060991832357683\n",
            "Loss :  0.06100392404347348\n",
            "Loss :  0.06095264357364833\n",
            "Validation: \n",
            " Loss :  0.05979669466614723\n",
            " Loss :  0.06399730752621378\n",
            " Loss :  0.06340723929972183\n",
            " Loss :  0.06395990328222025\n",
            " Loss :  0.06343527906286864\n",
            "\n",
            "Epoch: 60\n",
            "Loss :  0.062275514006614685\n",
            "Loss :  0.057587711309844795\n",
            "Loss :  0.057850091939880735\n",
            "Loss :  0.05755495892897729\n",
            "Loss :  0.05869796535954243\n",
            "Loss :  0.0588745309733877\n",
            "Loss :  0.05912939251446333\n",
            "Loss :  0.059497838131558727\n",
            "Loss :  0.05967723518426036\n",
            "Loss :  0.05992452079778189\n",
            "Loss :  0.05996874723546576\n",
            "Loss :  0.060025171955695025\n",
            "Loss :  0.05992186549773886\n",
            "Loss :  0.05988176738601605\n",
            "Loss :  0.06005363235025541\n",
            "Loss :  0.06008191658368963\n",
            "Loss :  0.06003924755391127\n",
            "Loss :  0.0600690345405138\n",
            "Loss :  0.06016029880603374\n",
            "Loss :  0.06015029629565658\n",
            "Loss :  0.06018794268666215\n",
            "Loss :  0.060219990133674225\n",
            "Loss :  0.06019400614756265\n",
            "Loss :  0.06024266066618296\n",
            "Loss :  0.06030420067090216\n",
            "Loss :  0.06039115078598854\n",
            "Loss :  0.06036073366230018\n",
            "Loss :  0.0603979138788042\n",
            "Loss :  0.06046069092597826\n",
            "Loss :  0.06045560669345954\n",
            "Loss :  0.06040021360069018\n",
            "Loss :  0.06039734487796136\n",
            "Loss :  0.060241982015865246\n",
            "Loss :  0.06027304641114621\n",
            "Loss :  0.06037497932304385\n",
            "Loss :  0.060389167605302274\n",
            "Loss :  0.06041910304298361\n",
            "Loss :  0.06051439121164723\n",
            "Loss :  0.06057910587922169\n",
            "Loss :  0.06050234522356097\n",
            "Loss :  0.06052567417782144\n",
            "Loss :  0.06052781646921687\n",
            "Loss :  0.060504905870648856\n",
            "Loss :  0.06048226617535969\n",
            "Loss :  0.06050093410338134\n",
            "Loss :  0.06055534078415376\n",
            "Loss :  0.060605051137361504\n",
            "Loss :  0.06059090706921181\n",
            "Loss :  0.060588602023102386\n",
            "Loss :  0.060560354261133674\n",
            "Validation: \n",
            " Loss :  0.061501070857048035\n",
            " Loss :  0.06635299750736781\n",
            " Loss :  0.06626150820676874\n",
            " Loss :  0.0670025044533073\n",
            " Loss :  0.06656454779483655\n",
            "\n",
            "Epoch: 61\n",
            "Loss :  0.06639624387025833\n",
            "Loss :  0.059493196958845314\n",
            "Loss :  0.05920792920958428\n",
            "Loss :  0.059315340533371896\n",
            "Loss :  0.05965414407049737\n",
            "Loss :  0.059496574030787334\n",
            "Loss :  0.059783189755971314\n",
            "Loss :  0.05972564194194028\n",
            "Loss :  0.05988739380314027\n",
            "Loss :  0.06005376745220069\n",
            "Loss :  0.059850599284809416\n",
            "Loss :  0.060040899023816395\n",
            "Loss :  0.05988487074813567\n",
            "Loss :  0.05968845629965076\n",
            "Loss :  0.059782218822139375\n",
            "Loss :  0.05993057989718898\n",
            "Loss :  0.05991414279578636\n",
            "Loss :  0.05995723844794502\n",
            "Loss :  0.05992322036409905\n",
            "Loss :  0.05985779442010126\n",
            "Loss :  0.05988609523915533\n",
            "Loss :  0.05991178916952621\n",
            "Loss :  0.05992239735582296\n",
            "Loss :  0.05998518787227668\n",
            "Loss :  0.060116423131643\n",
            "Loss :  0.060184899064530416\n",
            "Loss :  0.060097095966910034\n",
            "Loss :  0.06006657752823566\n",
            "Loss :  0.05997715516552806\n",
            "Loss :  0.05996120270482453\n",
            "Loss :  0.059936593623949444\n",
            "Loss :  0.05990802739833712\n",
            "Loss :  0.059803555037745065\n",
            "Loss :  0.05975558596390251\n",
            "Loss :  0.05989429230127167\n",
            "Loss :  0.05993895924668706\n",
            "Loss :  0.05999441552195192\n",
            "Loss :  0.06003373868341716\n",
            "Loss :  0.0600663919461368\n",
            "Loss :  0.05999254621088962\n",
            "Loss :  0.059946017298020626\n",
            "Loss :  0.05994345229617581\n",
            "Loss :  0.059922344097757\n",
            "Loss :  0.059885419810108684\n",
            "Loss :  0.0599675993445644\n",
            "Loss :  0.05999315658092763\n",
            "Loss :  0.06003847491165821\n",
            "Loss :  0.060059887374852114\n",
            "Loss :  0.06004211567574628\n",
            "Loss :  0.05998125399961734\n",
            "Validation: \n",
            " Loss :  0.05856306105852127\n",
            " Loss :  0.06427966750093869\n",
            " Loss :  0.06405500495215742\n",
            " Loss :  0.06446554114828344\n",
            " Loss :  0.0641505990959244\n",
            "\n",
            "Epoch: 62\n",
            "Loss :  0.0693383663892746\n",
            "Loss :  0.058972079645503654\n",
            "Loss :  0.057954647356555575\n",
            "Loss :  0.0583308175686867\n",
            "Loss :  0.059178435130090246\n",
            "Loss :  0.05925864295340052\n",
            "Loss :  0.05942813209334358\n",
            "Loss :  0.05956659539484642\n",
            "Loss :  0.059774018824100494\n",
            "Loss :  0.060090315620322805\n",
            "Loss :  0.06001978142574282\n",
            "Loss :  0.060121457688174805\n",
            "Loss :  0.05994324823302671\n",
            "Loss :  0.059907685993736935\n",
            "Loss :  0.060041794408086345\n",
            "Loss :  0.06004117663629008\n",
            "Loss :  0.06002010488362046\n",
            "Loss :  0.06007104014095507\n",
            "Loss :  0.060162692294759644\n",
            "Loss :  0.060046348662276545\n",
            "Loss :  0.060086575702796526\n",
            "Loss :  0.060003121617422286\n",
            "Loss :  0.059997264683516315\n",
            "Loss :  0.059999568331422226\n",
            "Loss :  0.06001565826446189\n",
            "Loss :  0.059990897433335084\n",
            "Loss :  0.06002286643189489\n",
            "Loss :  0.06000885867764589\n",
            "Loss :  0.059989647323338585\n",
            "Loss :  0.05993741881294349\n",
            "Loss :  0.05994232694472981\n",
            "Loss :  0.0598603699512037\n",
            "Loss :  0.05977479881401002\n",
            "Loss :  0.05977904618776817\n",
            "Loss :  0.05987429654886646\n",
            "Loss :  0.059923782244197324\n",
            "Loss :  0.05998666202675273\n",
            "Loss :  0.06004665393352187\n",
            "Loss :  0.06011054790004345\n",
            "Loss :  0.0600069813106371\n",
            "Loss :  0.059956271294272154\n",
            "Loss :  0.059945029272287724\n",
            "Loss :  0.059929931337221784\n",
            "Loss :  0.059871423635156414\n",
            "Loss :  0.05991514026273946\n",
            "Loss :  0.06002013414206103\n",
            "Loss :  0.060070901503079366\n",
            "Loss :  0.06011350624532204\n",
            "Loss :  0.060118409837915596\n",
            "Loss :  0.06006679008667435\n",
            "Validation: \n",
            " Loss :  0.05948859080672264\n",
            " Loss :  0.06255480310037023\n",
            " Loss :  0.062297501033399164\n",
            " Loss :  0.06275833532458445\n",
            " Loss :  0.06227318298669509\n",
            "\n",
            "Epoch: 63\n",
            "Loss :  0.06087912246584892\n",
            "Loss :  0.057722231203859505\n",
            "Loss :  0.0573223050506342\n",
            "Loss :  0.05724929477418623\n",
            "Loss :  0.058449634146399615\n",
            "Loss :  0.05869783139696308\n",
            "Loss :  0.05862729739947397\n",
            "Loss :  0.05875212184979882\n",
            "Loss :  0.059184633655312624\n",
            "Loss :  0.059439803790915145\n",
            "Loss :  0.059249366237090366\n",
            "Loss :  0.05962519102671125\n",
            "Loss :  0.05953355321337369\n",
            "Loss :  0.059364604557516014\n",
            "Loss :  0.05942646725803402\n",
            "Loss :  0.05940057754220552\n",
            "Loss :  0.059401308990413355\n",
            "Loss :  0.05931475353345536\n",
            "Loss :  0.059274044793449054\n",
            "Loss :  0.05915946587530106\n",
            "Loss :  0.05922365486992533\n",
            "Loss :  0.059291616788407636\n",
            "Loss :  0.05922906783438916\n",
            "Loss :  0.05929914792572265\n",
            "Loss :  0.05932673453295379\n",
            "Loss :  0.05932789064379802\n",
            "Loss :  0.05932848609384449\n",
            "Loss :  0.0593669965478327\n",
            "Loss :  0.05937648443936029\n",
            "Loss :  0.05933910425227532\n",
            "Loss :  0.05932584182170538\n",
            "Loss :  0.059329435073581925\n",
            "Loss :  0.059228486140661896\n",
            "Loss :  0.05923992614866744\n",
            "Loss :  0.05930345718505795\n",
            "Loss :  0.05930966668446519\n",
            "Loss :  0.059311408167730735\n",
            "Loss :  0.05932992420628707\n",
            "Loss :  0.05941410999246469\n",
            "Loss :  0.059343675432531426\n",
            "Loss :  0.05934019021671312\n",
            "Loss :  0.05933003723983927\n",
            "Loss :  0.05934717739775175\n",
            "Loss :  0.05932820877903022\n",
            "Loss :  0.05936701868633294\n",
            "Loss :  0.05941986301686705\n",
            "Loss :  0.05948257078608567\n",
            "Loss :  0.05949760111337508\n",
            "Loss :  0.05951003343827264\n",
            "Loss :  0.05947556026477192\n",
            "Validation: \n",
            " Loss :  0.06132335588335991\n",
            " Loss :  0.06357549663100924\n",
            " Loss :  0.06352876771877451\n",
            " Loss :  0.06384474613138887\n",
            " Loss :  0.06334876462265297\n",
            "\n",
            "Epoch: 64\n",
            "Loss :  0.07050967961549759\n",
            "Loss :  0.059561428021300926\n",
            "Loss :  0.05940898170783406\n",
            "Loss :  0.0592999525608555\n",
            "Loss :  0.059236015306740275\n",
            "Loss :  0.05928393650580855\n",
            "Loss :  0.05911470858044312\n",
            "Loss :  0.05930858336284127\n",
            "Loss :  0.059425034962686495\n",
            "Loss :  0.059386641278371705\n",
            "Loss :  0.05903772345863947\n",
            "Loss :  0.05924065307051212\n",
            "Loss :  0.05915800922296264\n",
            "Loss :  0.059103338943637965\n",
            "Loss :  0.05927977676615647\n",
            "Loss :  0.05939887577532143\n",
            "Loss :  0.05943845763154652\n",
            "Loss :  0.059489128316006466\n",
            "Loss :  0.05948221024887338\n",
            "Loss :  0.059413386245516585\n",
            "Loss :  0.05944181474583659\n",
            "Loss :  0.05955311129867183\n",
            "Loss :  0.05955418708367585\n",
            "Loss :  0.0595956863553235\n",
            "Loss :  0.05971541314451526\n",
            "Loss :  0.05977134709339218\n",
            "Loss :  0.05975869900307893\n",
            "Loss :  0.059738372442247244\n",
            "Loss :  0.059754288056036754\n",
            "Loss :  0.05978723930329392\n",
            "Loss :  0.059756967167917674\n",
            "Loss :  0.05970604001421637\n",
            "Loss :  0.05959709237019221\n",
            "Loss :  0.059596556719966526\n",
            "Loss :  0.0596962514117666\n",
            "Loss :  0.05973820254588739\n",
            "Loss :  0.0597398273461083\n",
            "Loss :  0.05978680446221822\n",
            "Loss :  0.05982568221572503\n",
            "Loss :  0.059701871955791096\n",
            "Loss :  0.05963329201624578\n",
            "Loss :  0.05961877381352033\n",
            "Loss :  0.05960998324911838\n",
            "Loss :  0.059528122031564226\n",
            "Loss :  0.059557678993124\n",
            "Loss :  0.0596111961815944\n",
            "Loss :  0.05962769654044102\n",
            "Loss :  0.059645353529736746\n",
            "Loss :  0.059619620876104076\n",
            "Loss :  0.059597915535424734\n",
            "Validation: \n",
            " Loss :  0.05987999215722084\n",
            " Loss :  0.06322808244398662\n",
            " Loss :  0.0628638335662644\n",
            " Loss :  0.0633493609360007\n",
            " Loss :  0.06293767897619142\n",
            "\n",
            "Epoch: 65\n",
            "Loss :  0.06592625379562378\n",
            "Loss :  0.05787000026215206\n",
            "Loss :  0.05759950993316514\n",
            "Loss :  0.05780058582463572\n",
            "Loss :  0.05795914207289859\n",
            "Loss :  0.058092031043534186\n",
            "Loss :  0.058260235507957274\n",
            "Loss :  0.05845714664795029\n",
            "Loss :  0.05847825611263146\n",
            "Loss :  0.05888751505331679\n",
            "Loss :  0.058580644059889386\n",
            "Loss :  0.05876444531856356\n",
            "Loss :  0.058592203789013475\n",
            "Loss :  0.05868694126492238\n",
            "Loss :  0.058853731536907504\n",
            "Loss :  0.058919424869564196\n",
            "Loss :  0.05889684683787897\n",
            "Loss :  0.059016739394058264\n",
            "Loss :  0.05903096287125382\n",
            "Loss :  0.05900263837026676\n",
            "Loss :  0.05901690170317147\n",
            "Loss :  0.05898756895796948\n",
            "Loss :  0.05896736302189697\n",
            "Loss :  0.05897575233018759\n",
            "Loss :  0.0590396316188747\n",
            "Loss :  0.05904561098650157\n",
            "Loss :  0.059058406829148875\n",
            "Loss :  0.05912577033702738\n",
            "Loss :  0.0591296370528983\n",
            "Loss :  0.05913474688568885\n",
            "Loss :  0.059084712939206946\n",
            "Loss :  0.059107998128489284\n",
            "Loss :  0.059064293204809645\n",
            "Loss :  0.05904282729656315\n",
            "Loss :  0.05915002953097268\n",
            "Loss :  0.059203039684951136\n",
            "Loss :  0.059246410324220185\n",
            "Loss :  0.05926994964319741\n",
            "Loss :  0.05932755916919608\n",
            "Loss :  0.059282290118048565\n",
            "Loss :  0.05925159494478506\n",
            "Loss :  0.059271006344153644\n",
            "Loss :  0.059245073296253586\n",
            "Loss :  0.059229678766080235\n",
            "Loss :  0.059253917673345056\n",
            "Loss :  0.05932735973874111\n",
            "Loss :  0.05936258431693737\n",
            "Loss :  0.05938336126617059\n",
            "Loss :  0.059370019641586745\n",
            "Loss :  0.059340329358995324\n",
            "Validation: \n",
            " Loss :  0.05852450057864189\n",
            " Loss :  0.06283522769808769\n",
            " Loss :  0.062371551808787555\n",
            " Loss :  0.062849437787396\n",
            " Loss :  0.06255747704410258\n",
            "\n",
            "Epoch: 66\n",
            "Loss :  0.06354282051324844\n",
            "Loss :  0.05659399046139284\n",
            "Loss :  0.05733359463158108\n",
            "Loss :  0.0572180627815185\n",
            "Loss :  0.05759085960141042\n",
            "Loss :  0.05757226875307513\n",
            "Loss :  0.057528306714823986\n",
            "Loss :  0.057939610221016576\n",
            "Loss :  0.05818888839380241\n",
            "Loss :  0.058489236530366834\n",
            "Loss :  0.05839250037575712\n",
            "Loss :  0.058495602125788596\n",
            "Loss :  0.05828645591400872\n",
            "Loss :  0.058277179778532216\n",
            "Loss :  0.05839258205172018\n",
            "Loss :  0.05853202675055984\n",
            "Loss :  0.05853058004416294\n",
            "Loss :  0.058623162365099145\n",
            "Loss :  0.0586617046874531\n",
            "Loss :  0.05856708304576225\n",
            "Loss :  0.05856541160549691\n",
            "Loss :  0.05860012384811284\n",
            "Loss :  0.05854739098729591\n",
            "Loss :  0.05858386159588248\n",
            "Loss :  0.05866040969537996\n",
            "Loss :  0.058712221355552215\n",
            "Loss :  0.058725257471946006\n",
            "Loss :  0.05875138715734341\n",
            "Loss :  0.05870088272035334\n",
            "Loss :  0.05873818292292123\n",
            "Loss :  0.058731097678508475\n",
            "Loss :  0.05874743760590385\n",
            "Loss :  0.05863751717074266\n",
            "Loss :  0.058688735898888005\n",
            "Loss :  0.058795635552962154\n",
            "Loss :  0.05884199075804137\n",
            "Loss :  0.058902248172631226\n",
            "Loss :  0.05888213437161034\n",
            "Loss :  0.05892156246374911\n",
            "Loss :  0.058836002626916024\n",
            "Loss :  0.058767075178926424\n",
            "Loss :  0.05874945382857265\n",
            "Loss :  0.05871491056298417\n",
            "Loss :  0.058685133609652794\n",
            "Loss :  0.05876054982242941\n",
            "Loss :  0.05883133008316192\n",
            "Loss :  0.05885428414880066\n",
            "Loss :  0.0588815649714313\n",
            "Loss :  0.058890271591111674\n",
            "Loss :  0.05884979658450954\n",
            "Validation: \n",
            " Loss :  0.05799827352166176\n",
            " Loss :  0.06388739975435394\n",
            " Loss :  0.06362255835315077\n",
            " Loss :  0.06389009757120101\n",
            " Loss :  0.06350293709540072\n",
            "\n",
            "Epoch: 67\n",
            "Loss :  0.06224678456783295\n",
            "Loss :  0.05770499564029954\n",
            "Loss :  0.05648452806330863\n",
            "Loss :  0.05713404258412699\n",
            "Loss :  0.05743775625781315\n",
            "Loss :  0.057563942437078436\n",
            "Loss :  0.0580823791564488\n",
            "Loss :  0.05827827699167628\n",
            "Loss :  0.0583884951049163\n",
            "Loss :  0.058676589239429644\n",
            "Loss :  0.05846581858868646\n",
            "Loss :  0.058658549880927745\n",
            "Loss :  0.05847678031803163\n",
            "Loss :  0.05844264554272171\n",
            "Loss :  0.05858804050383838\n",
            "Loss :  0.05864740829199355\n",
            "Loss :  0.05859526855327328\n",
            "Loss :  0.05856599382054039\n",
            "Loss :  0.05859791343383368\n",
            "Loss :  0.0585154385856933\n",
            "Loss :  0.05848199683264713\n",
            "Loss :  0.0585556766150687\n",
            "Loss :  0.058526926754017224\n",
            "Loss :  0.05851617979603412\n",
            "Loss :  0.058540369233278815\n",
            "Loss :  0.05858064560123174\n",
            "Loss :  0.05858051936745187\n",
            "Loss :  0.058631912574886834\n",
            "Loss :  0.05861091323435519\n",
            "Loss :  0.05856134627283234\n",
            "Loss :  0.05850671806010693\n",
            "Loss :  0.05854793988958815\n",
            "Loss :  0.05844824857475973\n",
            "Loss :  0.05841791161597675\n",
            "Loss :  0.05847625780708629\n",
            "Loss :  0.05844759257600518\n",
            "Loss :  0.05850736828845954\n",
            "Loss :  0.05853795649790057\n",
            "Loss :  0.0585646175160965\n",
            "Loss :  0.0584954461440101\n",
            "Loss :  0.05846012308450411\n",
            "Loss :  0.05846453652946038\n",
            "Loss :  0.058450363779931624\n",
            "Loss :  0.0584084649166209\n",
            "Loss :  0.05846625717690472\n",
            "Loss :  0.0585564234998168\n",
            "Loss :  0.058619123097678846\n",
            "Loss :  0.05866529940451533\n",
            "Loss :  0.05866657314423216\n",
            "Loss :  0.05864682137328835\n",
            "Validation: \n",
            " Loss :  0.059979889541864395\n",
            " Loss :  0.06299986690282822\n",
            " Loss :  0.06279072338124601\n",
            " Loss :  0.06327028456525724\n",
            " Loss :  0.06288549018862807\n",
            "\n",
            "Epoch: 68\n",
            "Loss :  0.06382518261671066\n",
            "Loss :  0.05684195187958804\n",
            "Loss :  0.05698607684600921\n",
            "Loss :  0.05711388023149583\n",
            "Loss :  0.05708194196951098\n",
            "Loss :  0.0575832986071998\n",
            "Loss :  0.057574010835807835\n",
            "Loss :  0.0577013412306846\n",
            "Loss :  0.05786443147578357\n",
            "Loss :  0.058106751764541144\n",
            "Loss :  0.05805301891252546\n",
            "Loss :  0.05812702727344659\n",
            "Loss :  0.05792491682920574\n",
            "Loss :  0.05787317480408508\n",
            "Loss :  0.05799384301541545\n",
            "Loss :  0.058096096758416156\n",
            "Loss :  0.058073946778078256\n",
            "Loss :  0.058172374658765846\n",
            "Loss :  0.05814680158680315\n",
            "Loss :  0.058011909293410664\n",
            "Loss :  0.058057106364129196\n",
            "Loss :  0.05809284291137451\n",
            "Loss :  0.058100895324039246\n",
            "Loss :  0.05813257992912681\n",
            "Loss :  0.05822395497770725\n",
            "Loss :  0.05826208738988614\n",
            "Loss :  0.058250305567550474\n",
            "Loss :  0.05825221699377268\n",
            "Loss :  0.058203998409556326\n",
            "Loss :  0.05820021442940964\n",
            "Loss :  0.05815532643880163\n",
            "Loss :  0.05816278973193031\n",
            "Loss :  0.058065404912393044\n",
            "Loss :  0.05806150345913953\n",
            "Loss :  0.058152082804972825\n",
            "Loss :  0.058180595023764506\n",
            "Loss :  0.05823732191216913\n",
            "Loss :  0.05824843701728271\n",
            "Loss :  0.058297731236522904\n",
            "Loss :  0.05817959035563347\n",
            "Loss :  0.0581689490808959\n",
            "Loss :  0.05815783132166758\n",
            "Loss :  0.05810618343942239\n",
            "Loss :  0.05805367805288452\n",
            "Loss :  0.05809370497592182\n",
            "Loss :  0.05819871388127957\n",
            "Loss :  0.05825467275176803\n",
            "Loss :  0.05827633067649134\n",
            "Loss :  0.058271502258871795\n",
            "Loss :  0.058233490900388804\n",
            "Validation: \n",
            " Loss :  0.06116801127791405\n",
            " Loss :  0.06564853890311151\n",
            " Loss :  0.06484330945262094\n",
            " Loss :  0.06533540852490019\n",
            " Loss :  0.06500198043606899\n",
            "\n",
            "Epoch: 69\n",
            "Loss :  0.06120534613728523\n",
            "Loss :  0.05680989270860499\n",
            "Loss :  0.05745624733113107\n",
            "Loss :  0.056874032703138164\n",
            "Loss :  0.05687496484052844\n",
            "Loss :  0.05719007632019473\n",
            "Loss :  0.05729763307532326\n",
            "Loss :  0.057494200556211066\n",
            "Loss :  0.05787534901389369\n",
            "Loss :  0.058171135383647876\n",
            "Loss :  0.05795972890192919\n",
            "Loss :  0.05815753625991108\n",
            "Loss :  0.0579710363233385\n",
            "Loss :  0.05775860315970792\n",
            "Loss :  0.05787969850902016\n",
            "Loss :  0.05788466763614819\n",
            "Loss :  0.057841935952812985\n",
            "Loss :  0.05792668910577283\n",
            "Loss :  0.05788612233999684\n",
            "Loss :  0.057802934102050924\n",
            "Loss :  0.057800930623539644\n",
            "Loss :  0.057828163366182156\n",
            "Loss :  0.057812459733151744\n",
            "Loss :  0.057804516641608565\n",
            "Loss :  0.05784732169815614\n",
            "Loss :  0.057944301129931\n",
            "Loss :  0.05796668861218339\n",
            "Loss :  0.058013513079428584\n",
            "Loss :  0.05802971705317073\n",
            "Loss :  0.058076269924640656\n",
            "Loss :  0.058063990402459305\n",
            "Loss :  0.058081011105197033\n",
            "Loss :  0.058037481853895095\n",
            "Loss :  0.05808381279339603\n",
            "Loss :  0.05813936705638237\n",
            "Loss :  0.05810480004447138\n",
            "Loss :  0.058131707496837895\n",
            "Loss :  0.05814608134269072\n",
            "Loss :  0.058205683361201146\n",
            "Loss :  0.05810041846636006\n",
            "Loss :  0.058065034300460484\n",
            "Loss :  0.05802713820389008\n",
            "Loss :  0.058027799792074536\n",
            "Loss :  0.057984170575222115\n",
            "Loss :  0.058085384138790115\n",
            "Loss :  0.0581269660208812\n",
            "Loss :  0.05817152520160872\n",
            "Loss :  0.058240733613633806\n",
            "Loss :  0.05824924591084528\n",
            "Loss :  0.05819471798032947\n",
            "Validation: \n",
            " Loss :  0.05727009475231171\n",
            " Loss :  0.06107994062559945\n",
            " Loss :  0.06091066649774226\n",
            " Loss :  0.06140290964089456\n",
            " Loss :  0.06130514427283664\n",
            "\n",
            "Epoch: 70\n",
            "Loss :  0.06666414439678192\n",
            "Loss :  0.058034355667504395\n",
            "Loss :  0.058138793955246605\n",
            "Loss :  0.05756700411438942\n",
            "Loss :  0.057857386949585705\n",
            "Loss :  0.05755274677101303\n",
            "Loss :  0.05743351480999931\n",
            "Loss :  0.057343213071285835\n",
            "Loss :  0.05766414128887801\n",
            "Loss :  0.05798261219656074\n",
            "Loss :  0.05772336402741989\n",
            "Loss :  0.05766905280383858\n",
            "Loss :  0.05746903021104079\n",
            "Loss :  0.057350273862594864\n",
            "Loss :  0.057433601495221996\n",
            "Loss :  0.05749731098875305\n",
            "Loss :  0.05754524122298874\n",
            "Loss :  0.057521022137319834\n",
            "Loss :  0.057466831511061495\n",
            "Loss :  0.05728888164484064\n",
            "Loss :  0.057236554981464176\n",
            "Loss :  0.05724370707338455\n",
            "Loss :  0.057303748182042154\n",
            "Loss :  0.05737921650404538\n",
            "Loss :  0.057379838728434815\n",
            "Loss :  0.05744574750324644\n",
            "Loss :  0.057441488008275345\n",
            "Loss :  0.057502602587876726\n",
            "Loss :  0.05752329174362892\n",
            "Loss :  0.057498936058114895\n",
            "Loss :  0.05746659270156657\n",
            "Loss :  0.05749253844812369\n",
            "Loss :  0.057418720883743786\n",
            "Loss :  0.0573662994513879\n",
            "Loss :  0.057397160766498784\n",
            "Loss :  0.05741907741099681\n",
            "Loss :  0.05743826738974064\n",
            "Loss :  0.057447966891077326\n",
            "Loss :  0.05753571601716552\n",
            "Loss :  0.057419660089113526\n",
            "Loss :  0.05743713176198434\n",
            "Loss :  0.05746652624612887\n",
            "Loss :  0.057476622767020855\n",
            "Loss :  0.05743269765805756\n",
            "Loss :  0.057518521953129176\n",
            "Loss :  0.057585735691756734\n",
            "Loss :  0.05760046716884522\n",
            "Loss :  0.05762334986572053\n",
            "Loss :  0.0576547762909575\n",
            "Loss :  0.05763580417165688\n",
            "Validation: \n",
            " Loss :  0.057373568415641785\n",
            " Loss :  0.0628111662254447\n",
            " Loss :  0.06212337605836915\n",
            " Loss :  0.062329360696135976\n",
            " Loss :  0.061936741027935054\n",
            "\n",
            "Epoch: 71\n",
            "Loss :  0.05764353647828102\n",
            "Loss :  0.0556508946147832\n",
            "Loss :  0.05570308154537564\n",
            "Loss :  0.05523089740064836\n",
            "Loss :  0.05583654852902017\n",
            "Loss :  0.05582755022481376\n",
            "Loss :  0.05609961000622296\n",
            "Loss :  0.05636040572549256\n",
            "Loss :  0.05658041248902863\n",
            "Loss :  0.05693925520057207\n",
            "Loss :  0.05687469055894578\n",
            "Loss :  0.05695531802537205\n",
            "Loss :  0.05693624220973204\n",
            "Loss :  0.05692210039905919\n",
            "Loss :  0.057076798950104\n",
            "Loss :  0.05720832793424461\n",
            "Loss :  0.057220757979413735\n",
            "Loss :  0.05728871292538113\n",
            "Loss :  0.05730320630564215\n",
            "Loss :  0.05725851649155168\n",
            "Loss :  0.057219191755524916\n",
            "Loss :  0.05727739132481729\n",
            "Loss :  0.05724926131073706\n",
            "Loss :  0.0572304143792107\n",
            "Loss :  0.05727703176840707\n",
            "Loss :  0.05733055113796219\n",
            "Loss :  0.057335956271923366\n",
            "Loss :  0.05732501208837182\n",
            "Loss :  0.05731852942567279\n",
            "Loss :  0.057289330794434365\n",
            "Loss :  0.05722492516189318\n",
            "Loss :  0.057200427152144566\n",
            "Loss :  0.05708875690479516\n",
            "Loss :  0.05715113311015587\n",
            "Loss :  0.057232203786848575\n",
            "Loss :  0.057256793481247376\n",
            "Loss :  0.05729165976645213\n",
            "Loss :  0.05735527033192128\n",
            "Loss :  0.057368031333828846\n",
            "Loss :  0.05725083535398973\n",
            "Loss :  0.0572411697775943\n",
            "Loss :  0.05725725109342241\n",
            "Loss :  0.05726135676094019\n",
            "Loss :  0.05724389960864027\n",
            "Loss :  0.05727147724048621\n",
            "Loss :  0.057348640515838656\n",
            "Loss :  0.057380736698426804\n",
            "Loss :  0.0574080036703948\n",
            "Loss :  0.05738264725789459\n",
            "Loss :  0.05735456375993196\n",
            "Validation: \n",
            " Loss :  0.06168022379279137\n",
            " Loss :  0.06446514562481925\n",
            " Loss :  0.06397794332446122\n",
            " Loss :  0.06418048956843674\n",
            " Loss :  0.06393264464022201\n",
            "\n",
            "Epoch: 72\n",
            "Loss :  0.06326019018888474\n",
            "Loss :  0.05582950636744499\n",
            "Loss :  0.05638980297815232\n",
            "Loss :  0.05651258797414841\n",
            "Loss :  0.05706621433903531\n",
            "Loss :  0.057100169433682575\n",
            "Loss :  0.057217431300487676\n",
            "Loss :  0.057245939235452195\n",
            "Loss :  0.05726555146184968\n",
            "Loss :  0.05742715405566352\n",
            "Loss :  0.05745726720531388\n",
            "Loss :  0.05762149618418367\n",
            "Loss :  0.0574910249715009\n",
            "Loss :  0.05738467979067154\n",
            "Loss :  0.057365486120924035\n",
            "Loss :  0.05741720158138022\n",
            "Loss :  0.0572950948515664\n",
            "Loss :  0.05746177719001882\n",
            "Loss :  0.057488113106614315\n",
            "Loss :  0.057303649768311316\n",
            "Loss :  0.05727553814278906\n",
            "Loss :  0.0572920357058116\n",
            "Loss :  0.05718142170105045\n",
            "Loss :  0.05714201166000201\n",
            "Loss :  0.05715842664056299\n",
            "Loss :  0.05721034719590172\n",
            "Loss :  0.05721390554217543\n",
            "Loss :  0.05730588540324866\n",
            "Loss :  0.057309528222817964\n",
            "Loss :  0.05726587346073279\n",
            "Loss :  0.05726083085881515\n",
            "Loss :  0.057202204259358035\n",
            "Loss :  0.057102515666841346\n",
            "Loss :  0.05711995786635537\n",
            "Loss :  0.0571754515892075\n",
            "Loss :  0.057209327816963196\n",
            "Loss :  0.057279794572380444\n",
            "Loss :  0.05734876598510138\n",
            "Loss :  0.05736918207697981\n",
            "Loss :  0.05725925434809512\n",
            "Loss :  0.05727937840798549\n",
            "Loss :  0.05725442283671268\n",
            "Loss :  0.0572089043284397\n",
            "Loss :  0.05717558424980347\n",
            "Loss :  0.05725447092703681\n",
            "Loss :  0.057331400012758514\n",
            "Loss :  0.05737979835355877\n",
            "Loss :  0.057444334773627564\n",
            "Loss :  0.05744307107929133\n",
            "Loss :  0.05739266349499677\n",
            "Validation: \n",
            " Loss :  0.054907236248254776\n",
            " Loss :  0.06190227344632149\n",
            " Loss :  0.06175399217300299\n",
            " Loss :  0.06211728669825147\n",
            " Loss :  0.06196137513091535\n",
            "\n",
            "Epoch: 73\n",
            "Loss :  0.06445156037807465\n",
            "Loss :  0.056528732519258156\n",
            "Loss :  0.05587383182275863\n",
            "Loss :  0.0558139743103135\n",
            "Loss :  0.05603839611498321\n",
            "Loss :  0.05634888341906024\n",
            "Loss :  0.05652068311073741\n",
            "Loss :  0.05652977111683765\n",
            "Loss :  0.05674523372709015\n",
            "Loss :  0.0569957061619549\n",
            "Loss :  0.05696978722468461\n",
            "Loss :  0.057166555611131424\n",
            "Loss :  0.05695187799201524\n",
            "Loss :  0.05689755147536292\n",
            "Loss :  0.05693398836128255\n",
            "Loss :  0.057124535867709984\n",
            "Loss :  0.057048284350344855\n",
            "Loss :  0.05703912346422324\n",
            "Loss :  0.05705512669040353\n",
            "Loss :  0.05693261684235478\n",
            "Loss :  0.05697599659437564\n",
            "Loss :  0.05703844733854041\n",
            "Loss :  0.05701560161299835\n",
            "Loss :  0.05696864174557971\n",
            "Loss :  0.05704883201611982\n",
            "Loss :  0.05708286587698051\n",
            "Loss :  0.05706904655843402\n",
            "Loss :  0.05708791314180927\n",
            "Loss :  0.05717322567073476\n",
            "Loss :  0.05714168993570551\n",
            "Loss :  0.0571683464553269\n",
            "Loss :  0.05713664222856043\n",
            "Loss :  0.05704760253197307\n",
            "Loss :  0.057002111143936204\n",
            "Loss :  0.05706098361099221\n",
            "Loss :  0.05707814872392222\n",
            "Loss :  0.05710958582129835\n",
            "Loss :  0.05716333207937906\n",
            "Loss :  0.05724416252743854\n",
            "Loss :  0.057134134032765925\n",
            "Loss :  0.057088298086736565\n",
            "Loss :  0.05706538599881813\n",
            "Loss :  0.05711106754255408\n",
            "Loss :  0.05709328437591373\n",
            "Loss :  0.05717345754575837\n",
            "Loss :  0.057177427429251554\n",
            "Loss :  0.057222200699005105\n",
            "Loss :  0.057252487440587606\n",
            "Loss :  0.057246648603876016\n",
            "Loss :  0.05722903219751574\n",
            "Validation: \n",
            " Loss :  0.05649460852146149\n",
            " Loss :  0.061899056214661824\n",
            " Loss :  0.06176120160919864\n",
            " Loss :  0.062215272947901586\n",
            " Loss :  0.06187182172764966\n",
            "\n",
            "Epoch: 74\n",
            "Loss :  0.06555125117301941\n",
            "Loss :  0.05659867890856483\n",
            "Loss :  0.05664282983967236\n",
            "Loss :  0.055848435168304754\n",
            "Loss :  0.05599180009306931\n",
            "Loss :  0.05578584793735953\n",
            "Loss :  0.056015134834852376\n",
            "Loss :  0.05596206510360812\n",
            "Loss :  0.056327148996017595\n",
            "Loss :  0.05669939264163866\n",
            "Loss :  0.056688098661085165\n",
            "Loss :  0.0568846713852238\n",
            "Loss :  0.05668830492895497\n",
            "Loss :  0.05672137882873302\n",
            "Loss :  0.05685784823611273\n",
            "Loss :  0.05688573447185637\n",
            "Loss :  0.05670249675001417\n",
            "Loss :  0.05678628507064797\n",
            "Loss :  0.05671498675455046\n",
            "Loss :  0.05661562841912215\n",
            "Loss :  0.05663220293412161\n",
            "Loss :  0.05663940993742355\n",
            "Loss :  0.056645631537313376\n",
            "Loss :  0.056656044218447305\n",
            "Loss :  0.056708181578341364\n",
            "Loss :  0.05669556335505262\n",
            "Loss :  0.056702121292951005\n",
            "Loss :  0.05671919362681378\n",
            "Loss :  0.05673333730720965\n",
            "Loss :  0.056696048844925725\n",
            "Loss :  0.05669596049922249\n",
            "Loss :  0.056716496562076155\n",
            "Loss :  0.056586344143219086\n",
            "Loss :  0.05660802693165321\n",
            "Loss :  0.05664448672769007\n",
            "Loss :  0.05667571596100799\n",
            "Loss :  0.05671483829328558\n",
            "Loss :  0.056743559922089154\n",
            "Loss :  0.05680590542399977\n",
            "Loss :  0.056679591269749204\n",
            "Loss :  0.05664525630691105\n",
            "Loss :  0.05663660746022443\n",
            "Loss :  0.05663503866616167\n",
            "Loss :  0.05660123605358905\n",
            "Loss :  0.056654331365139844\n",
            "Loss :  0.05674216475032121\n",
            "Loss :  0.05675254249844272\n",
            "Loss :  0.056792087236027806\n",
            "Loss :  0.05680228929323863\n",
            "Loss :  0.056782927235129406\n",
            "Validation: \n",
            " Loss :  0.0575171634554863\n",
            " Loss :  0.06326475970092274\n",
            " Loss :  0.06301470209912556\n",
            " Loss :  0.06350072229006251\n",
            " Loss :  0.06314117539628053\n",
            "\n",
            "Epoch: 75\n",
            "Loss :  0.06951424479484558\n",
            "Loss :  0.056385053152387794\n",
            "Loss :  0.05586582510953858\n",
            "Loss :  0.05570133523114266\n",
            "Loss :  0.05596130831939418\n",
            "Loss :  0.055763359236366605\n",
            "Loss :  0.05589903255955118\n",
            "Loss :  0.05607923025816259\n",
            "Loss :  0.05613513116115405\n",
            "Loss :  0.056385137631997956\n",
            "Loss :  0.05607990555391453\n",
            "Loss :  0.056245355455725045\n",
            "Loss :  0.056084528349894136\n",
            "Loss :  0.05613115455471832\n",
            "Loss :  0.05624430299016601\n",
            "Loss :  0.05649350867267476\n",
            "Loss :  0.056519145574073615\n",
            "Loss :  0.05646253357592382\n",
            "Loss :  0.05635244164677615\n",
            "Loss :  0.05622945677157472\n",
            "Loss :  0.056250563679049854\n",
            "Loss :  0.05624988800470863\n",
            "Loss :  0.056217639239260514\n",
            "Loss :  0.05624511581543204\n",
            "Loss :  0.056282482963998286\n",
            "Loss :  0.05634969992347923\n",
            "Loss :  0.05637868189777451\n",
            "Loss :  0.056456438702190935\n",
            "Loss :  0.056489762157413886\n",
            "Loss :  0.05644171919251226\n",
            "Loss :  0.05641752277250306\n",
            "Loss :  0.0564249592961989\n",
            "Loss :  0.056347540675479674\n",
            "Loss :  0.05632949118527761\n",
            "Loss :  0.05636421110360853\n",
            "Loss :  0.05639056636397315\n",
            "Loss :  0.05640700069408337\n",
            "Loss :  0.0564097621731681\n",
            "Loss :  0.05647373560258723\n",
            "Loss :  0.05641538365875059\n",
            "Loss :  0.0563795804568657\n",
            "Loss :  0.05639166194592079\n",
            "Loss :  0.05638911698075485\n",
            "Loss :  0.05636092240807628\n",
            "Loss :  0.05637773769208634\n",
            "Loss :  0.05645911187396081\n",
            "Loss :  0.05653132129775728\n",
            "Loss :  0.05653307355165228\n",
            "Loss :  0.05655705606615221\n",
            "Loss :  0.05651812985266299\n",
            "Validation: \n",
            " Loss :  0.05822199210524559\n",
            " Loss :  0.0610884428024292\n",
            " Loss :  0.061063267108870715\n",
            " Loss :  0.061560686555553655\n",
            " Loss :  0.06125620430634345\n",
            "\n",
            "Epoch: 76\n",
            "Loss :  0.06228455528616905\n",
            "Loss :  0.05601092597300356\n",
            "Loss :  0.056411679834127426\n",
            "Loss :  0.05617587376506098\n",
            "Loss :  0.05628600934656655\n",
            "Loss :  0.056375423848044635\n",
            "Loss :  0.0562591798847816\n",
            "Loss :  0.056326391516436994\n",
            "Loss :  0.05645181887127735\n",
            "Loss :  0.05663038679695392\n",
            "Loss :  0.056668906223655925\n",
            "Loss :  0.05679570195508433\n",
            "Loss :  0.056607339777483424\n",
            "Loss :  0.05642299388201182\n",
            "Loss :  0.05639884908888357\n",
            "Loss :  0.05635793993113846\n",
            "Loss :  0.05627824847372422\n",
            "Loss :  0.05629809422974001\n",
            "Loss :  0.05630705598227227\n",
            "Loss :  0.05618136575084706\n",
            "Loss :  0.05613896940181504\n",
            "Loss :  0.0560459957868567\n",
            "Loss :  0.05601993049768841\n",
            "Loss :  0.05597691505631327\n",
            "Loss :  0.05602915357813795\n",
            "Loss :  0.05604689687015526\n",
            "Loss :  0.056077666537857604\n",
            "Loss :  0.056124202233508946\n",
            "Loss :  0.05616185199005324\n",
            "Loss :  0.05616785055890526\n",
            "Loss :  0.05610640457153716\n",
            "Loss :  0.0560978712376291\n",
            "Loss :  0.05604260286020341\n",
            "Loss :  0.05601632114488553\n",
            "Loss :  0.05610356844765\n",
            "Loss :  0.05612830145873575\n",
            "Loss :  0.056121723845064475\n",
            "Loss :  0.056157135587898546\n",
            "Loss :  0.056238626694585396\n",
            "Loss :  0.0561767856940589\n",
            "Loss :  0.05616683509210101\n",
            "Loss :  0.05616201416854441\n",
            "Loss :  0.05615117683157218\n",
            "Loss :  0.05607673295896734\n",
            "Loss :  0.05612579292180587\n",
            "Loss :  0.05620665180015723\n",
            "Loss :  0.05621704201409977\n",
            "Loss :  0.05620034456632699\n",
            "Loss :  0.056201862740541446\n",
            "Loss :  0.0561427877885868\n",
            "Validation: \n",
            " Loss :  0.05465551093220711\n",
            " Loss :  0.06063917829167275\n",
            " Loss :  0.06024815414736911\n",
            " Loss :  0.060565919050427734\n",
            " Loss :  0.060239038073722224\n",
            "\n",
            "Epoch: 77\n",
            "Loss :  0.0624048225581646\n",
            "Loss :  0.05558209798552773\n",
            "Loss :  0.05528774467252549\n",
            "Loss :  0.05506745894109049\n",
            "Loss :  0.05532393359192988\n",
            "Loss :  0.05540611324649231\n",
            "Loss :  0.05542787787367086\n",
            "Loss :  0.05536137652439131\n",
            "Loss :  0.05555218246615963\n",
            "Loss :  0.05593018057745892\n",
            "Loss :  0.05574709704459304\n",
            "Loss :  0.05580554395779833\n",
            "Loss :  0.05560609418129133\n",
            "Loss :  0.055514638433019625\n",
            "Loss :  0.05574644473216212\n",
            "Loss :  0.05585704519357902\n",
            "Loss :  0.05577333370998779\n",
            "Loss :  0.0558091257617139\n",
            "Loss :  0.055952045469652885\n",
            "Loss :  0.05582806793729048\n",
            "Loss :  0.05580601020164751\n",
            "Loss :  0.05584082346391904\n",
            "Loss :  0.0558913603458739\n",
            "Loss :  0.05589794788461227\n",
            "Loss :  0.05589580718288778\n",
            "Loss :  0.05594073976594138\n",
            "Loss :  0.055918453339758506\n",
            "Loss :  0.055923706614773215\n",
            "Loss :  0.055969900428400346\n",
            "Loss :  0.05598556759840844\n",
            "Loss :  0.055943697307295974\n",
            "Loss :  0.05594802069271109\n",
            "Loss :  0.05588458732606095\n",
            "Loss :  0.05593080192894374\n",
            "Loss :  0.055989697062112716\n",
            "Loss :  0.05605385788952523\n",
            "Loss :  0.056093663162967176\n",
            "Loss :  0.05614694910791685\n",
            "Loss :  0.05618647751923934\n",
            "Loss :  0.05610427006964793\n",
            "Loss :  0.05609309459639309\n",
            "Loss :  0.05613086913732716\n",
            "Loss :  0.0561061316388803\n",
            "Loss :  0.056047266777043\n",
            "Loss :  0.05609998743888202\n",
            "Loss :  0.056182409345508943\n",
            "Loss :  0.05625055344076322\n",
            "Loss :  0.05624106873594019\n",
            "Loss :  0.05624484966301868\n",
            "Loss :  0.056216941945361025\n",
            "Validation: \n",
            " Loss :  0.05862514302134514\n",
            " Loss :  0.06317132480797313\n",
            " Loss :  0.062578125516089\n",
            " Loss :  0.06300747376240667\n",
            " Loss :  0.06259369105100632\n",
            "\n",
            "Epoch: 78\n",
            "Loss :  0.06500034779310226\n",
            "Loss :  0.05495176227255301\n",
            "Loss :  0.05482204063307671\n",
            "Loss :  0.05474960936173316\n",
            "Loss :  0.0556383787131891\n",
            "Loss :  0.05546566164668869\n",
            "Loss :  0.05565513145239627\n",
            "Loss :  0.05555721918042277\n",
            "Loss :  0.055675593515237175\n",
            "Loss :  0.05582796491600655\n",
            "Loss :  0.05572230287707678\n",
            "Loss :  0.05559308236246711\n",
            "Loss :  0.05559531931788468\n",
            "Loss :  0.05562314348480173\n",
            "Loss :  0.05569438345677464\n",
            "Loss :  0.05576311523057767\n",
            "Loss :  0.05572529944666424\n",
            "Loss :  0.05582525208592415\n",
            "Loss :  0.05584482916563914\n",
            "Loss :  0.055732651784313914\n",
            "Loss :  0.05567185045104122\n",
            "Loss :  0.05571229738223044\n",
            "Loss :  0.05566217923933025\n",
            "Loss :  0.055691269488432706\n",
            "Loss :  0.055822505975041646\n",
            "Loss :  0.05586966691145384\n",
            "Loss :  0.05583956819099028\n",
            "Loss :  0.05585877359518266\n",
            "Loss :  0.05584284421918231\n",
            "Loss :  0.05581802849200174\n",
            "Loss :  0.05575849718578234\n",
            "Loss :  0.05573592981820705\n",
            "Loss :  0.05557126777008686\n",
            "Loss :  0.05557208043435909\n",
            "Loss :  0.055623321545438684\n",
            "Loss :  0.055664985427404744\n",
            "Loss :  0.055727835073860726\n",
            "Loss :  0.055748186844860444\n",
            "Loss :  0.05579747906898263\n",
            "Loss :  0.05570025115138125\n",
            "Loss :  0.05563799028013115\n",
            "Loss :  0.055629687959332826\n",
            "Loss :  0.05560414578031191\n",
            "Loss :  0.05556011618290036\n",
            "Loss :  0.055610642791672896\n",
            "Loss :  0.05569486790016591\n",
            "Loss :  0.05575445537614977\n",
            "Loss :  0.05580293494741375\n",
            "Loss :  0.055789578984841985\n",
            "Loss :  0.055783870908794\n",
            "Validation: \n",
            " Loss :  0.059341173619031906\n",
            " Loss :  0.06230295201142629\n",
            " Loss :  0.06213714782057739\n",
            " Loss :  0.06256270677339835\n",
            " Loss :  0.06230158299023723\n",
            "\n",
            "Epoch: 79\n",
            "Loss :  0.05815151706337929\n",
            "Loss :  0.05414747616106814\n",
            "Loss :  0.054312163165637424\n",
            "Loss :  0.0542068581186956\n",
            "Loss :  0.054131181683482195\n",
            "Loss :  0.05411877724177697\n",
            "Loss :  0.054429692628442265\n",
            "Loss :  0.054611462942311464\n",
            "Loss :  0.05483765104486619\n",
            "Loss :  0.05511126663167398\n",
            "Loss :  0.054963903815144356\n",
            "Loss :  0.05517291864967561\n",
            "Loss :  0.05507231990167917\n",
            "Loss :  0.055034633441508274\n",
            "Loss :  0.05524471119468939\n",
            "Loss :  0.055304010306170444\n",
            "Loss :  0.05527275532298947\n",
            "Loss :  0.05530114994760145\n",
            "Loss :  0.05540636747828505\n",
            "Loss :  0.05527934982046407\n",
            "Loss :  0.055288633118517956\n",
            "Loss :  0.05529331656005145\n",
            "Loss :  0.05525275560862878\n",
            "Loss :  0.0552667181987267\n",
            "Loss :  0.05528909635741681\n",
            "Loss :  0.05534660641059458\n",
            "Loss :  0.0553041683274439\n",
            "Loss :  0.055404478044857396\n",
            "Loss :  0.0554198541171398\n",
            "Loss :  0.05542627340534708\n",
            "Loss :  0.05541868311077654\n",
            "Loss :  0.0553991826808146\n",
            "Loss :  0.055274411467284054\n",
            "Loss :  0.05529843794120042\n",
            "Loss :  0.05532156336298786\n",
            "Loss :  0.05535837432682684\n",
            "Loss :  0.05540044304406544\n",
            "Loss :  0.055445345354610376\n",
            "Loss :  0.055474686008582276\n",
            "Loss :  0.05539262595841342\n",
            "Loss :  0.055401965390491366\n",
            "Loss :  0.0553620372266665\n",
            "Loss :  0.055375005633983926\n",
            "Loss :  0.05533482274364706\n",
            "Loss :  0.05540393661209245\n",
            "Loss :  0.05548634668378502\n",
            "Loss :  0.05554319627243629\n",
            "Loss :  0.055568393288207917\n",
            "Loss :  0.05555580145913697\n",
            "Loss :  0.05549807647762862\n",
            "Validation: \n",
            " Loss :  0.055502526462078094\n",
            " Loss :  0.061594665405296144\n",
            " Loss :  0.061210517021940976\n",
            " Loss :  0.06188608162471505\n",
            " Loss :  0.06164049219202112\n",
            "\n",
            "Epoch: 80\n",
            "Loss :  0.06050121784210205\n",
            "Loss :  0.05401544509963556\n",
            "Loss :  0.05414886843590509\n",
            "Loss :  0.0536842214003686\n",
            "Loss :  0.05431690921143788\n",
            "Loss :  0.05442867230843095\n",
            "Loss :  0.0543561329851385\n",
            "Loss :  0.054239503042378893\n",
            "Loss :  0.05445375077334451\n",
            "Loss :  0.054564118139691406\n",
            "Loss :  0.05459989710609511\n",
            "Loss :  0.05483197239605156\n",
            "Loss :  0.054695240475914696\n",
            "Loss :  0.05464951668418091\n",
            "Loss :  0.05475044443357921\n",
            "Loss :  0.05482004938141399\n",
            "Loss :  0.05483593284991217\n",
            "Loss :  0.05485348512380444\n",
            "Loss :  0.05489204756051137\n",
            "Loss :  0.05484514238790692\n",
            "Loss :  0.054867639581658946\n",
            "Loss :  0.054879837267771715\n",
            "Loss :  0.05489762095372062\n",
            "Loss :  0.05487610090088535\n",
            "Loss :  0.054948041223267796\n",
            "Loss :  0.055007536958650766\n",
            "Loss :  0.055046841830471926\n",
            "Loss :  0.05511793935485872\n",
            "Loss :  0.055187150623874734\n",
            "Loss :  0.055188957921827785\n",
            "Loss :  0.05516350678975796\n",
            "Loss :  0.05519976287217769\n",
            "Loss :  0.05511307007287893\n",
            "Loss :  0.05511486018217222\n",
            "Loss :  0.055160353675790544\n",
            "Loss :  0.055118894273484195\n",
            "Loss :  0.0551074152693689\n",
            "Loss :  0.05514671584224765\n",
            "Loss :  0.05516879609602643\n",
            "Loss :  0.05508654288318761\n",
            "Loss :  0.05505872189887147\n",
            "Loss :  0.05504857380749825\n",
            "Loss :  0.055062927615401866\n",
            "Loss :  0.05500775452902035\n",
            "Loss :  0.055069298709398494\n",
            "Loss :  0.05511533672159368\n",
            "Loss :  0.055174676261018\n",
            "Loss :  0.05522456966862557\n",
            "Loss :  0.055241423513810006\n",
            "Loss :  0.05522204714005931\n",
            "Validation: \n",
            " Loss :  0.058450400829315186\n",
            " Loss :  0.0633671549814088\n",
            " Loss :  0.06286426297411686\n",
            " Loss :  0.06354190366434269\n",
            " Loss :  0.06317006690818587\n",
            "\n",
            "Epoch: 81\n",
            "Loss :  0.06135397404432297\n",
            "Loss :  0.05334073474461382\n",
            "Loss :  0.05344755486363456\n",
            "Loss :  0.05314373537417381\n",
            "Loss :  0.05409419182233694\n",
            "Loss :  0.05413321602870436\n",
            "Loss :  0.05438584186991707\n",
            "Loss :  0.05442864972520882\n",
            "Loss :  0.05477361210886343\n",
            "Loss :  0.055070237921816964\n",
            "Loss :  0.05482907641199556\n",
            "Loss :  0.05499013473053236\n",
            "Loss :  0.05480652342527366\n",
            "Loss :  0.054699566101527396\n",
            "Loss :  0.05484744887931127\n",
            "Loss :  0.05494293185654065\n",
            "Loss :  0.05495347367217822\n",
            "Loss :  0.05496522699270332\n",
            "Loss :  0.05500996084009086\n",
            "Loss :  0.05494451587190803\n",
            "Loss :  0.05496684806560403\n",
            "Loss :  0.05503340182004947\n",
            "Loss :  0.0550430317944531\n",
            "Loss :  0.05499314692564857\n",
            "Loss :  0.055073299858703656\n",
            "Loss :  0.055149503675589996\n",
            "Loss :  0.05519627167404383\n",
            "Loss :  0.05522884029853828\n",
            "Loss :  0.05523892009491598\n",
            "Loss :  0.055292497910687195\n",
            "Loss :  0.05526003437085801\n",
            "Loss :  0.05522825185604801\n",
            "Loss :  0.05511183516815815\n",
            "Loss :  0.05511495486757186\n",
            "Loss :  0.05516738444566727\n",
            "Loss :  0.05515313362888461\n",
            "Loss :  0.055179546424639195\n",
            "Loss :  0.05519810764776729\n",
            "Loss :  0.055254636073284576\n",
            "Loss :  0.05514877465794153\n",
            "Loss :  0.05512709117626906\n",
            "Loss :  0.0550786634757571\n",
            "Loss :  0.05505337428098053\n",
            "Loss :  0.055061481875775864\n",
            "Loss :  0.055143323682603385\n",
            "Loss :  0.055234135907069014\n",
            "Loss :  0.05527767645692101\n",
            "Loss :  0.055295977124552816\n",
            "Loss :  0.05528635696629972\n",
            "Loss :  0.055243342265092185\n",
            "Validation: \n",
            " Loss :  0.05594756081700325\n",
            " Loss :  0.0627056730999833\n",
            " Loss :  0.06243340189500553\n",
            " Loss :  0.06297571236481432\n",
            " Loss :  0.06255059077599902\n",
            "\n",
            "Epoch: 82\n",
            "Loss :  0.06053648889064789\n",
            "Loss :  0.05409072068604556\n",
            "Loss :  0.05354992264793033\n",
            "Loss :  0.05364289939884217\n",
            "Loss :  0.05430042044055171\n",
            "Loss :  0.05449282626310984\n",
            "Loss :  0.05483177255411617\n",
            "Loss :  0.054661623294084846\n",
            "Loss :  0.054730152504311666\n",
            "Loss :  0.054818411490746906\n",
            "Loss :  0.05476420088717253\n",
            "Loss :  0.05486856461376757\n",
            "Loss :  0.054686588381440186\n",
            "Loss :  0.05468035702600734\n",
            "Loss :  0.05484962352412812\n",
            "Loss :  0.05490547485205512\n",
            "Loss :  0.054786294397915374\n",
            "Loss :  0.05477401071734596\n",
            "Loss :  0.054818177276718026\n",
            "Loss :  0.05473508992042217\n",
            "Loss :  0.054673505696787764\n",
            "Loss :  0.05475148456215293\n",
            "Loss :  0.054746493562314306\n",
            "Loss :  0.05467906834565716\n",
            "Loss :  0.05469734716341208\n",
            "Loss :  0.054751369167134106\n",
            "Loss :  0.05481640437896224\n",
            "Loss :  0.054850512723751174\n",
            "Loss :  0.05487961974326402\n",
            "Loss :  0.054871391453796235\n",
            "Loss :  0.05487475939020762\n",
            "Loss :  0.054821887032587044\n",
            "Loss :  0.054706811742433505\n",
            "Loss :  0.05473793492711563\n",
            "Loss :  0.05482498174014329\n",
            "Loss :  0.054890103022597114\n",
            "Loss :  0.054935493720617984\n",
            "Loss :  0.0549656763030191\n",
            "Loss :  0.05501138243892687\n",
            "Loss :  0.05495618377119074\n",
            "Loss :  0.0549349470105849\n",
            "Loss :  0.054946255292335565\n",
            "Loss :  0.05490924237459686\n",
            "Loss :  0.05488823452778593\n",
            "Loss :  0.054964742464881367\n",
            "Loss :  0.05501949265929654\n",
            "Loss :  0.055057027674160916\n",
            "Loss :  0.05506285675455304\n",
            "Loss :  0.05507481184917775\n",
            "Loss :  0.05501422677530533\n",
            "Validation: \n",
            " Loss :  0.058298490941524506\n",
            " Loss :  0.06458036619282904\n",
            " Loss :  0.06394229147855829\n",
            " Loss :  0.06450055502965803\n",
            " Loss :  0.06432417214468673\n",
            "\n",
            "Epoch: 83\n",
            "Loss :  0.06058323755860329\n",
            "Loss :  0.05401179600845684\n",
            "Loss :  0.05391710801493554\n",
            "Loss :  0.054000233330072894\n",
            "Loss :  0.05470744375048614\n",
            "Loss :  0.05450225606852887\n",
            "Loss :  0.054477334572154965\n",
            "Loss :  0.05452670723619595\n",
            "Loss :  0.05423609305311133\n",
            "Loss :  0.05465587994063294\n",
            "Loss :  0.05452067537768052\n",
            "Loss :  0.054575608556603525\n",
            "Loss :  0.05448867968660741\n",
            "Loss :  0.0544754731120499\n",
            "Loss :  0.054487974878321306\n",
            "Loss :  0.054549314673767973\n",
            "Loss :  0.054444316705191356\n",
            "Loss :  0.0544086579908753\n",
            "Loss :  0.05443971052518866\n",
            "Loss :  0.054482629025325725\n",
            "Loss :  0.05442182971766932\n",
            "Loss :  0.05448683109399267\n",
            "Loss :  0.0544804504385631\n",
            "Loss :  0.054468973729259525\n",
            "Loss :  0.05452002337612057\n",
            "Loss :  0.05449352046823597\n",
            "Loss :  0.05448872845805468\n",
            "Loss :  0.054568351937176116\n",
            "Loss :  0.054577542091073517\n",
            "Loss :  0.05456192104011467\n",
            "Loss :  0.05455298651244949\n",
            "Loss :  0.05454228983718866\n",
            "Loss :  0.054491449263515505\n",
            "Loss :  0.054468424208574784\n",
            "Loss :  0.054566887914697444\n",
            "Loss :  0.054564681169484076\n",
            "Loss :  0.05461061606570624\n",
            "Loss :  0.05465778032684262\n",
            "Loss :  0.05468087610140873\n",
            "Loss :  0.054590267712807714\n",
            "Loss :  0.05458350275222797\n",
            "Loss :  0.05457437460802477\n",
            "Loss :  0.05458886316156727\n",
            "Loss :  0.054580540078629475\n",
            "Loss :  0.054647474577164706\n",
            "Loss :  0.05470967311783799\n",
            "Loss :  0.054744251128973\n",
            "Loss :  0.054776346433567644\n",
            "Loss :  0.05479045944160582\n",
            "Loss :  0.05476512344906869\n",
            "Validation: \n",
            " Loss :  0.055034540593624115\n",
            " Loss :  0.059704646113372985\n",
            " Loss :  0.05947330157931258\n",
            " Loss :  0.06011841769834034\n",
            " Loss :  0.059873472071356244\n",
            "\n",
            "Epoch: 84\n",
            "Loss :  0.05635889992117882\n",
            "Loss :  0.05295548960566521\n",
            "Loss :  0.052844579021135964\n",
            "Loss :  0.05217978226081017\n",
            "Loss :  0.05265776276951883\n",
            "Loss :  0.05288919835698371\n",
            "Loss :  0.05321432766122896\n",
            "Loss :  0.053510815587262034\n",
            "Loss :  0.05371287281130567\n",
            "Loss :  0.05405455782682031\n",
            "Loss :  0.053950014459614705\n",
            "Loss :  0.0539310402698345\n",
            "Loss :  0.05372547515290828\n",
            "Loss :  0.053830766581169524\n",
            "Loss :  0.05410055190007737\n",
            "Loss :  0.054088337482600814\n",
            "Loss :  0.05402438793771015\n",
            "Loss :  0.05403991825660767\n",
            "Loss :  0.05411428313969907\n",
            "Loss :  0.054122879555088065\n",
            "Loss :  0.054109407097694295\n",
            "Loss :  0.05413531153617312\n",
            "Loss :  0.05413582145628346\n",
            "Loss :  0.054162518593010966\n",
            "Loss :  0.05421725226202941\n",
            "Loss :  0.05423972757154727\n",
            "Loss :  0.05418404689865094\n",
            "Loss :  0.0541935109591792\n",
            "Loss :  0.0542205911381601\n",
            "Loss :  0.054204678182134924\n",
            "Loss :  0.05417345528586758\n",
            "Loss :  0.054174904498927465\n",
            "Loss :  0.05408544474023153\n",
            "Loss :  0.05412776454202718\n",
            "Loss :  0.05422430688684637\n",
            "Loss :  0.054251079779235045\n",
            "Loss :  0.0542779069218444\n",
            "Loss :  0.05433405621474323\n",
            "Loss :  0.05437037654788163\n",
            "Loss :  0.054299327856896784\n",
            "Loss :  0.054275812084166486\n",
            "Loss :  0.05425496189118592\n",
            "Loss :  0.05426815055717765\n",
            "Loss :  0.05425273957219312\n",
            "Loss :  0.054316747756231396\n",
            "Loss :  0.05438904221398867\n",
            "Loss :  0.05446566694664593\n",
            "Loss :  0.05447455954336563\n",
            "Loss :  0.054493549692283795\n",
            "Loss :  0.05445546394331635\n",
            "Validation: \n",
            " Loss :  0.054351624101400375\n",
            " Loss :  0.06059321406341735\n",
            " Loss :  0.06025082172780502\n",
            " Loss :  0.06061636858054849\n",
            " Loss :  0.06040330525533653\n",
            "\n",
            "Epoch: 85\n",
            "Loss :  0.06192765012383461\n",
            "Loss :  0.05285745317285711\n",
            "Loss :  0.052508178033999035\n",
            "Loss :  0.0528023520544652\n",
            "Loss :  0.05272697048579774\n",
            "Loss :  0.05291068290962892\n",
            "Loss :  0.05320293767774691\n",
            "Loss :  0.053481528037030934\n",
            "Loss :  0.05360638681385252\n",
            "Loss :  0.05390277566326843\n",
            "Loss :  0.05383079458433803\n",
            "Loss :  0.05399024452980574\n",
            "Loss :  0.05399963293563236\n",
            "Loss :  0.053930367136957084\n",
            "Loss :  0.054043942256599455\n",
            "Loss :  0.05413561553651134\n",
            "Loss :  0.05418670512967228\n",
            "Loss :  0.054229309902205106\n",
            "Loss :  0.0542287515548382\n",
            "Loss :  0.05412404547343079\n",
            "Loss :  0.05411305782658544\n",
            "Loss :  0.05408312256726044\n",
            "Loss :  0.054024449772964236\n",
            "Loss :  0.05397747223188867\n",
            "Loss :  0.05406636877673295\n",
            "Loss :  0.05416193175541452\n",
            "Loss :  0.05418781480855412\n",
            "Loss :  0.05420511519721953\n",
            "Loss :  0.05423976043471238\n",
            "Loss :  0.054303469270775\n",
            "Loss :  0.05431231110050433\n",
            "Loss :  0.0542809236255681\n",
            "Loss :  0.054196494471896846\n",
            "Loss :  0.05424397635721008\n",
            "Loss :  0.054290181861594976\n",
            "Loss :  0.05425338559595608\n",
            "Loss :  0.05428063082108867\n",
            "Loss :  0.05428376332969678\n",
            "Loss :  0.054276087172231646\n",
            "Loss :  0.05420256246957938\n",
            "Loss :  0.054197034410704996\n",
            "Loss :  0.05418610603632428\n",
            "Loss :  0.054189792084297486\n",
            "Loss :  0.054205546704425615\n",
            "Loss :  0.05428335105244265\n",
            "Loss :  0.05435314530015256\n",
            "Loss :  0.05441629030860944\n",
            "Loss :  0.054415254068665694\n",
            "Loss :  0.054400426759665325\n",
            "Loss :  0.05436348321026309\n",
            "Validation: \n",
            " Loss :  0.057814761996269226\n",
            " Loss :  0.06409097267758279\n",
            " Loss :  0.06338234227604983\n",
            " Loss :  0.06389995578859674\n",
            " Loss :  0.06351517090274963\n",
            "\n",
            "Epoch: 86\n",
            "Loss :  0.05909280106425285\n",
            "Loss :  0.05362913520498709\n",
            "Loss :  0.05279310242760749\n",
            "Loss :  0.05263141230229409\n",
            "Loss :  0.052962609725754436\n",
            "Loss :  0.05303342735358313\n",
            "Loss :  0.0535422324157152\n",
            "Loss :  0.053752738929970165\n",
            "Loss :  0.05396646040457266\n",
            "Loss :  0.05416322221140285\n",
            "Loss :  0.05397765139246931\n",
            "Loss :  0.0541078632210826\n",
            "Loss :  0.05395772970042938\n",
            "Loss :  0.0539266157411892\n",
            "Loss :  0.05395857873537862\n",
            "Loss :  0.054011611749004845\n",
            "Loss :  0.053942838787847426\n",
            "Loss :  0.053875598097928086\n",
            "Loss :  0.053857495133554076\n",
            "Loss :  0.053780867401217916\n",
            "Loss :  0.053690175752882936\n",
            "Loss :  0.05369812532623797\n",
            "Loss :  0.05372187311495591\n",
            "Loss :  0.05373506246384604\n",
            "Loss :  0.053759747737423515\n",
            "Loss :  0.05378317194749634\n",
            "Loss :  0.053770258623064705\n",
            "Loss :  0.053756351331833103\n",
            "Loss :  0.05380215457378757\n",
            "Loss :  0.05375793987174624\n",
            "Loss :  0.05368632529908637\n",
            "Loss :  0.05364794488936375\n",
            "Loss :  0.053565323828629614\n",
            "Loss :  0.05358465110427303\n",
            "Loss :  0.053660458953412045\n",
            "Loss :  0.05372650504239604\n",
            "Loss :  0.053786937095782104\n",
            "Loss :  0.05382490049635946\n",
            "Loss :  0.053888405264988344\n",
            "Loss :  0.05382113213962911\n",
            "Loss :  0.05379022838097261\n",
            "Loss :  0.05380342538194355\n",
            "Loss :  0.05381604438014948\n",
            "Loss :  0.05376139593477039\n",
            "Loss :  0.05379181051977367\n",
            "Loss :  0.053864451765419374\n",
            "Loss :  0.05391724970910916\n",
            "Loss :  0.05395627049472175\n",
            "Loss :  0.053971762321657055\n",
            "Loss :  0.05396309525320826\n",
            "Validation: \n",
            " Loss :  0.0556911826133728\n",
            " Loss :  0.06276450182000796\n",
            " Loss :  0.06281656462971758\n",
            " Loss :  0.0632123241170508\n",
            " Loss :  0.06292126516317144\n",
            "\n",
            "Epoch: 87\n",
            "Loss :  0.060612428933382034\n",
            "Loss :  0.05330124192617156\n",
            "Loss :  0.052876386436678115\n",
            "Loss :  0.05282009140618386\n",
            "Loss :  0.0532098494288398\n",
            "Loss :  0.0533217781168573\n",
            "Loss :  0.0533106198809186\n",
            "Loss :  0.05343968505170983\n",
            "Loss :  0.05361916285790043\n",
            "Loss :  0.053761329501867294\n",
            "Loss :  0.05387245623929666\n",
            "Loss :  0.05401849424516832\n",
            "Loss :  0.053765340900618186\n",
            "Loss :  0.053742032583433255\n",
            "Loss :  0.053890492397208584\n",
            "Loss :  0.05406942827910777\n",
            "Loss :  0.05407015315216521\n",
            "Loss :  0.05406521811907054\n",
            "Loss :  0.054063700818719126\n",
            "Loss :  0.05390088556636691\n",
            "Loss :  0.05396434744421522\n",
            "Loss :  0.05393735693670562\n",
            "Loss :  0.05399946138532453\n",
            "Loss :  0.054023136630718845\n",
            "Loss :  0.05402365411898407\n",
            "Loss :  0.054002733285208625\n",
            "Loss :  0.0539077067483659\n",
            "Loss :  0.053962384615217185\n",
            "Loss :  0.05399080129916982\n",
            "Loss :  0.05393593321807196\n",
            "Loss :  0.05389749380045159\n",
            "Loss :  0.05382215563459412\n",
            "Loss :  0.053762011926307855\n",
            "Loss :  0.05381359966905096\n",
            "Loss :  0.05389062580693502\n",
            "Loss :  0.053924506546085715\n",
            "Loss :  0.05391603483305083\n",
            "Loss :  0.053932264326717654\n",
            "Loss :  0.05398175654214198\n",
            "Loss :  0.053908129787201164\n",
            "Loss :  0.05387216031625681\n",
            "Loss :  0.053863535832314596\n",
            "Loss :  0.05383832913750424\n",
            "Loss :  0.05381773584230593\n",
            "Loss :  0.05387874219625715\n",
            "Loss :  0.053940374098569485\n",
            "Loss :  0.053966822793581996\n",
            "Loss :  0.053974246649732\n",
            "Loss :  0.05396532469473609\n",
            "Loss :  0.05393554608654345\n",
            "Validation: \n",
            " Loss :  0.05725201964378357\n",
            " Loss :  0.06258797131123997\n",
            " Loss :  0.06238585165361079\n",
            " Loss :  0.06260491974773955\n",
            " Loss :  0.06236648720539646\n",
            "\n",
            "Epoch: 88\n",
            "Loss :  0.06069200485944748\n",
            "Loss :  0.052143055268309334\n",
            "Loss :  0.052653741446279344\n",
            "Loss :  0.05269737325368389\n",
            "Loss :  0.05328971710873813\n",
            "Loss :  0.05324958345177127\n",
            "Loss :  0.05356215392468405\n",
            "Loss :  0.05368750007219718\n",
            "Loss :  0.053994576826139735\n",
            "Loss :  0.05416797085122748\n",
            "Loss :  0.05406431223053743\n",
            "Loss :  0.054001400196874466\n",
            "Loss :  0.05373431477418616\n",
            "Loss :  0.05358111118997326\n",
            "Loss :  0.05368186007683159\n",
            "Loss :  0.05365938293618082\n",
            "Loss :  0.053620977500765964\n",
            "Loss :  0.05372168735889663\n",
            "Loss :  0.05372107516747812\n",
            "Loss :  0.05365910223095205\n",
            "Loss :  0.0537033375185817\n",
            "Loss :  0.053810507147374315\n",
            "Loss :  0.053770088148197975\n",
            "Loss :  0.05374773011192099\n",
            "Loss :  0.053834200384582226\n",
            "Loss :  0.053772110566200015\n",
            "Loss :  0.05374125893659518\n",
            "Loss :  0.05381622489857937\n",
            "Loss :  0.05380225995575406\n",
            "Loss :  0.05374329389933868\n",
            "Loss :  0.053724676457255385\n",
            "Loss :  0.05371623651582712\n",
            "Loss :  0.05366530373831776\n",
            "Loss :  0.05371069321774644\n",
            "Loss :  0.053774974202393085\n",
            "Loss :  0.053814401367196334\n",
            "Loss :  0.05382676556747706\n",
            "Loss :  0.053843251939089794\n",
            "Loss :  0.05389377882513474\n",
            "Loss :  0.053816694051713285\n",
            "Loss :  0.05379242961877897\n",
            "Loss :  0.05376497734057062\n",
            "Loss :  0.05376201296433134\n",
            "Loss :  0.05372642514807995\n",
            "Loss :  0.05379316685488975\n",
            "Loss :  0.05385345771512277\n",
            "Loss :  0.053888851368634146\n",
            "Loss :  0.05391130369401788\n",
            "Loss :  0.05389548359504608\n",
            "Loss :  0.05383786134983275\n",
            "Validation: \n",
            " Loss :  0.05585835501551628\n",
            " Loss :  0.06176330592660677\n",
            " Loss :  0.061449104238574095\n",
            " Loss :  0.06193168894922147\n",
            " Loss :  0.0616263566929617\n",
            "\n",
            "Epoch: 89\n",
            "Loss :  0.0590197779238224\n",
            "Loss :  0.053117244758389213\n",
            "Loss :  0.05271990295676958\n",
            "Loss :  0.052714639012852026\n",
            "Loss :  0.05331144490983428\n",
            "Loss :  0.05323011187069556\n",
            "Loss :  0.05352150398443957\n",
            "Loss :  0.05332550464171759\n",
            "Loss :  0.053481720847848024\n",
            "Loss :  0.05370789434719872\n",
            "Loss :  0.053473005347912855\n",
            "Loss :  0.0535212426639355\n",
            "Loss :  0.05336483490984302\n",
            "Loss :  0.05347848734782852\n",
            "Loss :  0.05346519692886806\n",
            "Loss :  0.05341027052888018\n",
            "Loss :  0.05349708156463522\n",
            "Loss :  0.05354955940567262\n",
            "Loss :  0.053569208110235016\n",
            "Loss :  0.05359336917156948\n",
            "Loss :  0.05357820548080093\n",
            "Loss :  0.05357153764903828\n",
            "Loss :  0.0535841251289413\n",
            "Loss :  0.053590684403459744\n",
            "Loss :  0.053664106319183134\n",
            "Loss :  0.053630968147658734\n",
            "Loss :  0.05360554823816051\n",
            "Loss :  0.05365870765159491\n",
            "Loss :  0.0536647105715453\n",
            "Loss :  0.053611394581208935\n",
            "Loss :  0.053611878206662564\n",
            "Loss :  0.05364362125275986\n",
            "Loss :  0.05353853235317168\n",
            "Loss :  0.05353648555422837\n",
            "Loss :  0.05361108990970595\n",
            "Loss :  0.053640494117794556\n",
            "Loss :  0.05371035899647055\n",
            "Loss :  0.053736418897331245\n",
            "Loss :  0.053748997500327626\n",
            "Loss :  0.05366315133393268\n",
            "Loss :  0.05364683119956395\n",
            "Loss :  0.053655823189629255\n",
            "Loss :  0.05366033608549564\n",
            "Loss :  0.05365637481523777\n",
            "Loss :  0.053687492921914645\n",
            "Loss :  0.05370807820340482\n",
            "Loss :  0.05376145086875448\n",
            "Loss :  0.053746429827625834\n",
            "Loss :  0.053737346640557605\n",
            "Loss :  0.053698168843681116\n",
            "Validation: \n",
            " Loss :  0.057223524898290634\n",
            " Loss :  0.06287025571579025\n",
            " Loss :  0.06251996546620275\n",
            " Loss :  0.06288242559941089\n",
            " Loss :  0.06260902154040926\n",
            "\n",
            "Epoch: 90\n",
            "Loss :  0.057725515216588974\n",
            "Loss :  0.05325098505074328\n",
            "Loss :  0.054099331299463906\n",
            "Loss :  0.05405739226168202\n",
            "Loss :  0.05400450564012295\n",
            "Loss :  0.0538598548840074\n",
            "Loss :  0.05385651115755566\n",
            "Loss :  0.053839922738327106\n",
            "Loss :  0.05401993510715755\n",
            "Loss :  0.053960442379280764\n",
            "Loss :  0.05388773192125972\n",
            "Loss :  0.05385664531642252\n",
            "Loss :  0.05362269553271207\n",
            "Loss :  0.05355253133155007\n",
            "Loss :  0.05366072285893961\n",
            "Loss :  0.053750165275587944\n",
            "Loss :  0.05375448325128289\n",
            "Loss :  0.05372430475657446\n",
            "Loss :  0.053651012130369796\n",
            "Loss :  0.05353724747818178\n",
            "Loss :  0.053513818045160665\n",
            "Loss :  0.05349981650639484\n",
            "Loss :  0.05352765717859721\n",
            "Loss :  0.05354612741570968\n",
            "Loss :  0.053556141130533454\n",
            "Loss :  0.053564985138011646\n",
            "Loss :  0.053491499631797675\n",
            "Loss :  0.05356757862431537\n",
            "Loss :  0.053568070708008425\n",
            "Loss :  0.05351069383963277\n",
            "Loss :  0.05350087184545606\n",
            "Loss :  0.053470157398288275\n",
            "Loss :  0.05335576450481222\n",
            "Loss :  0.05336159592623797\n",
            "Loss :  0.0534294304107466\n",
            "Loss :  0.05342934177684308\n",
            "Loss :  0.05349616244063813\n",
            "Loss :  0.053507723717516005\n",
            "Loss :  0.053546174480689795\n",
            "Loss :  0.05348398575506857\n",
            "Loss :  0.05344811588191332\n",
            "Loss :  0.05340798798304985\n",
            "Loss :  0.053409360349178314\n",
            "Loss :  0.05340323113164326\n",
            "Loss :  0.053437188293049935\n",
            "Loss :  0.05350305989153369\n",
            "Loss :  0.05353588457350617\n",
            "Loss :  0.05355278556798674\n",
            "Loss :  0.05358430995700761\n",
            "Loss :  0.053511309776978676\n",
            "Validation: \n",
            " Loss :  0.054527610540390015\n",
            " Loss :  0.06110796793585732\n",
            " Loss :  0.060711382456668996\n",
            " Loss :  0.06111159407701649\n",
            " Loss :  0.06084356663955583\n",
            "\n",
            "Epoch: 91\n",
            "Loss :  0.05355125293135643\n",
            "Loss :  0.05234858766198158\n",
            "Loss :  0.053253170812413805\n",
            "Loss :  0.052758549610453266\n",
            "Loss :  0.05312194275419887\n",
            "Loss :  0.053065095608140905\n",
            "Loss :  0.05327561656471159\n",
            "Loss :  0.053172740613071014\n",
            "Loss :  0.05329841425941314\n",
            "Loss :  0.05372456322004507\n",
            "Loss :  0.053553897407975525\n",
            "Loss :  0.05342292463457262\n",
            "Loss :  0.05330710057631012\n",
            "Loss :  0.05332383564637817\n",
            "Loss :  0.05349200181927241\n",
            "Loss :  0.05348319047136812\n",
            "Loss :  0.0533622703428224\n",
            "Loss :  0.053186941238348946\n",
            "Loss :  0.05319346034724409\n",
            "Loss :  0.05315089594397245\n",
            "Loss :  0.053137686511325596\n",
            "Loss :  0.05315337789143431\n",
            "Loss :  0.05311755148264078\n",
            "Loss :  0.05305980033043659\n",
            "Loss :  0.0531525148302935\n",
            "Loss :  0.05318331304358771\n",
            "Loss :  0.053157823338705\n",
            "Loss :  0.05314218331252077\n",
            "Loss :  0.05315657868132897\n",
            "Loss :  0.053138642709484624\n",
            "Loss :  0.05312890080468995\n",
            "Loss :  0.05312018300080223\n",
            "Loss :  0.053015509138690346\n",
            "Loss :  0.053026867552106116\n",
            "Loss :  0.05311290328759602\n",
            "Loss :  0.05313719658345578\n",
            "Loss :  0.05320495337124016\n",
            "Loss :  0.05323351379433732\n",
            "Loss :  0.0532236092594352\n",
            "Loss :  0.05315590931860077\n",
            "Loss :  0.05314749530388827\n",
            "Loss :  0.053138831653009075\n",
            "Loss :  0.05313723490102274\n",
            "Loss :  0.05311409282884852\n",
            "Loss :  0.05320313775620493\n",
            "Loss :  0.05319668804429587\n",
            "Loss :  0.05324295978516405\n",
            "Loss :  0.05327275384320322\n",
            "Loss :  0.05327140879885075\n",
            "Loss :  0.05325714941089119\n",
            "Validation: \n",
            " Loss :  0.05629243329167366\n",
            " Loss :  0.062211567092509495\n",
            " Loss :  0.062350331919222346\n",
            " Loss :  0.06271701880165788\n",
            " Loss :  0.06233193568977309\n",
            "\n",
            "Epoch: 92\n",
            "Loss :  0.05637562274932861\n",
            "Loss :  0.05339771509170532\n",
            "Loss :  0.053609829928193777\n",
            "Loss :  0.053006793582631696\n",
            "Loss :  0.052942222120558345\n",
            "Loss :  0.05307627827221272\n",
            "Loss :  0.05297389559325625\n",
            "Loss :  0.05287171879284818\n",
            "Loss :  0.05301327235352846\n",
            "Loss :  0.053289026472267215\n",
            "Loss :  0.05305673598800555\n",
            "Loss :  0.05306090226581505\n",
            "Loss :  0.052912641686102574\n",
            "Loss :  0.0528618264232428\n",
            "Loss :  0.05295441880927864\n",
            "Loss :  0.05303850893350626\n",
            "Loss :  0.05299126102317194\n",
            "Loss :  0.05301058414386727\n",
            "Loss :  0.05305609158471803\n",
            "Loss :  0.05295158002076973\n",
            "Loss :  0.05294340937884886\n",
            "Loss :  0.05295715341565168\n",
            "Loss :  0.05294194906882571\n",
            "Loss :  0.05291532689616794\n",
            "Loss :  0.053017309994371105\n",
            "Loss :  0.05308161307853532\n",
            "Loss :  0.05303418704833107\n",
            "Loss :  0.05311614361581327\n",
            "Loss :  0.05313832052980029\n",
            "Loss :  0.05318512553765192\n",
            "Loss :  0.05316667466662651\n",
            "Loss :  0.05320384443237467\n",
            "Loss :  0.05312922854568357\n",
            "Loss :  0.05315521553023704\n",
            "Loss :  0.05322152387099532\n",
            "Loss :  0.053195162970795594\n",
            "Loss :  0.05320908178748186\n",
            "Loss :  0.05326142303665693\n",
            "Loss :  0.053269100881467656\n",
            "Loss :  0.05317611028166378\n",
            "Loss :  0.053140548808021736\n",
            "Loss :  0.05310012629468656\n",
            "Loss :  0.05310506082522614\n",
            "Loss :  0.053103993903940626\n",
            "Loss :  0.053127419093704005\n",
            "Loss :  0.05320915831727886\n",
            "Loss :  0.053223693343344064\n",
            "Loss :  0.05321759679957813\n",
            "Loss :  0.05319939030183328\n",
            "Loss :  0.05316648499512867\n",
            "Validation: \n",
            " Loss :  0.05523989722132683\n",
            " Loss :  0.06363762631302788\n",
            " Loss :  0.06355063771692718\n",
            " Loss :  0.0637181133764689\n",
            " Loss :  0.06336482591283174\n",
            "\n",
            "Epoch: 93\n",
            "Loss :  0.048619937151670456\n",
            "Loss :  0.0510261363603852\n",
            "Loss :  0.051694843208505994\n",
            "Loss :  0.051905894712094336\n",
            "Loss :  0.05235112476639631\n",
            "Loss :  0.052273696763258354\n",
            "Loss :  0.052154657049257244\n",
            "Loss :  0.05221290666032845\n",
            "Loss :  0.052463754965567294\n",
            "Loss :  0.052713020285079765\n",
            "Loss :  0.05280740861550416\n",
            "Loss :  0.05280076866751319\n",
            "Loss :  0.05270246561031696\n",
            "Loss :  0.052707974066716116\n",
            "Loss :  0.052872297928688375\n",
            "Loss :  0.0528019160920421\n",
            "Loss :  0.05270835866006265\n",
            "Loss :  0.052810369854607776\n",
            "Loss :  0.052853867308704894\n",
            "Loss :  0.05278434711679114\n",
            "Loss :  0.05280342253286447\n",
            "Loss :  0.052825149572432326\n",
            "Loss :  0.052714962107698306\n",
            "Loss :  0.052745477042414925\n",
            "Loss :  0.05276735711270843\n",
            "Loss :  0.0528025127264608\n",
            "Loss :  0.052726009853498235\n",
            "Loss :  0.05275630882195441\n",
            "Loss :  0.05281288753465826\n",
            "Loss :  0.0527877512666368\n",
            "Loss :  0.05277608429078644\n",
            "Loss :  0.05281297551090694\n",
            "Loss :  0.052685233372672696\n",
            "Loss :  0.0527123940877864\n",
            "Loss :  0.0527731998225461\n",
            "Loss :  0.05279222886446874\n",
            "Loss :  0.052826596563294984\n",
            "Loss :  0.05283942406951899\n",
            "Loss :  0.052902678912627725\n",
            "Loss :  0.05282170931472803\n",
            "Loss :  0.0528131977206751\n",
            "Loss :  0.05278610554127217\n",
            "Loss :  0.05274472481210555\n",
            "Loss :  0.05271821581432272\n",
            "Loss :  0.05271384557561269\n",
            "Loss :  0.05275656880104621\n",
            "Loss :  0.052777245382166216\n",
            "Loss :  0.05278970542385573\n",
            "Loss :  0.05279688056163381\n",
            "Loss :  0.05277320491472475\n",
            "Validation: \n",
            " Loss :  0.05366141349077225\n",
            " Loss :  0.06177245701352755\n",
            " Loss :  0.06154370244319846\n",
            " Loss :  0.06166472133310115\n",
            " Loss :  0.06125150386382033\n",
            "\n",
            "Epoch: 94\n",
            "Loss :  0.05717407166957855\n",
            "Loss :  0.0523628717796369\n",
            "Loss :  0.05226238799237069\n",
            "Loss :  0.051911554028910976\n",
            "Loss :  0.05224399786533379\n",
            "Loss :  0.05217171606479907\n",
            "Loss :  0.0521843752167264\n",
            "Loss :  0.05213610576072209\n",
            "Loss :  0.052224059071805745\n",
            "Loss :  0.052442380892378945\n",
            "Loss :  0.052407046097635045\n",
            "Loss :  0.05238700037201246\n",
            "Loss :  0.05235521843241266\n",
            "Loss :  0.052262791467986944\n",
            "Loss :  0.052359332138976304\n",
            "Loss :  0.05242376485901163\n",
            "Loss :  0.052350924437090475\n",
            "Loss :  0.05237572531253971\n",
            "Loss :  0.05240812920322076\n",
            "Loss :  0.05234492638426301\n",
            "Loss :  0.052351141580153464\n",
            "Loss :  0.05243194498722022\n",
            "Loss :  0.05241636202009015\n",
            "Loss :  0.05236605688671529\n",
            "Loss :  0.052420563114752905\n",
            "Loss :  0.052389646327471825\n",
            "Loss :  0.05238663041214833\n",
            "Loss :  0.052400628661537524\n",
            "Loss :  0.05240267739230203\n",
            "Loss :  0.05237704505215805\n",
            "Loss :  0.05237310034442581\n",
            "Loss :  0.05240599387040859\n",
            "Loss :  0.05232798024965595\n",
            "Loss :  0.052358950170325366\n",
            "Loss :  0.052408027574638465\n",
            "Loss :  0.05240853800520598\n",
            "Loss :  0.0523916163334721\n",
            "Loss :  0.05242200061237073\n",
            "Loss :  0.05242359517948834\n",
            "Loss :  0.052313608843880845\n",
            "Loss :  0.05232302650363368\n",
            "Loss :  0.05232195822644408\n",
            "Loss :  0.05230365056464904\n",
            "Loss :  0.052292265672141604\n",
            "Loss :  0.052391529750594204\n",
            "Loss :  0.05244924026456746\n",
            "Loss :  0.052462782735033305\n",
            "Loss :  0.052471653686840314\n",
            "Loss :  0.05250034341879049\n",
            "Loss :  0.052445060207438325\n",
            "Validation: \n",
            " Loss :  0.05724209547042847\n",
            " Loss :  0.062034282833337784\n",
            " Loss :  0.061837689451328136\n",
            " Loss :  0.06211042031645775\n",
            " Loss :  0.061898359683928664\n",
            "\n",
            "Epoch: 95\n",
            "Loss :  0.05427905544638634\n",
            "Loss :  0.05125438794493675\n",
            "Loss :  0.05161636553349949\n",
            "Loss :  0.05141186185421482\n",
            "Loss :  0.05175478847288504\n",
            "Loss :  0.05181443530554865\n",
            "Loss :  0.0518919611197026\n",
            "Loss :  0.052011461377563614\n",
            "Loss :  0.05221426468572499\n",
            "Loss :  0.05238978834925117\n",
            "Loss :  0.05233245398295988\n",
            "Loss :  0.05243243958364736\n",
            "Loss :  0.052267356352372604\n",
            "Loss :  0.052240840806305866\n",
            "Loss :  0.05220574452310589\n",
            "Loss :  0.052244998119919506\n",
            "Loss :  0.05213810973193334\n",
            "Loss :  0.05213230438748298\n",
            "Loss :  0.05217048636951499\n",
            "Loss :  0.052157552805558546\n",
            "Loss :  0.05215311157910978\n",
            "Loss :  0.05224681209543305\n",
            "Loss :  0.05220026163831016\n",
            "Loss :  0.05217067013571273\n",
            "Loss :  0.052268296418348285\n",
            "Loss :  0.05233732274032209\n",
            "Loss :  0.052321992100883716\n",
            "Loss :  0.05237099359231241\n",
            "Loss :  0.05240198094861787\n",
            "Loss :  0.05240100272691127\n",
            "Loss :  0.052365983047358615\n",
            "Loss :  0.052337005997969026\n",
            "Loss :  0.0522413756872449\n",
            "Loss :  0.05219492301614983\n",
            "Loss :  0.052240554844179464\n",
            "Loss :  0.052237982455736555\n",
            "Loss :  0.05228647878625717\n",
            "Loss :  0.05231982061885438\n",
            "Loss :  0.05235391356578962\n",
            "Loss :  0.05229603689726051\n",
            "Loss :  0.05227200015264556\n",
            "Loss :  0.052299262044856146\n",
            "Loss :  0.05231705004451111\n",
            "Loss :  0.05231974634348378\n",
            "Loss :  0.05236201445295427\n",
            "Loss :  0.05239835392245961\n",
            "Loss :  0.05241283131945159\n",
            "Loss :  0.052399536622431124\n",
            "Loss :  0.05240753096317303\n",
            "Loss :  0.05239863395539175\n",
            "Validation: \n",
            " Loss :  0.05657957121729851\n",
            " Loss :  0.06336685482944761\n",
            " Loss :  0.06298589124912168\n",
            " Loss :  0.06309265551752731\n",
            " Loss :  0.06268019440733356\n",
            "\n",
            "Epoch: 96\n",
            "Loss :  0.059411730617284775\n",
            "Loss :  0.051897202703085815\n",
            "Loss :  0.050950850936628524\n",
            "Loss :  0.050930161870295004\n",
            "Loss :  0.05127471385569107\n",
            "Loss :  0.05124618675486714\n",
            "Loss :  0.051245272770279744\n",
            "Loss :  0.05142373633636555\n",
            "Loss :  0.05167963328184905\n",
            "Loss :  0.052036231076651875\n",
            "Loss :  0.051790631606732265\n",
            "Loss :  0.051815998849568064\n",
            "Loss :  0.05174538166808688\n",
            "Loss :  0.051776633974705034\n",
            "Loss :  0.05179437973820571\n",
            "Loss :  0.051844323041620635\n",
            "Loss :  0.05176635750609895\n",
            "Loss :  0.05179079896525333\n",
            "Loss :  0.05178447038923179\n",
            "Loss :  0.05169393046130061\n",
            "Loss :  0.0517655389744844\n",
            "Loss :  0.05184503136229176\n",
            "Loss :  0.05191039418742668\n",
            "Loss :  0.051939643451790786\n",
            "Loss :  0.05199503300410089\n",
            "Loss :  0.052034353577639476\n",
            "Loss :  0.05200443943512851\n",
            "Loss :  0.05206428547956847\n",
            "Loss :  0.0520772800857062\n",
            "Loss :  0.05207226356280219\n",
            "Loss :  0.052042804225239644\n",
            "Loss :  0.051981794057934995\n",
            "Loss :  0.05190768322654973\n",
            "Loss :  0.05195600023982748\n",
            "Loss :  0.052003219865721345\n",
            "Loss :  0.05208400784437133\n",
            "Loss :  0.05214636335329996\n",
            "Loss :  0.052163679865653304\n",
            "Loss :  0.05221034487675181\n",
            "Loss :  0.052115445999461975\n",
            "Loss :  0.05207225413728236\n",
            "Loss :  0.052073042604102414\n",
            "Loss :  0.05206636174840769\n",
            "Loss :  0.05205538438464539\n",
            "Loss :  0.05211350268551281\n",
            "Loss :  0.0521561391238627\n",
            "Loss :  0.0522065134618003\n",
            "Loss :  0.05222814762668245\n",
            "Loss :  0.05223588784866175\n",
            "Loss :  0.05219400312956385\n",
            "Validation: \n",
            " Loss :  0.05608329549431801\n",
            " Loss :  0.061624333972022646\n",
            " Loss :  0.0615934790452806\n",
            " Loss :  0.06209090974975805\n",
            " Loss :  0.061828182619295\n",
            "\n",
            "Epoch: 97\n",
            "Loss :  0.06285364925861359\n",
            "Loss :  0.052350289103659714\n",
            "Loss :  0.05146995169066247\n",
            "Loss :  0.05151728564693082\n",
            "Loss :  0.05138081194060605\n",
            "Loss :  0.05134642942278993\n",
            "Loss :  0.05147470008642947\n",
            "Loss :  0.051504122224492084\n",
            "Loss :  0.05175653279379562\n",
            "Loss :  0.052028764898960404\n",
            "Loss :  0.05185953745304948\n",
            "Loss :  0.05179535016954482\n",
            "Loss :  0.05165758076285528\n",
            "Loss :  0.051463381963149285\n",
            "Loss :  0.051587517459130455\n",
            "Loss :  0.05159346517545498\n",
            "Loss :  0.05157125535311166\n",
            "Loss :  0.05168796217406702\n",
            "Loss :  0.05164230195033616\n",
            "Loss :  0.05148312164694851\n",
            "Loss :  0.051524611569903976\n",
            "Loss :  0.05153030561397991\n",
            "Loss :  0.051563870161771774\n",
            "Loss :  0.05160685531891786\n",
            "Loss :  0.05167021458636181\n",
            "Loss :  0.05172833715717631\n",
            "Loss :  0.051714200084008476\n",
            "Loss :  0.051674250664526244\n",
            "Loss :  0.05165731616556857\n",
            "Loss :  0.051723231833517756\n",
            "Loss :  0.0516958115158287\n",
            "Loss :  0.05167754406522708\n",
            "Loss :  0.051590169956936646\n",
            "Loss :  0.0516203033690006\n",
            "Loss :  0.05171161723975912\n",
            "Loss :  0.051731810408864605\n",
            "Loss :  0.05176646356313513\n",
            "Loss :  0.05179810569172278\n",
            "Loss :  0.05179922378516886\n",
            "Loss :  0.05171858484063612\n",
            "Loss :  0.05173685753769114\n",
            "Loss :  0.05174881108377102\n",
            "Loss :  0.05178996665743921\n",
            "Loss :  0.05176737897633953\n",
            "Loss :  0.051777089551057406\n",
            "Loss :  0.05185032038724343\n",
            "Loss :  0.0519045124359736\n",
            "Loss :  0.05194131107783368\n",
            "Loss :  0.05196495505975338\n",
            "Loss :  0.051918060384129314\n",
            "Validation: \n",
            " Loss :  0.054508913308382034\n",
            " Loss :  0.06013561607826324\n",
            " Loss :  0.06016419900626671\n",
            " Loss :  0.060516653796199894\n",
            " Loss :  0.06031830677831614\n",
            "\n",
            "Epoch: 98\n",
            "Loss :  0.05641048774123192\n",
            "Loss :  0.05182854763486169\n",
            "Loss :  0.051694851191270916\n",
            "Loss :  0.05132468705696444\n",
            "Loss :  0.05148508526929995\n",
            "Loss :  0.051522240627045725\n",
            "Loss :  0.051512310006579416\n",
            "Loss :  0.05163046171967412\n",
            "Loss :  0.05179526390116892\n",
            "Loss :  0.052048398284139215\n",
            "Loss :  0.051951057434377106\n",
            "Loss :  0.0519259762992193\n",
            "Loss :  0.05179832992721195\n",
            "Loss :  0.05172228821702586\n",
            "Loss :  0.05189905051432603\n",
            "Loss :  0.05202284243130526\n",
            "Loss :  0.05197000869127534\n",
            "Loss :  0.05195925759467465\n",
            "Loss :  0.05198542499986801\n",
            "Loss :  0.05190573679527063\n",
            "Loss :  0.051903370452757495\n",
            "Loss :  0.05198687685750672\n",
            "Loss :  0.0519638606326073\n",
            "Loss :  0.05196680792998442\n",
            "Loss :  0.05200732381883004\n",
            "Loss :  0.052006545072176544\n",
            "Loss :  0.051987208254720974\n",
            "Loss :  0.051961848642883265\n",
            "Loss :  0.051999782672768384\n",
            "Loss :  0.05198320189548522\n",
            "Loss :  0.05198146541451299\n",
            "Loss :  0.05192374120619136\n",
            "Loss :  0.05186006918874485\n",
            "Loss :  0.05186055166213534\n",
            "Loss :  0.05192605496720135\n",
            "Loss :  0.05192005356237759\n",
            "Loss :  0.05191060388847731\n",
            "Loss :  0.05189433815384489\n",
            "Loss :  0.05197637137074483\n",
            "Loss :  0.05185729740640087\n",
            "Loss :  0.051823555791467205\n",
            "Loss :  0.051820235159202796\n",
            "Loss :  0.051785282560595426\n",
            "Loss :  0.05180299195640325\n",
            "Loss :  0.05183366331316176\n",
            "Loss :  0.05189752710896427\n",
            "Loss :  0.05193375425974873\n",
            "Loss :  0.05196171688141337\n",
            "Loss :  0.05196016705488465\n",
            "Loss :  0.05190485217390381\n",
            "Validation: \n",
            " Loss :  0.05689864978194237\n",
            " Loss :  0.062288545604263036\n",
            " Loss :  0.062014269029221886\n",
            " Loss :  0.06223723970231463\n",
            " Loss :  0.062026326073172655\n",
            "\n",
            "Epoch: 99\n",
            "Loss :  0.06374762207269669\n",
            "Loss :  0.05135031315413388\n",
            "Loss :  0.05068222184975942\n",
            "Loss :  0.05119867286374492\n",
            "Loss :  0.05166142442967833\n",
            "Loss :  0.05151592932787596\n",
            "Loss :  0.05144395740305791\n",
            "Loss :  0.051455006370661964\n",
            "Loss :  0.05143203029845968\n",
            "Loss :  0.05157195690732736\n",
            "Loss :  0.051488683807967915\n",
            "Loss :  0.051694366543948114\n",
            "Loss :  0.05144293295327297\n",
            "Loss :  0.051463368114169315\n",
            "Loss :  0.051495914790013156\n",
            "Loss :  0.05159984218166364\n",
            "Loss :  0.051576842663821225\n",
            "Loss :  0.05162381618256457\n",
            "Loss :  0.05157829796411714\n",
            "Loss :  0.05147682459956689\n",
            "Loss :  0.051526483481944496\n",
            "Loss :  0.051465570909010855\n",
            "Loss :  0.05145973128970392\n",
            "Loss :  0.051546306385622395\n",
            "Loss :  0.05158486447579139\n",
            "Loss :  0.05157154666415724\n",
            "Loss :  0.051538691652574756\n",
            "Loss :  0.05157400157673772\n",
            "Loss :  0.05158265221373466\n",
            "Loss :  0.05156480920376237\n",
            "Loss :  0.05156410059758595\n",
            "Loss :  0.05155675827115295\n",
            "Loss :  0.0514468360007552\n",
            "Loss :  0.05145422700128526\n",
            "Loss :  0.05149201391117314\n",
            "Loss :  0.051536322053935796\n",
            "Loss :  0.05157532179702352\n",
            "Loss :  0.05158966914261127\n",
            "Loss :  0.05164246039166851\n",
            "Loss :  0.05155955924821631\n",
            "Loss :  0.05149606568854943\n",
            "Loss :  0.05146004328472481\n",
            "Loss :  0.051483679642445115\n",
            "Loss :  0.05145378209425789\n",
            "Loss :  0.0515042407746488\n",
            "Loss :  0.05156304660829367\n",
            "Loss :  0.05163855427257927\n",
            "Loss :  0.05163673478358617\n",
            "Loss :  0.05164812826657989\n",
            "Loss :  0.05163440920882944\n",
            "Validation: \n",
            " Loss :  0.05496005341410637\n",
            " Loss :  0.062496506564673926\n",
            " Loss :  0.0626286172830477\n",
            " Loss :  0.06292824685329297\n",
            " Loss :  0.06264482135022129\n",
            "\n",
            "Epoch: 100\n",
            "Loss :  0.052265465259552\n",
            "Loss :  0.05092886462807655\n",
            "Loss :  0.05088927366194271\n",
            "Loss :  0.05022581883015171\n",
            "Loss :  0.050846517358611266\n",
            "Loss :  0.05086465523231263\n",
            "Loss :  0.05086408934143723\n",
            "Loss :  0.05121688245677612\n",
            "Loss :  0.051329603901615846\n",
            "Loss :  0.051706177151792654\n",
            "Loss :  0.05163129939153643\n",
            "Loss :  0.051628736124650854\n",
            "Loss :  0.051606440808901116\n",
            "Loss :  0.051686409178353446\n",
            "Loss :  0.05180296576614921\n",
            "Loss :  0.05185031762580998\n",
            "Loss :  0.051843864345772664\n",
            "Loss :  0.051868564008097896\n",
            "Loss :  0.051896101265157785\n",
            "Loss :  0.051811078592588766\n",
            "Loss :  0.05181558671134028\n",
            "Loss :  0.051790200880071\n",
            "Loss :  0.0517698630050027\n",
            "Loss :  0.051784451712261544\n",
            "Loss :  0.05184899343185405\n",
            "Loss :  0.05185812503930582\n",
            "Loss :  0.05185056122383852\n",
            "Loss :  0.0518951580973129\n",
            "Loss :  0.05189176015158141\n",
            "Loss :  0.05186982679459238\n",
            "Loss :  0.05183703612004008\n",
            "Loss :  0.05176912363127497\n",
            "Loss :  0.051705967344784665\n",
            "Loss :  0.05174526928999993\n",
            "Loss :  0.051777602902605385\n",
            "Loss :  0.051782512469509046\n",
            "Loss :  0.05179115119609476\n",
            "Loss :  0.05179315773303297\n",
            "Loss :  0.051836840896390554\n",
            "Loss :  0.051741586917120476\n",
            "Loss :  0.051763720345913325\n",
            "Loss :  0.05178053855642205\n",
            "Loss :  0.051778485640140155\n",
            "Loss :  0.05173177725858036\n",
            "Loss :  0.05176715773283219\n",
            "Loss :  0.051830352335639644\n",
            "Loss :  0.051908792503614486\n",
            "Loss :  0.05189050670768552\n",
            "Loss :  0.051861366477800756\n",
            "Loss :  0.051848753315610945\n",
            "Validation: \n",
            " Loss :  0.05715262144804001\n",
            " Loss :  0.06368664031227429\n",
            " Loss :  0.06336690694457148\n",
            " Loss :  0.06360434093436257\n",
            " Loss :  0.06336126159186717\n"
          ]
        }
      ],
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+100):\n",
        "    train(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test(epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHEGzhc-53nG"
      },
      "outputs": [],
      "source": [
        "# save model parameters\n",
        "PATH = \"/content/gdrive/MyDrive/Deep Learning/A5/s1_model\"\n",
        "torch.save(s1.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWu9Jy1vHcDA"
      },
      "source": [
        "#Finetuning student on crossentropy\n",
        "1.5 Now the student is trained. In this cell you need to replace the classifier(i.e: Fully Connected layer) of student network from one with output shape of dense feature to one with shape of classes. e.g: nn.Linear(256,512) to nn.Linear(256,10). After this you need to freez Conv layers in the network and finetune the network using orignal dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Poiy07J279OR",
        "outputId": "97480de9-7b04-424f-a39a-166987c0cec1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 -> VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU(inplace=True)\n",
            "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (12): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (14): ReLU(inplace=True)\n",
            "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (20): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            "  )\n",
            "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "<generator object Module.parameters at 0x7f4ab531f0d0>\n",
            "1 -> Sequential(\n",
            "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): ReLU(inplace=True)\n",
            "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (6): ReLU(inplace=True)\n",
            "  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (10): ReLU(inplace=True)\n",
            "  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (12): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (14): ReLU(inplace=True)\n",
            "  (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (18): ReLU(inplace=True)\n",
            "  (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (20): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            ")\n",
            "<generator object Module.parameters at 0x7f4ab531f050>\n",
            "2 -> Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f4ab531fed0>\n",
            "3 -> BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f4ab531fed0>\n",
            "4 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f4ab531fed0>\n",
            "5 -> MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "<generator object Module.parameters at 0x7f4ab531f2d0>\n",
            "6 -> Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f4ab531f6d0>\n",
            "7 -> BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f4ab531f6d0>\n",
            "8 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f4ab531f6d0>\n",
            "9 -> MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "<generator object Module.parameters at 0x7f4ab531fad0>\n",
            "10 -> Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f4ab531f650>\n",
            "11 -> BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f4ab531f650>\n",
            "12 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f4ab531f650>\n",
            "13 -> MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "<generator object Module.parameters at 0x7f4ab531f3d0>\n",
            "14 -> Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f4ab531f8d0>\n",
            "15 -> BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f4ab531f8d0>\n",
            "16 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f4ab531f8d0>\n",
            "17 -> MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "<generator object Module.parameters at 0x7f4ab531fb50>\n",
            "18 -> Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f4ab531fbd0>\n",
            "19 -> BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f4ab531fbd0>\n",
            "20 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f4ab531fbd0>\n",
            "21 -> MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "<generator object Module.parameters at 0x7f4ab531f5d0>\n",
            "22 -> AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            "<generator object Module.parameters at 0x7f4ab531f5d0>\n",
            "23 -> Linear(in_features=512, out_features=10, bias=True)\n",
            "<generator object Module.parameters at 0x7f4ab531f050>\n"
          ]
        }
      ],
      "source": [
        "s1.classifier = nn.Linear(512, 10)\n",
        "for i, m in enumerate(s1.modules()):\n",
        "  print(i, '->', m)\n",
        "  print(m.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Uq-ZNpGAlFk",
        "outputId": "d0443cbf-7f75-4a18-f968-e17a476f471a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 64, 16, 16]               0\n",
            "            Conv2d-5          [-1, 128, 16, 16]          73,856\n",
            "       BatchNorm2d-6          [-1, 128, 16, 16]             256\n",
            "              ReLU-7          [-1, 128, 16, 16]               0\n",
            "         MaxPool2d-8            [-1, 128, 8, 8]               0\n",
            "            Conv2d-9            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-10            [-1, 128, 8, 8]             256\n",
            "             ReLU-11            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-12            [-1, 128, 4, 4]               0\n",
            "           Conv2d-13            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-14            [-1, 256, 4, 4]             512\n",
            "             ReLU-15            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-16            [-1, 256, 2, 2]               0\n",
            "           Conv2d-17            [-1, 512, 2, 2]       1,180,160\n",
            "      BatchNorm2d-18            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-19            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-20            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-21            [-1, 512, 1, 1]               0\n",
            "           Linear-22                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 1,705,866\n",
            "Trainable params: 7,306\n",
            "Non-trainable params: 1,698,560\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.80\n",
            "Params size (MB): 6.51\n",
            "Estimated Total Size (MB): 9.32\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "s1.classifier = nn.Linear(512, 10)\n",
        "\n",
        "# This is found by printing the model summary abouve using the s1.module() and then finding the index of convolutional layers\n",
        "index_of_conv_layers = [2,6,10,14,18]\n",
        "\n",
        "for i, m in enumerate(s1.modules()):\n",
        "    # only update if the index matches index of a conv2d layer\n",
        "    if i in index_of_conv_layers:\n",
        "      for param in m.parameters():\n",
        "        param.requires_grad = False\n",
        "s1 = s1.to(device)\n",
        "summary(s1, (3, 32, 32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4pbyfqhDyvF"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(s1.parameters(), lr=0.0001)\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    s1.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        inputs = Variable(inputs, requires_grad=False)\n",
        "        targets = Variable(targets)\n",
        "        s1.zero_grad()\n",
        "        outputs = s1(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if(batch_idx % 200 == 0):\n",
        "          print(\"Accuracy : \",100.*correct/total,\" Loss : \", train_loss/(batch_idx+1))\n",
        "def test(epoch):\n",
        "    s1.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = s1(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\"Accuracy : \",100.*correct/total,\" Loss : \", test_loss/(batch_idx+1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHp37wF7DG5V",
        "outputId": "95dfa0a7-3166-4f51-d218-7f1f5ff20248"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Accuracy :  3.0  Loss :  2.4733028411865234\n",
            "Accuracy :  54.76119402985075  Loss :  1.647194907439882\n",
            "Accuracy :  66.94513715710723  Loss :  1.3137869756120697\n",
            "Validation: \n",
            "Accuracy :  76.0  Loss :  0.7670699954032898\n",
            "Accuracy :  81.38095238095238  Loss :  0.7331053870064872\n",
            "Accuracy :  80.7560975609756  Loss :  0.7469167898340923\n",
            "Accuracy :  80.80327868852459  Loss :  0.7457917498760536\n",
            "Accuracy :  80.66666666666667  Loss :  0.7424066868829139\n",
            "\n",
            "Epoch: 2\n",
            "Accuracy :  84.0  Loss :  0.7549332976341248\n",
            "Accuracy :  82.78109452736318  Loss :  0.6767922316617634\n",
            "Accuracy :  83.3790523690773  Loss :  0.632064999413312\n",
            "Validation: \n",
            "Accuracy :  79.0  Loss :  0.5862411260604858\n",
            "Accuracy :  82.71428571428571  Loss :  0.5528255601723989\n",
            "Accuracy :  82.34146341463415  Loss :  0.5667841986912053\n",
            "Accuracy :  82.39344262295081  Loss :  0.5643514595070823\n",
            "Accuracy :  82.33333333333333  Loss :  0.5607238746719596\n",
            "\n",
            "Epoch: 3\n",
            "Accuracy :  89.0  Loss :  0.4995458722114563\n",
            "Accuracy :  84.42786069651741  Loss :  0.5191625539046615\n",
            "Accuracy :  84.63341645885286  Loss :  0.5033329364813474\n",
            "Validation: \n",
            "Accuracy :  82.0  Loss :  0.5209563970565796\n",
            "Accuracy :  83.38095238095238  Loss :  0.49844867558706374\n",
            "Accuracy :  82.97560975609755  Loss :  0.512792929643538\n",
            "Accuracy :  83.14754098360656  Loss :  0.5098206332472505\n",
            "Accuracy :  83.23456790123457  Loss :  0.5060758369940298\n",
            "\n",
            "Epoch: 4\n",
            "Accuracy :  88.0  Loss :  0.43674686551094055\n",
            "Accuracy :  84.82587064676616  Loss :  0.4641825624959386\n",
            "Accuracy :  85.0423940149626  Loss :  0.45718688278126896\n",
            "Validation: \n",
            "Accuracy :  85.0  Loss :  0.4857045114040375\n",
            "Accuracy :  84.04761904761905  Loss :  0.4753216469571704\n",
            "Accuracy :  83.4390243902439  Loss :  0.4900832383370981\n",
            "Accuracy :  83.63934426229508  Loss :  0.48646040065366714\n",
            "Accuracy :  83.77777777777777  Loss :  0.4827068159241735\n",
            "\n",
            "Epoch: 5\n",
            "Accuracy :  90.0  Loss :  0.38811326026916504\n",
            "Accuracy :  85.44776119402985  Loss :  0.4363495851334055\n",
            "Accuracy :  85.66334164588528  Loss :  0.43010500322404943\n",
            "Validation: \n",
            "Accuracy :  85.0  Loss :  0.46741196513175964\n",
            "Accuracy :  84.19047619047619  Loss :  0.46388059783549535\n",
            "Accuracy :  83.6829268292683  Loss :  0.4785203225002056\n",
            "Accuracy :  83.81967213114754  Loss :  0.4749690291822934\n",
            "Accuracy :  83.91358024691358  Loss :  0.47147346481128977\n",
            "\n",
            "Epoch: 6\n",
            "Accuracy :  90.0  Loss :  0.3125387132167816\n",
            "Accuracy :  85.71144278606965  Loss :  0.42109148634310384\n",
            "Accuracy :  85.88279301745636  Loss :  0.4161715298668108\n",
            "Validation: \n",
            "Accuracy :  84.0  Loss :  0.4544563591480255\n",
            "Accuracy :  84.28571428571429  Loss :  0.4575237894342059\n",
            "Accuracy :  83.85365853658537  Loss :  0.47166230438685997\n",
            "Accuracy :  83.9672131147541  Loss :  0.4678992846461593\n",
            "Accuracy :  84.09876543209876  Loss :  0.46448013719953135\n",
            "\n",
            "Epoch: 7\n",
            "Accuracy :  89.0  Loss :  0.32866230607032776\n",
            "Accuracy :  85.87562189054727  Loss :  0.4118789078584358\n",
            "Accuracy :  86.0074812967581  Loss :  0.40992936046046213\n",
            "Validation: \n",
            "Accuracy :  84.0  Loss :  0.43971121311187744\n",
            "Accuracy :  84.42857142857143  Loss :  0.4527181415330796\n",
            "Accuracy :  83.90243902439025  Loss :  0.46719234164168194\n",
            "Accuracy :  84.08196721311475  Loss :  0.46279308444163836\n",
            "Accuracy :  84.25925925925925  Loss :  0.45933439518198554\n",
            "\n",
            "Epoch: 8\n",
            "Accuracy :  90.0  Loss :  0.3132326602935791\n",
            "Accuracy :  86.05970149253731  Loss :  0.40287750114255877\n",
            "Accuracy :  86.29177057356608  Loss :  0.3999915025998232\n",
            "Validation: \n",
            "Accuracy :  85.0  Loss :  0.4361385703086853\n",
            "Accuracy :  84.28571428571429  Loss :  0.4508410543203354\n",
            "Accuracy :  83.92682926829268  Loss :  0.4647362323068991\n",
            "Accuracy :  84.1311475409836  Loss :  0.4602581836649629\n",
            "Accuracy :  84.38271604938272  Loss :  0.45701763567365244\n",
            "\n",
            "Epoch: 9\n",
            "Accuracy :  92.0  Loss :  0.2884169816970825\n",
            "Accuracy :  86.34328358208955  Loss :  0.4001528063965081\n",
            "Accuracy :  86.40399002493766  Loss :  0.39666221266672796\n",
            "Validation: \n",
            "Accuracy :  85.0  Loss :  0.4273698925971985\n",
            "Accuracy :  84.47619047619048  Loss :  0.4486066819656463\n",
            "Accuracy :  84.17073170731707  Loss :  0.4625857141686649\n",
            "Accuracy :  84.45901639344262  Loss :  0.45767361829515363\n",
            "Accuracy :  84.64197530864197  Loss :  0.45430361617494514\n",
            "\n",
            "Epoch: 10\n",
            "Accuracy :  89.0  Loss :  0.32165390253067017\n",
            "Accuracy :  86.48258706467662  Loss :  0.3939461320342116\n",
            "Accuracy :  86.54862842892769  Loss :  0.39094647371263574\n",
            "Validation: \n",
            "Accuracy :  85.0  Loss :  0.4186592996120453\n",
            "Accuracy :  84.38095238095238  Loss :  0.4469211073148818\n",
            "Accuracy :  84.17073170731707  Loss :  0.46092915098841597\n",
            "Accuracy :  84.42622950819673  Loss :  0.45599221743521146\n",
            "Accuracy :  84.61728395061728  Loss :  0.45261751575234493\n"
          ]
        }
      ],
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+10):\n",
        "    train(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test(epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUTWo9s4D-pT"
      },
      "source": [
        "**Finetuned student model** has greater final validation accuracy of **85.0%** as compared to the teacher model which had final accuracy of 83.0%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_03EQevID7Zd"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVCncc_ZMO3L"
      },
      "source": [
        "#Multi Students\n",
        "The paper impliments multiple students but till now you have implimented only one student. In this part you will create two smaller students, train them on different chunks of Dense features(i.e: DenseTrain[:,:256] for one student and DenseTrain[:,256:] for other student) and then ensamble them in the final output network. Constraint is that the sum of both students should not be greater then single student in terms of perimeters. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZwJ_J4eib9U"
      },
      "source": [
        "#Creating Multiple students\n",
        "1.6 In this step you will create two students smaller in size with outputs 256 each. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmFB-B_DisxP",
        "outputId": "2950e31e-813c-4813-cf3e-1dca53ba970c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 64, 16, 16]               0\n",
            "            Conv2d-5           [-1, 64, 16, 16]          36,928\n",
            "       BatchNorm2d-6           [-1, 64, 16, 16]             128\n",
            "              ReLU-7           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 64, 8, 8]               0\n",
            "            Conv2d-9            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-10            [-1, 128, 8, 8]             256\n",
            "             ReLU-11            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-12            [-1, 128, 4, 4]               0\n",
            "           Conv2d-13            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-14            [-1, 128, 4, 4]             256\n",
            "             ReLU-15            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-16            [-1, 128, 2, 2]               0\n",
            "           Conv2d-17            [-1, 256, 2, 2]         295,168\n",
            "      BatchNorm2d-18            [-1, 256, 2, 2]             512\n",
            "             ReLU-19            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-20            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-21            [-1, 256, 1, 1]               0\n",
            "           Linear-22                  [-1, 256]          65,792\n",
            "================================================================\n",
            "Total params: 622,400\n",
            "Trainable params: 622,400\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.31\n",
            "Params size (MB): 2.37\n",
            "Estimated Total Size (MB): 4.70\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 64, 16, 16]               0\n",
            "            Conv2d-5           [-1, 64, 16, 16]          36,928\n",
            "       BatchNorm2d-6           [-1, 64, 16, 16]             128\n",
            "              ReLU-7           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 64, 8, 8]               0\n",
            "            Conv2d-9            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-10            [-1, 128, 8, 8]             256\n",
            "             ReLU-11            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-12            [-1, 128, 4, 4]               0\n",
            "           Conv2d-13            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-14            [-1, 128, 4, 4]             256\n",
            "             ReLU-15            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-16            [-1, 128, 2, 2]               0\n",
            "           Conv2d-17            [-1, 256, 2, 2]         295,168\n",
            "      BatchNorm2d-18            [-1, 256, 2, 2]             512\n",
            "             ReLU-19            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-20            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-21            [-1, 256, 1, 1]               0\n",
            "           Linear-22                  [-1, 256]          65,792\n",
            "================================================================\n",
            "Total params: 622,400\n",
            "Trainable params: 622,400\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.31\n",
            "Params size (MB): 2.37\n",
            "Estimated Total Size (MB): 4.70\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "    'VGGS':  [64, 'M', 64, 'M', 128, 'M', 128, 'M', 256, 'M'],\n",
        "\n",
        "}\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(256, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "s1 = VGG('VGGS')\n",
        "s1 = s1.to(device)\n",
        "summary(s1, (3, 32, 32))\n",
        "s2 = VGG('VGGS')\n",
        "s2 = s2.to(device)\n",
        "summary(s2, (3, 32, 32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46YZcfQ0Fgti"
      },
      "source": [
        "Sum of parameters of both student models is: 622,400*2 = 1,244,800. This is less than 2M parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU8lHor3hpI8"
      },
      "source": [
        "#Training Multi Students\n",
        "1.6 In this step you will train two students instead of one. In the training loop you will pass the input from both students and then backwark both the losses. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "5TbTN57Ke7ZL"
      },
      "outputs": [],
      "source": [
        "optimizer1 = optim.Adam(s1.parameters(), lr=0.0001)\n",
        "optimizer2 = optim.Adam(s2.parameters(), lr=0.0001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    s1.train()\n",
        "    train_loss1 = 0\n",
        "    train_loss2 = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        #code here\n",
        "        # load the targets which are dense feature vectors\n",
        "        batch_size=100\n",
        "        targets = DenseTrain[batch_idx]\n",
        "        targets.to(device)\n",
        "        targets = Variable(targets)\n",
        "        \n",
        "        s1.zero_grad()\n",
        "        s2.zero_grad()\n",
        "        \n",
        "        # find outputs of both models\n",
        "        outputs1 = s1(inputs)\n",
        "        outputs2 = s2(inputs)\n",
        "\n",
        "        # find loss by splitting dense vector of teacher into 2 subspaces so\n",
        "        # that each student learns a separate subspace\n",
        "        targets1 = 0\n",
        "        targets2 = 0\n",
        "        for i in range(targets.shape[0]):\n",
        "            if i == 0:\n",
        "              targets1 = torch.unsqueeze(targets[i][:256], dim=0)\n",
        "              # print(\"why is this happening\", targets1.shape)\n",
        "              targets2 = torch.unsqueeze(targets[i][256:], dim=0)\n",
        "            else:\n",
        "              targets1 = torch.cat((targets1, torch.unsqueeze(targets[i][:256], dim=0)),0)\n",
        "              targets2 = torch.cat((targets2, torch.unsqueeze(targets[i][256:], dim=0)),0)\n",
        "\n",
        "        # print(\"targets1 shape:\", targets1.shape)\n",
        "        # print(\"targets2 shape:\", targets2.shape)\n",
        "        # find the loss according to the criteria\n",
        "        loss1 = criterion(outputs1, targets1)\n",
        "        loss2 = criterion(outputs2, targets2)\n",
        "        \n",
        "        # backpropagation steps to update weights\n",
        "        loss1.backward()\n",
        "        loss2.backward()\n",
        "        \n",
        "        optimizer1.step()\n",
        "        optimizer2.step()\n",
        "\n",
        "        train_loss1 += loss1.item()\n",
        "        train_loss2 += loss2.item()\n",
        "        if(batch_idx % 10 == 0):\n",
        "          print(\"Loss S1: \", train_loss1/(batch_idx+1))\n",
        "          print(\"Loss S2: \", train_loss2/(batch_idx+1))\n",
        "def test(epoch):\n",
        "    s1.eval()\n",
        "    test_loss1 = 0\n",
        "    test_loss2 = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            #code here\n",
        "            # load the targets which are dense feature vectors\n",
        "            batch_size=100\n",
        "            targets = DenseTrain[batch_idx]\n",
        "            targets.to(device)\n",
        "            targets = Variable(targets)\n",
        "            \n",
        "            s1.zero_grad()\n",
        "            s2.zero_grad()\n",
        "            \n",
        "            # find outputs of both models\n",
        "            outputs1 = s1(inputs)\n",
        "            outputs2 = s2(inputs)\n",
        "\n",
        "            # find loss by splitting dense vector of teacher into 2 subspaces so\n",
        "            # that each student learns a separate subspace\n",
        "            targets1 = 0\n",
        "            targets2 = 0\n",
        "            for i in range(targets.shape[0]):\n",
        "                # print(\"what\")\n",
        "                if i == 0:\n",
        "                  targets1 = torch.unsqueeze(targets[i][:256], dim=0)\n",
        "                  # print(\"why is this happening\", targets1.shape)\n",
        "                  targets2 = torch.unsqueeze(targets[i][256:], dim=0)\n",
        "                else:\n",
        "                  targets1 = torch.cat((targets1, torch.unsqueeze(targets[i][:256], dim=0)),0)\n",
        "                  targets2 = torch.cat((targets2, torch.unsqueeze(targets[i][256:], dim=0)),0)\n",
        "\n",
        "            # print(\"targets shape:\", targets2.shape)\n",
        "\n",
        "            loss1 = criterion(outputs1, targets1)\n",
        "            loss2 = criterion(outputs2, targets2)\n",
        "            \n",
        "            test_loss1 += loss1.item()\n",
        "            test_loss2 += loss2.item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\" Loss S1: \", test_loss1/(batch_idx+1))\n",
        "              print(\" Loss S2: \", test_loss2/(batch_idx+1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNZSZfOmiFDl",
        "outputId": "5da1d580-1ae9-44d3-d5e3-3107130e6417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Loss S2:  0.0721916065997741\n",
            "Loss S1:  0.0730183014995695\n",
            "Loss S2:  0.07215476217469523\n",
            "Loss S1:  0.07304882083580831\n",
            "Loss S2:  0.07215465575007164\n",
            "Loss S1:  0.07298981008920269\n",
            "Loss S2:  0.07205922154882868\n",
            "Loss S1:  0.07308571368555007\n",
            "Loss S2:  0.07213517478326471\n",
            "Loss S1:  0.07307825045995495\n",
            "Loss S2:  0.07213230309866514\n",
            "Loss S1:  0.0730904453768219\n",
            "Loss S2:  0.07217536214936818\n",
            "Loss S1:  0.07308103602666122\n",
            "Loss S2:  0.07217740264417229\n",
            "Loss S1:  0.07298367190537045\n",
            "Loss S2:  0.07209080476655251\n",
            "Validation: \n",
            " Loss S1:  1.6104755401611328\n",
            " Loss S2:  1.5243847370147705\n",
            " Loss S1:  1.5639704011735462\n",
            " Loss S2:  1.5196441468738375\n",
            " Loss S1:  1.555942401653383\n",
            " Loss S2:  1.5103547311410672\n",
            " Loss S1:  1.5490869361846173\n",
            " Loss S2:  1.5014789593024331\n",
            " Loss S1:  1.5524690401406935\n",
            " Loss S2:  1.5024688567644284\n",
            "\n",
            "Epoch: 57\n",
            "Loss S1:  0.07264567911624908\n",
            "Loss S2:  0.06728608161211014\n",
            "Loss S1:  0.07374458014965057\n",
            "Loss S2:  0.07197785038839687\n",
            "Loss S1:  0.07293651714211419\n",
            "Loss S2:  0.0715588295743579\n",
            "Loss S1:  0.07285110003525211\n",
            "Loss S2:  0.07113812503314787\n",
            "Loss S1:  0.07319646982884989\n",
            "Loss S2:  0.07180476243176111\n",
            "Loss S1:  0.07317338986139671\n",
            "Loss S2:  0.07175331811110179\n",
            "Loss S1:  0.07321723758197221\n",
            "Loss S2:  0.07256691658594569\n",
            "Loss S1:  0.07344836447860154\n",
            "Loss S2:  0.07260061873936317\n",
            "Loss S1:  0.07362026592463623\n",
            "Loss S2:  0.07269394609295292\n",
            "Loss S1:  0.07348165415473036\n",
            "Loss S2:  0.07261415963972008\n",
            "Loss S1:  0.07321280845911196\n",
            "Loss S2:  0.07246494588285389\n",
            "Loss S1:  0.07293345806029466\n",
            "Loss S2:  0.07210687198885926\n",
            "Loss S1:  0.07256507220839666\n",
            "Loss S2:  0.07194242530987283\n",
            "Loss S1:  0.07267921844511542\n",
            "Loss S2:  0.07213451778729453\n",
            "Loss S1:  0.07271663416573342\n",
            "Loss S2:  0.07207639301393895\n",
            "Loss S1:  0.07275145135770571\n",
            "Loss S2:  0.07218122356478741\n",
            "Loss S1:  0.07262448101125149\n",
            "Loss S2:  0.07209729141792896\n",
            "Loss S1:  0.07278019669111709\n",
            "Loss S2:  0.07215263300210412\n",
            "Loss S1:  0.07280834521377944\n",
            "Loss S2:  0.07222462896237057\n",
            "Loss S1:  0.07277480469948334\n",
            "Loss S2:  0.0722585912813379\n",
            "Loss S1:  0.07272883019044031\n",
            "Loss S2:  0.07220964291276623\n",
            "Loss S1:  0.0727770488126583\n",
            "Loss S2:  0.07212993497320261\n",
            "Loss S1:  0.07277333250951983\n",
            "Loss S2:  0.07209038220312261\n",
            "Loss S1:  0.07264271076946031\n",
            "Loss S2:  0.07199108170030953\n",
            "Loss S1:  0.07260190792907323\n",
            "Loss S2:  0.07199297871641598\n",
            "Loss S1:  0.0725541144639135\n",
            "Loss S2:  0.07200428729216415\n",
            "Loss S1:  0.07270238307301112\n",
            "Loss S2:  0.07202870286002012\n",
            "Loss S1:  0.07273313979106195\n",
            "Loss S2:  0.07200008111136426\n",
            "Loss S1:  0.07282329317566763\n",
            "Loss S2:  0.07202375855204049\n",
            "Loss S1:  0.07280672604019699\n",
            "Loss S2:  0.07194878265927337\n",
            "Loss S1:  0.072746476102806\n",
            "Loss S2:  0.07186685475301109\n",
            "Loss S1:  0.07272120372323361\n",
            "Loss S2:  0.07183092633723447\n",
            "Loss S1:  0.07266160430288018\n",
            "Loss S2:  0.0718459211220251\n",
            "Loss S1:  0.07257408445020097\n",
            "Loss S2:  0.07176522149780365\n",
            "Loss S1:  0.07269194898061739\n",
            "Loss S2:  0.07192561613761785\n",
            "Loss S1:  0.07279364362443953\n",
            "Loss S2:  0.07207652934935697\n",
            "Loss S1:  0.07283496059002638\n",
            "Loss S2:  0.07203692645660068\n",
            "Loss S1:  0.07284058561182086\n",
            "Loss S2:  0.07206280812661281\n",
            "Loss S1:  0.07283081435977318\n",
            "Loss S2:  0.07200433503533286\n",
            "Loss S1:  0.07278551096501558\n",
            "Loss S2:  0.07195292760991989\n",
            "Loss S1:  0.07279661295011157\n",
            "Loss S2:  0.07202017168860483\n",
            "Loss S1:  0.0727393823764185\n",
            "Loss S2:  0.07190719261366665\n",
            "Loss S1:  0.07274087648529055\n",
            "Loss S2:  0.07188686839970727\n",
            "Loss S1:  0.07273164888511124\n",
            "Loss S2:  0.07189322484272538\n",
            "Loss S1:  0.07270375768444975\n",
            "Loss S2:  0.0718414175605017\n",
            "Loss S1:  0.07273461089000734\n",
            "Loss S2:  0.07187218706385788\n",
            "Loss S1:  0.07267835059904225\n",
            "Loss S2:  0.07181495738391504\n",
            "Loss S1:  0.0726896745653036\n",
            "Loss S2:  0.07180676570522052\n",
            "Loss S1:  0.07269985355018578\n",
            "Loss S2:  0.0718233600615092\n",
            "Loss S1:  0.07264075894857133\n",
            "Loss S2:  0.07174230566487293\n",
            "Validation: \n",
            " Loss S1:  1.5895261764526367\n",
            " Loss S2:  1.5050915479660034\n",
            " Loss S1:  1.5565851018542336\n",
            " Loss S2:  1.5142019873573667\n",
            " Loss S1:  1.549305540759389\n",
            " Loss S2:  1.5057571864709622\n",
            " Loss S1:  1.545054527579761\n",
            " Loss S2:  1.4980064747763462\n",
            " Loss S1:  1.5479693471649547\n",
            " Loss S2:  1.499065633173342\n",
            "\n",
            "Epoch: 58\n",
            "Loss S1:  0.07421901077032089\n",
            "Loss S2:  0.0772223100066185\n",
            "Loss S1:  0.07171042602170598\n",
            "Loss S2:  0.07267490367997777\n",
            "Loss S1:  0.07115462848118373\n",
            "Loss S2:  0.0714479929634503\n",
            "Loss S1:  0.07095459404010926\n",
            "Loss S2:  0.07122814727406349\n",
            "Loss S1:  0.0715675045986001\n",
            "Loss S2:  0.07159927451029056\n",
            "Loss S1:  0.07146190508615737\n",
            "Loss S2:  0.07107459264350872\n",
            "Loss S1:  0.07176798582077026\n",
            "Loss S2:  0.07167078365312247\n",
            "Loss S1:  0.07193770222890546\n",
            "Loss S2:  0.0719211990879455\n",
            "Loss S1:  0.07196138619824692\n",
            "Loss S2:  0.07196241741379102\n",
            "Loss S1:  0.07213306799530983\n",
            "Loss S2:  0.07217835688165256\n",
            "Loss S1:  0.07202499997940394\n",
            "Loss S2:  0.0720306403639883\n",
            "Loss S1:  0.07188176521444106\n",
            "Loss S2:  0.07180058264786059\n",
            "Loss S1:  0.07187626698662426\n",
            "Loss S2:  0.07155953092146511\n",
            "Loss S1:  0.07195740764718929\n",
            "Loss S2:  0.07171136433727869\n",
            "Loss S1:  0.07189809766115872\n",
            "Loss S2:  0.07166847383193936\n",
            "Loss S1:  0.0719293047310106\n",
            "Loss S2:  0.07168080576306937\n",
            "Loss S1:  0.07187533760385484\n",
            "Loss S2:  0.07151013520193396\n",
            "Loss S1:  0.07197982602213558\n",
            "Loss S2:  0.07160052813981709\n",
            "Loss S1:  0.07213904294931428\n",
            "Loss S2:  0.07176509042635806\n",
            "Loss S1:  0.07209393192882313\n",
            "Loss S2:  0.07183098075277518\n",
            "Loss S1:  0.07209945145753485\n",
            "Loss S2:  0.07174265962928089\n",
            "Loss S1:  0.07222177316015366\n",
            "Loss S2:  0.07167874068304261\n",
            "Loss S1:  0.0722551495679633\n",
            "Loss S2:  0.07168585894738927\n",
            "Loss S1:  0.07224350727417252\n",
            "Loss S2:  0.0716977898370136\n",
            "Loss S1:  0.07232394896416744\n",
            "Loss S2:  0.0717723607027679\n",
            "Loss S1:  0.07232098078050936\n",
            "Loss S2:  0.07183226346375933\n",
            "Loss S1:  0.07240060418558761\n",
            "Loss S2:  0.07184257863581865\n",
            "Loss S1:  0.07241513685931579\n",
            "Loss S2:  0.07171247216333322\n",
            "Loss S1:  0.07248127138328297\n",
            "Loss S2:  0.07175325875392588\n",
            "Loss S1:  0.07248209405344785\n",
            "Loss S2:  0.07167390910620541\n",
            "Loss S1:  0.0724056764224241\n",
            "Loss S2:  0.07159453724507873\n",
            "Loss S1:  0.07235194006006435\n",
            "Loss S2:  0.07164636287850198\n",
            "Loss S1:  0.07240309631248872\n",
            "Loss S2:  0.07165277856068447\n",
            "Loss S1:  0.0723218685325539\n",
            "Loss S2:  0.07162929894655853\n",
            "Loss S1:  0.07246390625266386\n",
            "Loss S2:  0.07172148275978404\n",
            "Loss S1:  0.07254153940073106\n",
            "Loss S2:  0.07182537364187064\n",
            "Loss S1:  0.07255533607960407\n",
            "Loss S2:  0.07185826243167108\n",
            "Loss S1:  0.0725945389134222\n",
            "Loss S2:  0.07185590105517856\n",
            "Loss S1:  0.0725754333997336\n",
            "Loss S2:  0.07180796920510102\n",
            "Loss S1:  0.07252845268153474\n",
            "Loss S2:  0.07178097830899537\n",
            "Loss S1:  0.07254668374422779\n",
            "Loss S2:  0.07179494299682002\n",
            "Loss S1:  0.07254179568005015\n",
            "Loss S2:  0.07178474390775037\n",
            "Loss S1:  0.07253084978588686\n",
            "Loss S2:  0.07176728110379957\n",
            "Loss S1:  0.07253716368305987\n",
            "Loss S2:  0.07172952167983675\n",
            "Loss S1:  0.07255166261340755\n",
            "Loss S2:  0.07169454199285194\n",
            "Loss S1:  0.07257311258407231\n",
            "Loss S2:  0.07170148992584709\n",
            "Loss S1:  0.07253301744056369\n",
            "Loss S2:  0.07162405956140568\n",
            "Loss S1:  0.07254878408621578\n",
            "Loss S2:  0.07168185907058149\n",
            "Loss S1:  0.07247404744093483\n",
            "Loss S2:  0.07167461781671539\n",
            "Loss S1:  0.07241816695802324\n",
            "Loss S2:  0.07166752453643047\n",
            "Validation: \n",
            " Loss S1:  1.5537374019622803\n",
            " Loss S2:  1.510105848312378\n",
            " Loss S1:  1.5345003888720559\n",
            " Loss S2:  1.507915116491772\n",
            " Loss S1:  1.5244341536266048\n",
            " Loss S2:  1.4981871552583648\n",
            " Loss S1:  1.5207738250982565\n",
            " Loss S2:  1.4901866580619187\n",
            " Loss S1:  1.5236863236368439\n",
            " Loss S2:  1.4912712515136342\n",
            "\n",
            "Epoch: 59\n",
            "Loss S1:  0.06884422153234482\n",
            "Loss S2:  0.0726410299539566\n",
            "Loss S1:  0.07060557874766263\n",
            "Loss S2:  0.0721104693683711\n",
            "Loss S1:  0.07067924134788059\n",
            "Loss S2:  0.071229545488244\n",
            "Loss S1:  0.07109002504617937\n",
            "Loss S2:  0.07126640961054832\n",
            "Loss S1:  0.07156344030688448\n",
            "Loss S2:  0.07140107761795927\n",
            "Loss S1:  0.07141757398551586\n",
            "Loss S2:  0.07123146147704591\n",
            "Loss S1:  0.07140424157508084\n",
            "Loss S2:  0.07163970807536704\n",
            "Loss S1:  0.0713623406916437\n",
            "Loss S2:  0.071674639385351\n",
            "Loss S1:  0.0716078482384299\n",
            "Loss S2:  0.07172871629397075\n",
            "Loss S1:  0.07168206159066368\n",
            "Loss S2:  0.07150681423289436\n",
            "Loss S1:  0.0711224476004591\n",
            "Loss S2:  0.07121716819629811\n",
            "Loss S1:  0.0709590473220692\n",
            "Loss S2:  0.07091860335555163\n",
            "Loss S1:  0.07092618231068958\n",
            "Loss S2:  0.07077867667044489\n",
            "Loss S1:  0.07114694307323631\n",
            "Loss S2:  0.07090392947651958\n",
            "Loss S1:  0.07123606895089996\n",
            "Loss S2:  0.0710725578221869\n",
            "Loss S1:  0.0712814832365276\n",
            "Loss S2:  0.07094737877514189\n",
            "Loss S1:  0.07110598685671084\n",
            "Loss S2:  0.07079827709135061\n",
            "Loss S1:  0.07130432057014682\n",
            "Loss S2:  0.07096149382448336\n",
            "Loss S1:  0.0713859069783714\n",
            "Loss S2:  0.07104752111912432\n",
            "Loss S1:  0.07146191969513893\n",
            "Loss S2:  0.07101379892744943\n",
            "Loss S1:  0.07147179182563256\n",
            "Loss S2:  0.07091609140237172\n",
            "Loss S1:  0.07148539679239711\n",
            "Loss S2:  0.0709034510274634\n",
            "Loss S1:  0.07154672102353692\n",
            "Loss S2:  0.07096970431944903\n",
            "Loss S1:  0.07151765186033207\n",
            "Loss S2:  0.07094976047932844\n",
            "Loss S1:  0.07152929299228913\n",
            "Loss S2:  0.07105596474839444\n",
            "Loss S1:  0.07149870940057405\n",
            "Loss S2:  0.07101641883294421\n",
            "Loss S1:  0.07155994911075095\n",
            "Loss S2:  0.07107330332028455\n",
            "Loss S1:  0.07155582554830836\n",
            "Loss S2:  0.0711105553728848\n",
            "Loss S1:  0.07162080508958403\n",
            "Loss S2:  0.07118626139077003\n",
            "Loss S1:  0.07159596091437176\n",
            "Loss S2:  0.07114124797361414\n",
            "Loss S1:  0.07163482280664665\n",
            "Loss S2:  0.07111624274340975\n",
            "Loss S1:  0.07170121283105715\n",
            "Loss S2:  0.07117382008163109\n",
            "Loss S1:  0.0717857991145036\n",
            "Loss S2:  0.07122192230087203\n",
            "Loss S1:  0.07179187589361588\n",
            "Loss S2:  0.07113391347792214\n",
            "Loss S1:  0.0719346731813772\n",
            "Loss S2:  0.07116831871858441\n",
            "Loss S1:  0.0719043890762533\n",
            "Loss S2:  0.07120230006441432\n",
            "Loss S1:  0.07197286471353014\n",
            "Loss S2:  0.07123402547390507\n",
            "Loss S1:  0.07203806605380822\n",
            "Loss S2:  0.07120613938553956\n",
            "Loss S1:  0.07197649257741576\n",
            "Loss S2:  0.07111420722927635\n",
            "Loss S1:  0.07192005335217547\n",
            "Loss S2:  0.07111103541177252\n",
            "Loss S1:  0.07194377821951435\n",
            "Loss S2:  0.0711226298513258\n",
            "Loss S1:  0.07190464373560138\n",
            "Loss S2:  0.07107357283795837\n",
            "Loss S1:  0.0719388030699483\n",
            "Loss S2:  0.07107058850574946\n",
            "Loss S1:  0.07196201854266589\n",
            "Loss S2:  0.07102920668169406\n",
            "Loss S1:  0.0719201630002517\n",
            "Loss S2:  0.0709939896891447\n",
            "Loss S1:  0.07198678436141849\n",
            "Loss S2:  0.07102914151225545\n",
            "Loss S1:  0.07192241954829326\n",
            "Loss S2:  0.0709803723558428\n",
            "Loss S1:  0.07196170129593771\n",
            "Loss S2:  0.07100870565465808\n",
            "Loss S1:  0.07191826085600685\n",
            "Loss S2:  0.07100573938781407\n",
            "Loss S1:  0.07185557870053953\n",
            "Loss S2:  0.07093937483344932\n",
            "Validation: \n",
            " Loss S1:  1.577966570854187\n",
            " Loss S2:  1.5129895210266113\n",
            " Loss S1:  1.546904376574925\n",
            " Loss S2:  1.5134891612189156\n",
            " Loss S1:  1.5383013661314802\n",
            " Loss S2:  1.506704132731368\n",
            " Loss S1:  1.5342892095690868\n",
            " Loss S2:  1.4982617468130393\n",
            " Loss S1:  1.537408839037389\n",
            " Loss S2:  1.4987889013172668\n",
            "\n",
            "Epoch: 60\n",
            "Loss S1:  0.07051078975200653\n",
            "Loss S2:  0.07465113699436188\n",
            "Loss S1:  0.07142584906382994\n",
            "Loss S2:  0.0717336968942122\n",
            "Loss S1:  0.07136835583618709\n",
            "Loss S2:  0.07139486819505692\n",
            "Loss S1:  0.07105493785873536\n",
            "Loss S2:  0.07078824108166079\n",
            "Loss S1:  0.0715409542729215\n",
            "Loss S2:  0.07146288954266687\n",
            "Loss S1:  0.07151958156450122\n",
            "Loss S2:  0.07115817398709409\n",
            "Loss S1:  0.07189787338014508\n",
            "Loss S2:  0.07147053897869392\n",
            "Loss S1:  0.07226884511994644\n",
            "Loss S2:  0.07170251609993653\n",
            "Loss S1:  0.07241971284886937\n",
            "Loss S2:  0.0719233708065233\n",
            "Loss S1:  0.07233959059793871\n",
            "Loss S2:  0.07176336220332555\n",
            "Loss S1:  0.07206637253708179\n",
            "Loss S2:  0.07131641719600942\n",
            "Loss S1:  0.07183857809181686\n",
            "Loss S2:  0.0711650063728427\n",
            "Loss S1:  0.07166296773213\n",
            "Loss S2:  0.07102258842099797\n",
            "Loss S1:  0.07180398850495578\n",
            "Loss S2:  0.07128245963622595\n",
            "Loss S1:  0.07176227508284522\n",
            "Loss S2:  0.07117839432354514\n",
            "Loss S1:  0.07174740317246772\n",
            "Loss S2:  0.07121349155705496\n",
            "Loss S1:  0.07161909966550259\n",
            "Loss S2:  0.07103178888466788\n",
            "Loss S1:  0.07174302079872778\n",
            "Loss S2:  0.07108889897054399\n",
            "Loss S1:  0.07180452103937529\n",
            "Loss S2:  0.07115710351760215\n",
            "Loss S1:  0.07188140452688277\n",
            "Loss S2:  0.07115354550836599\n",
            "Loss S1:  0.07185405164734641\n",
            "Loss S2:  0.0711730441903297\n",
            "Loss S1:  0.07181612119714231\n",
            "Loss S2:  0.07122131177528775\n",
            "Loss S1:  0.0718385727899107\n",
            "Loss S2:  0.07105842905279199\n",
            "Loss S1:  0.07186075003245176\n",
            "Loss S2:  0.07101856197887685\n",
            "Loss S1:  0.07186162879852834\n",
            "Loss S2:  0.0709860349591837\n",
            "Loss S1:  0.0718818444654761\n",
            "Loss S2:  0.07101853114675241\n",
            "Loss S1:  0.07193751932903268\n",
            "Loss S2:  0.07113454159763125\n",
            "Loss S1:  0.07192015594276994\n",
            "Loss S2:  0.07110862893042089\n",
            "Loss S1:  0.0719717860990784\n",
            "Loss S2:  0.07114457468969541\n",
            "Loss S1:  0.07191238749477871\n",
            "Loss S2:  0.07110148268876616\n",
            "Loss S1:  0.07192372993575379\n",
            "Loss S2:  0.07110336033064266\n",
            "Loss S1:  0.0719355573081127\n",
            "Loss S2:  0.07112540296275899\n",
            "Loss S1:  0.07197357589999835\n",
            "Loss S2:  0.07114494850748795\n",
            "Loss S1:  0.07190819544959645\n",
            "Loss S2:  0.0711005497329905\n",
            "Loss S1:  0.07196505052034806\n",
            "Loss S2:  0.07123193922630154\n",
            "Loss S1:  0.07203069607331881\n",
            "Loss S2:  0.07125605914497647\n",
            "Loss S1:  0.07206145989762779\n",
            "Loss S2:  0.07128082903055603\n",
            "Loss S1:  0.07205702858472449\n",
            "Loss S2:  0.07129330583659786\n",
            "Loss S1:  0.07195388707588976\n",
            "Loss S2:  0.07127166740850514\n",
            "Loss S1:  0.07191050827236431\n",
            "Loss S2:  0.07122549015428405\n",
            "Loss S1:  0.07197466998957934\n",
            "Loss S2:  0.07127615035583551\n",
            "Loss S1:  0.07193064788433468\n",
            "Loss S2:  0.0712521548888254\n",
            "Loss S1:  0.07194116477126747\n",
            "Loss S2:  0.07120302285901724\n",
            "Loss S1:  0.0719801636192334\n",
            "Loss S2:  0.07120095826080393\n",
            "Loss S1:  0.07194351223286857\n",
            "Loss S2:  0.07115054914470162\n",
            "Loss S1:  0.0719933459522851\n",
            "Loss S2:  0.07119774122922753\n",
            "Loss S1:  0.07195683382855579\n",
            "Loss S2:  0.07115188131912929\n",
            "Loss S1:  0.07195306804244685\n",
            "Loss S2:  0.07112071556365414\n",
            "Loss S1:  0.07194630037257428\n",
            "Loss S2:  0.0711605574029523\n",
            "Loss S1:  0.0719319954893861\n",
            "Loss S2:  0.07111611290855709\n",
            "Validation: \n",
            " Loss S1:  1.5711983442306519\n",
            " Loss S2:  1.5253456830978394\n",
            " Loss S1:  1.5438122238431657\n",
            " Loss S2:  1.521175742149353\n",
            " Loss S1:  1.5348612331762546\n",
            " Loss S2:  1.5134137461825115\n",
            " Loss S1:  1.5309044161780936\n",
            " Loss S2:  1.5055416118903238\n",
            " Loss S1:  1.5340960143524924\n",
            " Loss S2:  1.507208956612481\n",
            "\n",
            "Epoch: 61\n",
            "Loss S1:  0.07359054684638977\n",
            "Loss S2:  0.07153921574354172\n",
            "Loss S1:  0.07022030516104265\n",
            "Loss S2:  0.06897456537593495\n",
            "Loss S1:  0.06970849455822081\n",
            "Loss S2:  0.06869134697176162\n",
            "Loss S1:  0.06976449549678833\n",
            "Loss S2:  0.06913287949658209\n",
            "Loss S1:  0.07052547530066676\n",
            "Loss S2:  0.0695873254501238\n",
            "Loss S1:  0.06995358055128771\n",
            "Loss S2:  0.06936760955289298\n",
            "Loss S1:  0.07029166939805766\n",
            "Loss S2:  0.07004114454154109\n",
            "Loss S1:  0.07025244181424799\n",
            "Loss S2:  0.07008903772688248\n",
            "Loss S1:  0.0702739676207672\n",
            "Loss S2:  0.07035610571871569\n",
            "Loss S1:  0.07044831188497963\n",
            "Loss S2:  0.0705776303433455\n",
            "Loss S1:  0.07032464385622798\n",
            "Loss S2:  0.07044000945764013\n",
            "Loss S1:  0.0702800189589595\n",
            "Loss S2:  0.07041431966799873\n",
            "Loss S1:  0.07064883922003518\n",
            "Loss S2:  0.0707059985042111\n",
            "Loss S1:  0.07071281429237992\n",
            "Loss S2:  0.07060965850153042\n",
            "Loss S1:  0.07055374927131842\n",
            "Loss S2:  0.070542500268483\n",
            "Loss S1:  0.07061020284891129\n",
            "Loss S2:  0.07055936783354803\n",
            "Loss S1:  0.07056194909425996\n",
            "Loss S2:  0.07042606818194715\n",
            "Loss S1:  0.07061770654212661\n",
            "Loss S2:  0.07052181377919794\n",
            "Loss S1:  0.0707279871428869\n",
            "Loss S2:  0.070555849258083\n",
            "Loss S1:  0.07081673563462902\n",
            "Loss S2:  0.07063281976895806\n",
            "Loss S1:  0.07082944430077254\n",
            "Loss S2:  0.07058986673011115\n",
            "Loss S1:  0.07082867760087641\n",
            "Loss S2:  0.07057813782756928\n",
            "Loss S1:  0.07096812378497146\n",
            "Loss S2:  0.07057564958458033\n",
            "Loss S1:  0.07096594056009731\n",
            "Loss S2:  0.07056578790599649\n",
            "Loss S1:  0.07104530874871615\n",
            "Loss S2:  0.07061581196132043\n",
            "Loss S1:  0.0710914370666937\n",
            "Loss S2:  0.07065614431621543\n",
            "Loss S1:  0.07117660574871919\n",
            "Loss S2:  0.07083854467476008\n",
            "Loss S1:  0.0711739861932188\n",
            "Loss S2:  0.07084548588828407\n",
            "Loss S1:  0.07126354699245127\n",
            "Loss S2:  0.07088177249471912\n",
            "Loss S1:  0.07118792588665723\n",
            "Loss S2:  0.07084151453424975\n",
            "Loss S1:  0.07116771694829693\n",
            "Loss S2:  0.07081265568089644\n",
            "Loss S1:  0.07119988257191188\n",
            "Loss S2:  0.0708139539363875\n",
            "Loss S1:  0.0712773527056443\n",
            "Loss S2:  0.07078939683247949\n",
            "Loss S1:  0.07125416493712955\n",
            "Loss S2:  0.0707560412338133\n",
            "Loss S1:  0.07133254080005755\n",
            "Loss S2:  0.07076956578625962\n",
            "Loss S1:  0.07137055647296783\n",
            "Loss S2:  0.0707159069210206\n",
            "Loss S1:  0.07145471089723368\n",
            "Loss S2:  0.07076246420391048\n",
            "Loss S1:  0.07142688393231351\n",
            "Loss S2:  0.07075562987807947\n",
            "Loss S1:  0.07140449785536981\n",
            "Loss S2:  0.07074826405312759\n",
            "Loss S1:  0.07137740492020421\n",
            "Loss S2:  0.07075263145367813\n",
            "Loss S1:  0.07137653361680799\n",
            "Loss S2:  0.07073197223041718\n",
            "Loss S1:  0.07134430632539039\n",
            "Loss S2:  0.07063764938291552\n",
            "Loss S1:  0.07140042958449298\n",
            "Loss S2:  0.07061509905723665\n",
            "Loss S1:  0.07140210018495396\n",
            "Loss S2:  0.07059616744172545\n",
            "Loss S1:  0.0713613570040586\n",
            "Loss S2:  0.07052419665786955\n",
            "Loss S1:  0.07138782462101025\n",
            "Loss S2:  0.070559468178157\n",
            "Loss S1:  0.07131301580954529\n",
            "Loss S2:  0.07047783135721326\n",
            "Loss S1:  0.07126700455785558\n",
            "Loss S2:  0.0704875976606539\n",
            "Loss S1:  0.07130461864622616\n",
            "Loss S2:  0.07052206516018032\n",
            "Loss S1:  0.0712141430274775\n",
            "Loss S2:  0.07046458805681732\n",
            "Validation: \n",
            " Loss S1:  1.578086495399475\n",
            " Loss S2:  1.5220648050308228\n",
            " Loss S1:  1.5525773479824974\n",
            " Loss S2:  1.5209248406546456\n",
            " Loss S1:  1.5438499712362521\n",
            " Loss S2:  1.5127942940083945\n",
            " Loss S1:  1.540176893843979\n",
            " Loss S2:  1.5040083107401112\n",
            " Loss S1:  1.542537169691957\n",
            " Loss S2:  1.5045296910368366\n",
            "\n",
            "Epoch: 62\n",
            "Loss S1:  0.07926460355520248\n",
            "Loss S2:  0.07477173954248428\n",
            "Loss S1:  0.07350929555567828\n",
            "Loss S2:  0.07100242579525168\n",
            "Loss S1:  0.07123844006231853\n",
            "Loss S2:  0.07003420626833326\n",
            "Loss S1:  0.07140311658863098\n",
            "Loss S2:  0.07001475437033561\n",
            "Loss S1:  0.07137664452922053\n",
            "Loss S2:  0.06979500120732843\n",
            "Loss S1:  0.0710777467226281\n",
            "Loss S2:  0.06949849543618221\n",
            "Loss S1:  0.07145259474388889\n",
            "Loss S2:  0.07030543306323349\n",
            "Loss S1:  0.07155132687217752\n",
            "Loss S2:  0.07061558358476196\n",
            "Loss S1:  0.07154402214987779\n",
            "Loss S2:  0.07076290061260447\n",
            "Loss S1:  0.07148871869667546\n",
            "Loss S2:  0.07075865746854426\n",
            "Loss S1:  0.07096834122987077\n",
            "Loss S2:  0.07065359672697463\n",
            "Loss S1:  0.07088326726545084\n",
            "Loss S2:  0.07043354709943135\n",
            "Loss S1:  0.07073081127745061\n",
            "Loss S2:  0.07034122340442721\n",
            "Loss S1:  0.07099101991030096\n",
            "Loss S2:  0.07051327909904583\n",
            "Loss S1:  0.07081324974378796\n",
            "Loss S2:  0.07037121279442564\n",
            "Loss S1:  0.07088620030623398\n",
            "Loss S2:  0.0703214756305644\n",
            "Loss S1:  0.07080238495276582\n",
            "Loss S2:  0.07013657443826983\n",
            "Loss S1:  0.07103352656054218\n",
            "Loss S2:  0.07023917933740811\n",
            "Loss S1:  0.0711242140203879\n",
            "Loss S2:  0.07038605186409054\n",
            "Loss S1:  0.07110085724536037\n",
            "Loss S2:  0.07038202123102093\n",
            "Loss S1:  0.07114320422582958\n",
            "Loss S2:  0.07042862684350108\n",
            "Loss S1:  0.07118186961983053\n",
            "Loss S2:  0.07039531410376043\n",
            "Loss S1:  0.07121363120381109\n",
            "Loss S2:  0.07042788122506703\n",
            "Loss S1:  0.0712289765993238\n",
            "Loss S2:  0.07039635320439999\n",
            "Loss S1:  0.07134956362583825\n",
            "Loss S2:  0.07057448716146322\n",
            "Loss S1:  0.07136923674330768\n",
            "Loss S2:  0.07057426016466076\n",
            "Loss S1:  0.0713861520434248\n",
            "Loss S2:  0.07062496300572636\n",
            "Loss S1:  0.07141605990069379\n",
            "Loss S2:  0.07064018292675599\n",
            "Loss S1:  0.0714209562174154\n",
            "Loss S2:  0.07063032196191706\n",
            "Loss S1:  0.07138779722282157\n",
            "Loss S2:  0.07058732182145938\n",
            "Loss S1:  0.07134881035186523\n",
            "Loss S2:  0.07063352481223815\n",
            "Loss S1:  0.07142254879093247\n",
            "Loss S2:  0.07062843306606989\n",
            "Loss S1:  0.07147125773294322\n",
            "Loss S2:  0.07063215540437684\n",
            "Loss S1:  0.07141460938621144\n",
            "Loss S2:  0.07055580405535294\n",
            "Loss S1:  0.07145744508618483\n",
            "Loss S2:  0.0705964831400477\n",
            "Loss S1:  0.07149152902413977\n",
            "Loss S2:  0.07066124819877141\n",
            "Loss S1:  0.0715420000626605\n",
            "Loss S2:  0.0706773337358583\n",
            "Loss S1:  0.07153055711456065\n",
            "Loss S2:  0.07064631914674754\n",
            "Loss S1:  0.07152718400431117\n",
            "Loss S2:  0.07056086840904917\n",
            "Loss S1:  0.07150237046925308\n",
            "Loss S2:  0.07049291505647437\n",
            "Loss S1:  0.07150555281270472\n",
            "Loss S2:  0.07049370858994802\n",
            "Loss S1:  0.07145944389077288\n",
            "Loss S2:  0.0704285130720504\n",
            "Loss S1:  0.0715051835885099\n",
            "Loss S2:  0.07040883990938193\n",
            "Loss S1:  0.07150176394808597\n",
            "Loss S2:  0.07038095113745699\n",
            "Loss S1:  0.07146068320286517\n",
            "Loss S2:  0.07032485950797324\n",
            "Loss S1:  0.07151166348823422\n",
            "Loss S2:  0.0703619177104232\n",
            "Loss S1:  0.07146307378360868\n",
            "Loss S2:  0.07034024108128538\n",
            "Loss S1:  0.0714243689429355\n",
            "Loss S2:  0.07034196392998827\n",
            "Loss S1:  0.07142541243882536\n",
            "Loss S2:  0.07036969377785116\n",
            "Loss S1:  0.07139450846554792\n",
            "Loss S2:  0.07029454202946966\n",
            "Validation: \n",
            " Loss S1:  1.5888943672180176\n",
            " Loss S2:  1.5261465311050415\n",
            " Loss S1:  1.5541723909832181\n",
            " Loss S2:  1.5269627968470256\n",
            " Loss S1:  1.5451587002451828\n",
            " Loss S2:  1.5194060075573805\n",
            " Loss S1:  1.5405030660941952\n",
            " Loss S2:  1.5103355157570761\n",
            " Loss S1:  1.543335031580042\n",
            " Loss S2:  1.5111422538757324\n",
            "\n",
            "Epoch: 63\n",
            "Loss S1:  0.07592064887285233\n",
            "Loss S2:  0.07989577203989029\n",
            "Loss S1:  0.0692949809811332\n",
            "Loss S2:  0.07024575431238521\n",
            "Loss S1:  0.06857927533842269\n",
            "Loss S2:  0.06810297054194268\n",
            "Loss S1:  0.0692533954016624\n",
            "Loss S2:  0.06802314291557958\n",
            "Loss S1:  0.0698935581234897\n",
            "Loss S2:  0.06850534368578981\n",
            "Loss S1:  0.07015074080988473\n",
            "Loss S2:  0.06849239401373208\n",
            "Loss S1:  0.07060530152721484\n",
            "Loss S2:  0.06930401102929819\n",
            "Loss S1:  0.0705526379317465\n",
            "Loss S2:  0.06971091944986665\n",
            "Loss S1:  0.07076742578251863\n",
            "Loss S2:  0.06989579960519884\n",
            "Loss S1:  0.0708070495217056\n",
            "Loss S2:  0.06990628992463206\n",
            "Loss S1:  0.07059578254523843\n",
            "Loss S2:  0.06966037163049868\n",
            "Loss S1:  0.07039272426082208\n",
            "Loss S2:  0.06935365649091231\n",
            "Loss S1:  0.07046791461627346\n",
            "Loss S2:  0.06935805029982378\n",
            "Loss S1:  0.07047899249400802\n",
            "Loss S2:  0.06939161287350509\n",
            "Loss S1:  0.07050413199773072\n",
            "Loss S2:  0.06942435557432208\n",
            "Loss S1:  0.07037161603964717\n",
            "Loss S2:  0.06939072435758761\n",
            "Loss S1:  0.07030814967077711\n",
            "Loss S2:  0.06937346549600548\n",
            "Loss S1:  0.07037394973095397\n",
            "Loss S2:  0.06952387018249048\n",
            "Loss S1:  0.07055022201156089\n",
            "Loss S2:  0.06970527461467527\n",
            "Loss S1:  0.07062723288673381\n",
            "Loss S2:  0.069914424353089\n",
            "Loss S1:  0.07071058726429347\n",
            "Loss S2:  0.06998340309185175\n",
            "Loss S1:  0.07074580235645105\n",
            "Loss S2:  0.07004305527885378\n",
            "Loss S1:  0.07064747881161142\n",
            "Loss S2:  0.07000734581194852\n",
            "Loss S1:  0.07058870340838577\n",
            "Loss S2:  0.06995996434515689\n",
            "Loss S1:  0.07060003645439861\n",
            "Loss S2:  0.07004434267576799\n",
            "Loss S1:  0.07064156235748553\n",
            "Loss S2:  0.07008740673977065\n",
            "Loss S1:  0.07076913570821057\n",
            "Loss S2:  0.07015021223846067\n",
            "Loss S1:  0.07072971201745787\n",
            "Loss S2:  0.0701487680065456\n",
            "Loss S1:  0.07073884910748099\n",
            "Loss S2:  0.07020129923612622\n",
            "Loss S1:  0.07070711843951051\n",
            "Loss S2:  0.07018954397732859\n",
            "Loss S1:  0.0707414503212387\n",
            "Loss S2:  0.07016251055505189\n",
            "Loss S1:  0.07072558587003751\n",
            "Loss S2:  0.07018146328508279\n",
            "Loss S1:  0.07071058054068749\n",
            "Loss S2:  0.0701593430335648\n",
            "Loss S1:  0.07062947491234522\n",
            "Loss S2:  0.07010363382245119\n",
            "Loss S1:  0.07079705449580447\n",
            "Loss S2:  0.07017715072089975\n",
            "Loss S1:  0.07081229671666085\n",
            "Loss S2:  0.07016796383083376\n",
            "Loss S1:  0.07086885215874524\n",
            "Loss S2:  0.07023415799121117\n",
            "Loss S1:  0.07091405504757183\n",
            "Loss S2:  0.07026861529706944\n",
            "Loss S1:  0.07091319299940987\n",
            "Loss S2:  0.07024335830895294\n",
            "Loss S1:  0.07091232016682625\n",
            "Loss S2:  0.07016823121615688\n",
            "Loss S1:  0.070950610744314\n",
            "Loss S2:  0.07018721704731261\n",
            "Loss S1:  0.07094804952578243\n",
            "Loss S2:  0.07017074098443463\n",
            "Loss S1:  0.0709266885849614\n",
            "Loss S2:  0.0701425953230637\n",
            "Loss S1:  0.07092363121994963\n",
            "Loss S2:  0.07010172296254928\n",
            "Loss S1:  0.07089131744472889\n",
            "Loss S2:  0.07006560521128496\n",
            "Loss S1:  0.07096137117982704\n",
            "Loss S2:  0.07013408983427775\n",
            "Loss S1:  0.07089804870090619\n",
            "Loss S2:  0.07007121119381549\n",
            "Loss S1:  0.07090569595211124\n",
            "Loss S2:  0.07010039174632662\n",
            "Loss S1:  0.07093969991976655\n",
            "Loss S2:  0.07015507198916651\n",
            "Loss S1:  0.07090585940572491\n",
            "Loss S2:  0.07009135322482182\n",
            "Validation: \n",
            " Loss S1:  1.5914989709854126\n",
            " Loss S2:  1.5160127878189087\n",
            " Loss S1:  1.5426492747806368\n",
            " Loss S2:  1.5161281994410925\n",
            " Loss S1:  1.5337734425940164\n",
            " Loss S2:  1.506314556773116\n",
            " Loss S1:  1.529125934741536\n",
            " Loss S2:  1.4983816381360664\n",
            " Loss S1:  1.5317637846793657\n",
            " Loss S2:  1.4992443161246218\n",
            "\n",
            "Epoch: 64\n",
            "Loss S1:  0.07084017246961594\n",
            "Loss S2:  0.07886430621147156\n",
            "Loss S1:  0.06964727863669395\n",
            "Loss S2:  0.0714854130690748\n",
            "Loss S1:  0.06947059361707597\n",
            "Loss S2:  0.07050011697269622\n",
            "Loss S1:  0.06939925145237677\n",
            "Loss S2:  0.06976803728649693\n",
            "Loss S1:  0.06997011084018684\n",
            "Loss S2:  0.06965694558329699\n",
            "Loss S1:  0.06979032642408914\n",
            "Loss S2:  0.06953826217966921\n",
            "Loss S1:  0.06997011119469268\n",
            "Loss S2:  0.06972117251792892\n",
            "Loss S1:  0.0700939324435214\n",
            "Loss S2:  0.06976935639977455\n",
            "Loss S1:  0.07010048071359411\n",
            "Loss S2:  0.06977385576860404\n",
            "Loss S1:  0.07020278148107477\n",
            "Loss S2:  0.0695684976712033\n",
            "Loss S1:  0.07006609288625198\n",
            "Loss S2:  0.06942112655332773\n",
            "Loss S1:  0.06976869229126621\n",
            "Loss S2:  0.06934731218728933\n",
            "Loss S1:  0.06982355247721199\n",
            "Loss S2:  0.0693207170414038\n",
            "Loss S1:  0.07009636596527719\n",
            "Loss S2:  0.06953934682462051\n",
            "Loss S1:  0.06994562286962853\n",
            "Loss S2:  0.0694402055484606\n",
            "Loss S1:  0.07002620127619497\n",
            "Loss S2:  0.06938822288682919\n",
            "Loss S1:  0.0700233477798308\n",
            "Loss S2:  0.06933450122705158\n",
            "Loss S1:  0.07031930728178275\n",
            "Loss S2:  0.0695422474098833\n",
            "Loss S1:  0.07043794894498356\n",
            "Loss S2:  0.06972943153746879\n",
            "Loss S1:  0.07042003758446709\n",
            "Loss S2:  0.06977580021579229\n",
            "Loss S1:  0.07029932461197104\n",
            "Loss S2:  0.06977371723201145\n",
            "Loss S1:  0.07035178043195421\n",
            "Loss S2:  0.06980575626424704\n",
            "Loss S1:  0.07035463294899302\n",
            "Loss S2:  0.06967131983977637\n",
            "Loss S1:  0.07031018433697296\n",
            "Loss S2:  0.06959087119409532\n",
            "Loss S1:  0.0705118309938314\n",
            "Loss S2:  0.06967694558110475\n",
            "Loss S1:  0.07045032919581193\n",
            "Loss S2:  0.06966838396462312\n",
            "Loss S1:  0.07049852713828343\n",
            "Loss S2:  0.06967826425972112\n",
            "Loss S1:  0.07048394851304068\n",
            "Loss S2:  0.06959278478778597\n",
            "Loss S1:  0.07049964502241687\n",
            "Loss S2:  0.06963883649252912\n",
            "Loss S1:  0.07051344350287594\n",
            "Loss S2:  0.06962212810094413\n",
            "Loss S1:  0.07049361663403306\n",
            "Loss S2:  0.06960298467514127\n",
            "Loss S1:  0.07054814008750333\n",
            "Loss S2:  0.06960574923220939\n",
            "Loss S1:  0.07061376762677947\n",
            "Loss S2:  0.06962205604229389\n",
            "Loss S1:  0.07050683885092461\n",
            "Loss S2:  0.06957127963083028\n",
            "Loss S1:  0.07060992833543733\n",
            "Loss S2:  0.06966044645636313\n",
            "Loss S1:  0.07062430600197907\n",
            "Loss S2:  0.0696769371267907\n",
            "Loss S1:  0.07063761963367132\n",
            "Loss S2:  0.06966699506065852\n",
            "Loss S1:  0.07069217954885285\n",
            "Loss S2:  0.06966758618819104\n",
            "Loss S1:  0.07065823128608269\n",
            "Loss S2:  0.06967903363696859\n",
            "Loss S1:  0.07060948969877284\n",
            "Loss S2:  0.06958140372810766\n",
            "Loss S1:  0.07062261656595585\n",
            "Loss S2:  0.06960354911679044\n",
            "Loss S1:  0.07052083496557245\n",
            "Loss S2:  0.06952216375591981\n",
            "Loss S1:  0.07051021975604485\n",
            "Loss S2:  0.06949466025355026\n",
            "Loss S1:  0.07054773654157767\n",
            "Loss S2:  0.06946307311824192\n",
            "Loss S1:  0.07054157590582258\n",
            "Loss S2:  0.06945462209803988\n",
            "Loss S1:  0.07057454840108719\n",
            "Loss S2:  0.06953478716066037\n",
            "Loss S1:  0.07055262507721297\n",
            "Loss S2:  0.06952527497537482\n",
            "Loss S1:  0.07054415792068605\n",
            "Loss S2:  0.06956356544751524\n",
            "Loss S1:  0.07051012057059768\n",
            "Loss S2:  0.06961535785556336\n",
            "Loss S1:  0.0704213789153973\n",
            "Loss S2:  0.06956938275585592\n",
            "Validation: \n",
            " Loss S1:  1.5808159112930298\n",
            " Loss S2:  1.5096136331558228\n",
            " Loss S1:  1.5514548051924932\n",
            " Loss S2:  1.5199932768231346\n",
            " Loss S1:  1.5415473420445511\n",
            " Loss S2:  1.510997935039241\n",
            " Loss S1:  1.5369431464398493\n",
            " Loss S2:  1.5032731900449658\n",
            " Loss S1:  1.539964814244965\n",
            " Loss S2:  1.504524952099647\n",
            "\n",
            "Epoch: 65\n",
            "Loss S1:  0.07074921578168869\n",
            "Loss S2:  0.07099219411611557\n",
            "Loss S1:  0.0692064267667857\n",
            "Loss S2:  0.06875714659690857\n",
            "Loss S1:  0.06884784854593731\n",
            "Loss S2:  0.06862676782267434\n",
            "Loss S1:  0.06923137665275604\n",
            "Loss S2:  0.06925478024828818\n",
            "Loss S1:  0.06946575632546007\n",
            "Loss S2:  0.06989531537018172\n",
            "Loss S1:  0.06889920630583576\n",
            "Loss S2:  0.0695199910186085\n",
            "Loss S1:  0.0691228224361529\n",
            "Loss S2:  0.06950280729864465\n",
            "Loss S1:  0.06894410968246595\n",
            "Loss S2:  0.0697884319323889\n",
            "Loss S1:  0.06923908685092572\n",
            "Loss S2:  0.06994276107461364\n",
            "Loss S1:  0.06939043362553303\n",
            "Loss S2:  0.06996067621550717\n",
            "Loss S1:  0.0692392172347201\n",
            "Loss S2:  0.06973033663955065\n",
            "Loss S1:  0.06917324642071852\n",
            "Loss S2:  0.06935893858338261\n",
            "Loss S1:  0.0690634247869992\n",
            "Loss S2:  0.06918057081990006\n",
            "Loss S1:  0.06920850427664874\n",
            "Loss S2:  0.06943014494449128\n",
            "Loss S1:  0.06936035133528372\n",
            "Loss S2:  0.06948749694629763\n",
            "Loss S1:  0.06929318895501806\n",
            "Loss S2:  0.06948615176393497\n",
            "Loss S1:  0.06926346255570465\n",
            "Loss S2:  0.0693873336411411\n",
            "Loss S1:  0.06941185214104709\n",
            "Loss S2:  0.06951147291743964\n",
            "Loss S1:  0.06960214092010293\n",
            "Loss S2:  0.06965273501382348\n",
            "Loss S1:  0.06957123411029421\n",
            "Loss S2:  0.06976353944674213\n",
            "Loss S1:  0.06962127210711365\n",
            "Loss S2:  0.06971154443511915\n",
            "Loss S1:  0.06967855829268835\n",
            "Loss S2:  0.0697321962406285\n",
            "Loss S1:  0.06967967962719736\n",
            "Loss S2:  0.069728266430926\n",
            "Loss S1:  0.06968986723588143\n",
            "Loss S2:  0.06959956916637751\n",
            "Loss S1:  0.06968077007851166\n",
            "Loss S2:  0.06968854744899322\n",
            "Loss S1:  0.0696842610628244\n",
            "Loss S2:  0.06970364855698856\n",
            "Loss S1:  0.06978647058322969\n",
            "Loss S2:  0.06979709458305461\n",
            "Loss S1:  0.06986199186021991\n",
            "Loss S2:  0.06978256612796185\n",
            "Loss S1:  0.06990457033188317\n",
            "Loss S2:  0.0698010703263758\n",
            "Loss S1:  0.0699343874512874\n",
            "Loss S2:  0.06977410425025572\n",
            "Loss S1:  0.06990398579212122\n",
            "Loss S2:  0.06974491908948287\n",
            "Loss S1:  0.06996530599868182\n",
            "Loss S2:  0.06976512765625666\n",
            "Loss S1:  0.07002066599199333\n",
            "Loss S2:  0.06982449316811339\n",
            "Loss S1:  0.069963062322752\n",
            "Loss S2:  0.06982195586686409\n",
            "Loss S1:  0.07005727872296162\n",
            "Loss S2:  0.06988301960603932\n",
            "Loss S1:  0.07005788540823167\n",
            "Loss S2:  0.06990774364298226\n",
            "Loss S1:  0.07013783407838721\n",
            "Loss S2:  0.06993626217019855\n",
            "Loss S1:  0.07013298900461583\n",
            "Loss S2:  0.06992451860657278\n",
            "Loss S1:  0.07009223168055842\n",
            "Loss S2:  0.06988555281924137\n",
            "Loss S1:  0.07013077738568606\n",
            "Loss S2:  0.06982090179343968\n",
            "Loss S1:  0.07019185606492429\n",
            "Loss S2:  0.0698722742795201\n",
            "Loss S1:  0.0701912426618619\n",
            "Loss S2:  0.06983813599042069\n",
            "Loss S1:  0.07019211315591942\n",
            "Loss S2:  0.06985449726359295\n",
            "Loss S1:  0.07019066757883384\n",
            "Loss S2:  0.06981330047731056\n",
            "Loss S1:  0.07017642926940032\n",
            "Loss S2:  0.06979193667672118\n",
            "Loss S1:  0.070248795900403\n",
            "Loss S2:  0.06986784911043364\n",
            "Loss S1:  0.0702379591876152\n",
            "Loss S2:  0.06979092034367833\n",
            "Loss S1:  0.07025127396019148\n",
            "Loss S2:  0.06978236284352159\n",
            "Loss S1:  0.07022445270958165\n",
            "Loss S2:  0.06974778827783223\n",
            "Loss S1:  0.07020063196781211\n",
            "Loss S2:  0.06967208790166073\n",
            "Validation: \n",
            " Loss S1:  1.6235665082931519\n",
            " Loss S2:  1.5187954902648926\n",
            " Loss S1:  1.5658163456689744\n",
            " Loss S2:  1.5175465175083704\n",
            " Loss S1:  1.556738745875475\n",
            " Loss S2:  1.5088848515254696\n",
            " Loss S1:  1.5515098083214682\n",
            " Loss S2:  1.5005395588327626\n",
            " Loss S1:  1.5541541576385498\n",
            " Loss S2:  1.5025968801828078\n",
            "\n",
            "Epoch: 66\n",
            "Loss S1:  0.06590230017900467\n",
            "Loss S2:  0.06629505008459091\n",
            "Loss S1:  0.06992005014961417\n",
            "Loss S2:  0.07021157308058305\n",
            "Loss S1:  0.06941076084261849\n",
            "Loss S2:  0.07002810743593034\n",
            "Loss S1:  0.0695580393075943\n",
            "Loss S2:  0.06943651024372346\n",
            "Loss S1:  0.07009988387183445\n",
            "Loss S2:  0.06979987188810255\n",
            "Loss S1:  0.07015773113451752\n",
            "Loss S2:  0.06986093002499319\n",
            "Loss S1:  0.0703972007651798\n",
            "Loss S2:  0.07013306894996127\n",
            "Loss S1:  0.07014295164967926\n",
            "Loss S2:  0.0700244771342882\n",
            "Loss S1:  0.07020490359009048\n",
            "Loss S2:  0.06996837276735424\n",
            "Loss S1:  0.06981893632929403\n",
            "Loss S2:  0.06987519148778129\n",
            "Loss S1:  0.06961134286357624\n",
            "Loss S2:  0.06971053048820779\n",
            "Loss S1:  0.06948756446709504\n",
            "Loss S2:  0.06940938226945766\n",
            "Loss S1:  0.06942398864621958\n",
            "Loss S2:  0.06948775536388405\n",
            "Loss S1:  0.06956290157924172\n",
            "Loss S2:  0.0695920193013344\n",
            "Loss S1:  0.06960886864797443\n",
            "Loss S2:  0.06974463207079164\n",
            "Loss S1:  0.06957483017780133\n",
            "Loss S2:  0.06977206468582153\n",
            "Loss S1:  0.06954777081097875\n",
            "Loss S2:  0.06956620745777344\n",
            "Loss S1:  0.06959363829061302\n",
            "Loss S2:  0.06961436369265729\n",
            "Loss S1:  0.06976459984209656\n",
            "Loss S2:  0.069750035006697\n",
            "Loss S1:  0.0698058366268405\n",
            "Loss S2:  0.06973933432426752\n",
            "Loss S1:  0.06990454670282739\n",
            "Loss S2:  0.06974232882557817\n",
            "Loss S1:  0.069912253751009\n",
            "Loss S2:  0.06957855637039619\n",
            "Loss S1:  0.06998060353740848\n",
            "Loss S2:  0.06959742573995935\n",
            "Loss S1:  0.07001338576599633\n",
            "Loss S2:  0.06959730202888513\n",
            "Loss S1:  0.07008184708994948\n",
            "Loss S2:  0.06969433673560867\n",
            "Loss S1:  0.0700507307373195\n",
            "Loss S2:  0.06966243874266803\n",
            "Loss S1:  0.07007705134792803\n",
            "Loss S2:  0.06972353094724859\n",
            "Loss S1:  0.07005873721134179\n",
            "Loss S2:  0.06970321234466845\n",
            "Loss S1:  0.07011975765599475\n",
            "Loss S2:  0.06979951388418038\n",
            "Loss S1:  0.07003189189378749\n",
            "Loss S2:  0.06975007029026235\n",
            "Loss S1:  0.0700132418485773\n",
            "Loss S2:  0.06977462704197512\n",
            "Loss S1:  0.07002107170185859\n",
            "Loss S2:  0.06978311145324799\n",
            "Loss S1:  0.0700776287676575\n",
            "Loss S2:  0.0698557720168543\n",
            "Loss S1:  0.06998832194061438\n",
            "Loss S2:  0.06982008421799207\n",
            "Loss S1:  0.0700930690070576\n",
            "Loss S2:  0.06989196058679537\n",
            "Loss S1:  0.0701478033239006\n",
            "Loss S2:  0.06991124660455943\n",
            "Loss S1:  0.07021514694735284\n",
            "Loss S2:  0.06988615564320887\n",
            "Loss S1:  0.0702262467471255\n",
            "Loss S2:  0.06984657530153215\n",
            "Loss S1:  0.07018864335702472\n",
            "Loss S2:  0.06979940451310063\n",
            "Loss S1:  0.0701678155652245\n",
            "Loss S2:  0.06971879648354352\n",
            "Loss S1:  0.07019615962989907\n",
            "Loss S2:  0.06977271234751342\n",
            "Loss S1:  0.07014032549376616\n",
            "Loss S2:  0.06966068582051863\n",
            "Loss S1:  0.0701708380230249\n",
            "Loss S2:  0.06966076261251103\n",
            "Loss S1:  0.07018449173034483\n",
            "Loss S2:  0.06966289982476256\n",
            "Loss S1:  0.07021121268504872\n",
            "Loss S2:  0.06964072835992793\n",
            "Loss S1:  0.07026828343085863\n",
            "Loss S2:  0.06964526123092603\n",
            "Loss S1:  0.07019963556670315\n",
            "Loss S2:  0.06959974346379395\n",
            "Loss S1:  0.07015866579017316\n",
            "Loss S2:  0.06962447862621325\n",
            "Loss S1:  0.07021536439533294\n",
            "Loss S2:  0.06966059005155137\n",
            "Loss S1:  0.07018430817819175\n",
            "Loss S2:  0.06957572543815295\n",
            "Validation: \n",
            " Loss S1:  1.588877558708191\n",
            " Loss S2:  1.5247396230697632\n",
            " Loss S1:  1.5449907098497664\n",
            " Loss S2:  1.5232695454642886\n",
            " Loss S1:  1.5363952037764759\n",
            " Loss S2:  1.5145479208085595\n",
            " Loss S1:  1.5318740156830335\n",
            " Loss S2:  1.5056741472150459\n",
            " Loss S1:  1.5341015509617182\n",
            " Loss S2:  1.5070215669679052\n",
            "\n",
            "Epoch: 67\n",
            "Loss S1:  0.07005242258310318\n",
            "Loss S2:  0.06989393383264542\n",
            "Loss S1:  0.06742418523539197\n",
            "Loss S2:  0.0684474997899749\n",
            "Loss S1:  0.06840761875112851\n",
            "Loss S2:  0.06788970352638335\n",
            "Loss S1:  0.06826605022914949\n",
            "Loss S2:  0.06775731629421634\n",
            "Loss S1:  0.0689919357437913\n",
            "Loss S2:  0.06909256137725783\n",
            "Loss S1:  0.06854181619835835\n",
            "Loss S2:  0.06875426650923841\n",
            "Loss S1:  0.06882855577058479\n",
            "Loss S2:  0.06918962018900231\n",
            "Loss S1:  0.06906729469626723\n",
            "Loss S2:  0.06924265192847856\n",
            "Loss S1:  0.06942837809522946\n",
            "Loss S2:  0.06923614350366003\n",
            "Loss S1:  0.06942970473523978\n",
            "Loss S2:  0.06917358533694194\n",
            "Loss S1:  0.06938068683047106\n",
            "Loss S2:  0.06909642464453632\n",
            "Loss S1:  0.0691429629317812\n",
            "Loss S2:  0.06882551684975624\n",
            "Loss S1:  0.06918421717098922\n",
            "Loss S2:  0.06884063007540939\n",
            "Loss S1:  0.0692189454122354\n",
            "Loss S2:  0.06880110442524648\n",
            "Loss S1:  0.06922765105222979\n",
            "Loss S2:  0.06880218704752888\n",
            "Loss S1:  0.0693009409634088\n",
            "Loss S2:  0.0690285341431763\n",
            "Loss S1:  0.0691564089578131\n",
            "Loss S2:  0.0689322876495234\n",
            "Loss S1:  0.06923103654942317\n",
            "Loss S2:  0.06895809187090884\n",
            "Loss S1:  0.0694056180053653\n",
            "Loss S2:  0.0690890533118946\n",
            "Loss S1:  0.06940374460988019\n",
            "Loss S2:  0.06907776514975188\n",
            "Loss S1:  0.06939233839511871\n",
            "Loss S2:  0.06908292551316432\n",
            "Loss S1:  0.06944363145819772\n",
            "Loss S2:  0.0691727898238112\n",
            "Loss S1:  0.06947342441227641\n",
            "Loss S2:  0.06919272635047792\n",
            "Loss S1:  0.06944909478936877\n",
            "Loss S2:  0.06920526476649495\n",
            "Loss S1:  0.06949016988215605\n",
            "Loss S2:  0.06920132455242126\n",
            "Loss S1:  0.06953227304783001\n",
            "Loss S2:  0.06918244636747467\n",
            "Loss S1:  0.06965021841884572\n",
            "Loss S2:  0.06932342620530804\n",
            "Loss S1:  0.06963382430402115\n",
            "Loss S2:  0.06929907569751088\n",
            "Loss S1:  0.06965057335767458\n",
            "Loss S2:  0.06933662677150604\n",
            "Loss S1:  0.06963358882212967\n",
            "Loss S2:  0.06932880164882571\n",
            "Loss S1:  0.06960707451170466\n",
            "Loss S2:  0.06929811624890546\n",
            "Loss S1:  0.06965640991015833\n",
            "Loss S2:  0.06934842395437492\n",
            "Loss S1:  0.06968053081000335\n",
            "Loss S2:  0.06933632732087577\n",
            "Loss S1:  0.06964739001274468\n",
            "Loss S2:  0.06931637425617149\n",
            "Loss S1:  0.06968730193210487\n",
            "Loss S2:  0.06935484473568952\n",
            "Loss S1:  0.06972811289365136\n",
            "Loss S2:  0.06936134210127032\n",
            "Loss S1:  0.06977804829663187\n",
            "Loss S2:  0.06940126541867811\n",
            "Loss S1:  0.06977297676943062\n",
            "Loss S2:  0.06940835420132647\n",
            "Loss S1:  0.06977145698439731\n",
            "Loss S2:  0.06937332014281919\n",
            "Loss S1:  0.06978961011714033\n",
            "Loss S2:  0.06929121560910169\n",
            "Loss S1:  0.06982098867248121\n",
            "Loss S2:  0.06930925635774236\n",
            "Loss S1:  0.06978270407865807\n",
            "Loss S2:  0.06927695842301178\n",
            "Loss S1:  0.06977951747858609\n",
            "Loss S2:  0.0692452007723251\n",
            "Loss S1:  0.06981016897754161\n",
            "Loss S2:  0.0691881477141325\n",
            "Loss S1:  0.06979139092591614\n",
            "Loss S2:  0.06915812290647402\n",
            "Loss S1:  0.06977518456275605\n",
            "Loss S2:  0.06920239474715256\n",
            "Loss S1:  0.06971623849552004\n",
            "Loss S2:  0.06912900260982958\n",
            "Loss S1:  0.06973206039098924\n",
            "Loss S2:  0.0691796779205465\n",
            "Loss S1:  0.06971403051810553\n",
            "Loss S2:  0.06919363501197087\n",
            "Loss S1:  0.06969019967888135\n",
            "Loss S2:  0.06911196084712042\n",
            "Validation: \n",
            " Loss S1:  1.5857043266296387\n",
            " Loss S2:  1.5186535120010376\n",
            " Loss S1:  1.5485665457589286\n",
            " Loss S2:  1.5163059518450783\n",
            " Loss S1:  1.5396629717291854\n",
            " Loss S2:  1.507774219280336\n",
            " Loss S1:  1.5352760885582595\n",
            " Loss S2:  1.4986179988892352\n",
            " Loss S1:  1.5377316106984644\n",
            " Loss S2:  1.5003684302906932\n",
            "\n",
            "Epoch: 68\n",
            "Loss S1:  0.07137488573789597\n",
            "Loss S2:  0.07294055074453354\n",
            "Loss S1:  0.06889529052105817\n",
            "Loss S2:  0.067834849384698\n",
            "Loss S1:  0.06817136101779483\n",
            "Loss S2:  0.06702804778303419\n",
            "Loss S1:  0.06880285230375105\n",
            "Loss S2:  0.06782282600479742\n",
            "Loss S1:  0.06954826605392665\n",
            "Loss S2:  0.0683437713035723\n",
            "Loss S1:  0.06953586893630963\n",
            "Loss S2:  0.06800198277422026\n",
            "Loss S1:  0.06952942718492179\n",
            "Loss S2:  0.06816060743370994\n",
            "Loss S1:  0.06929424022075156\n",
            "Loss S2:  0.06857694255214342\n",
            "Loss S1:  0.06946033299153234\n",
            "Loss S2:  0.0686422396497226\n",
            "Loss S1:  0.0693247197502917\n",
            "Loss S2:  0.06855096139914386\n",
            "Loss S1:  0.06916979917942888\n",
            "Loss S2:  0.06815466122462017\n",
            "Loss S1:  0.06883708501721288\n",
            "Loss S2:  0.0678602646667141\n",
            "Loss S1:  0.06889454250739625\n",
            "Loss S2:  0.06790201636877927\n",
            "Loss S1:  0.06894894882695365\n",
            "Loss S2:  0.06802707250791652\n",
            "Loss S1:  0.06875970663753807\n",
            "Loss S2:  0.06804682269481056\n",
            "Loss S1:  0.06884349468133308\n",
            "Loss S2:  0.06804275108094247\n",
            "Loss S1:  0.06881994434765407\n",
            "Loss S2:  0.0679739742823269\n",
            "Loss S1:  0.06908255606366877\n",
            "Loss S2:  0.06812850488295331\n",
            "Loss S1:  0.06915694878575551\n",
            "Loss S2:  0.06835938072171659\n",
            "Loss S1:  0.06905035489282683\n",
            "Loss S2:  0.06844336960796286\n",
            "Loss S1:  0.0691376734543499\n",
            "Loss S2:  0.06845700106958845\n",
            "Loss S1:  0.0691537015245989\n",
            "Loss S2:  0.06843363498977575\n",
            "Loss S1:  0.06916792382279673\n",
            "Loss S2:  0.06842267191315668\n",
            "Loss S1:  0.06911986200259877\n",
            "Loss S2:  0.06839452093684828\n",
            "Loss S1:  0.06922775196274782\n",
            "Loss S2:  0.0684532928145278\n",
            "Loss S1:  0.06929606551015044\n",
            "Loss S2:  0.06856568371394715\n",
            "Loss S1:  0.06939428566573223\n",
            "Loss S2:  0.06865685278999395\n",
            "Loss S1:  0.06942697173170058\n",
            "Loss S2:  0.06864323240199652\n",
            "Loss S1:  0.06951673898938712\n",
            "Loss S2:  0.06868651228231043\n",
            "Loss S1:  0.0694794530797865\n",
            "Loss S2:  0.06859810486487097\n",
            "Loss S1:  0.06949060744423406\n",
            "Loss S2:  0.06853496442948069\n",
            "Loss S1:  0.06957349588441696\n",
            "Loss S2:  0.06856835596383193\n",
            "Loss S1:  0.06959649744063523\n",
            "Loss S2:  0.06855648450808734\n",
            "Loss S1:  0.06956437064666762\n",
            "Loss S2:  0.06859153949196965\n",
            "Loss S1:  0.06964022440076574\n",
            "Loss S2:  0.06871098023224786\n",
            "Loss S1:  0.0696892669037027\n",
            "Loss S2:  0.06872001526999337\n",
            "Loss S1:  0.06971593274427913\n",
            "Loss S2:  0.06877520384261814\n",
            "Loss S1:  0.06972220386416121\n",
            "Loss S2:  0.06882441743726357\n",
            "Loss S1:  0.06972859853639064\n",
            "Loss S2:  0.06884408449837229\n",
            "Loss S1:  0.06966451292528826\n",
            "Loss S2:  0.06879567302515745\n",
            "Loss S1:  0.06973848890448152\n",
            "Loss S2:  0.0688516741102919\n",
            "Loss S1:  0.0696837616826061\n",
            "Loss S2:  0.06884425752535643\n",
            "Loss S1:  0.0697032219670589\n",
            "Loss S2:  0.06885723223701792\n",
            "Loss S1:  0.06970970276440656\n",
            "Loss S2:  0.06890400959796807\n",
            "Loss S1:  0.06966569027155976\n",
            "Loss S2:  0.06883059242685366\n",
            "Loss S1:  0.06972211861194105\n",
            "Loss S2:  0.06882995549597655\n",
            "Loss S1:  0.06968022614652837\n",
            "Loss S2:  0.06878466243146557\n",
            "Loss S1:  0.06967295069571766\n",
            "Loss S2:  0.06879404706053927\n",
            "Loss S1:  0.06965596985513356\n",
            "Loss S2:  0.06883085348687895\n",
            "Loss S1:  0.06958732169022872\n",
            "Loss S2:  0.06876340337750139\n",
            "Validation: \n",
            " Loss S1:  1.5736814737319946\n",
            " Loss S2:  1.5288567543029785\n",
            " Loss S1:  1.540548631123134\n",
            " Loss S2:  1.5223016171228319\n",
            " Loss S1:  1.532197644070881\n",
            " Loss S2:  1.5128061509713895\n",
            " Loss S1:  1.5283083094925176\n",
            " Loss S2:  1.5038116584058667\n",
            " Loss S1:  1.530711970211547\n",
            " Loss S2:  1.5055065110877708\n",
            "\n",
            "Epoch: 69\n",
            "Loss S1:  0.07058904320001602\n",
            "Loss S2:  0.07185027748346329\n",
            "Loss S1:  0.06752271645448425\n",
            "Loss S2:  0.06725225800817664\n",
            "Loss S1:  0.06886311212465876\n",
            "Loss S2:  0.06852720216626212\n",
            "Loss S1:  0.06859343102382075\n",
            "Loss S2:  0.06856257552581449\n",
            "Loss S1:  0.06941306273021348\n",
            "Loss S2:  0.0691445231801126\n",
            "Loss S1:  0.0692438584040193\n",
            "Loss S2:  0.06891349768813919\n",
            "Loss S1:  0.06935178824379797\n",
            "Loss S2:  0.0688971963817956\n",
            "Loss S1:  0.06979419862930204\n",
            "Loss S2:  0.06918728582456078\n",
            "Loss S1:  0.07016375012419841\n",
            "Loss S2:  0.06943470350018253\n",
            "Loss S1:  0.07020944483823828\n",
            "Loss S2:  0.06958360654803422\n",
            "Loss S1:  0.06984405094149089\n",
            "Loss S2:  0.06925940875074652\n",
            "Loss S1:  0.06957455845297994\n",
            "Loss S2:  0.06897067849163536\n",
            "Loss S1:  0.06935524940490723\n",
            "Loss S2:  0.06891692140378243\n",
            "Loss S1:  0.0694901201561207\n",
            "Loss S2:  0.06888471364633728\n",
            "Loss S1:  0.06945363601260152\n",
            "Loss S2:  0.06872624452126787\n",
            "Loss S1:  0.0692911557606514\n",
            "Loss S2:  0.06869481545903824\n",
            "Loss S1:  0.06918652779104546\n",
            "Loss S2:  0.0686313976412234\n",
            "Loss S1:  0.06925620323820421\n",
            "Loss S2:  0.0686533437791275\n",
            "Loss S1:  0.06935207907995466\n",
            "Loss S2:  0.06876531422385194\n",
            "Loss S1:  0.06925432928179572\n",
            "Loss S2:  0.06876273991549826\n",
            "Loss S1:  0.06918019999689724\n",
            "Loss S2:  0.06877287647765667\n",
            "Loss S1:  0.06927550612326482\n",
            "Loss S2:  0.06879496911620077\n",
            "Loss S1:  0.06940446075945418\n",
            "Loss S2:  0.06877698504884319\n",
            "Loss S1:  0.06945033803646698\n",
            "Loss S2:  0.06880431670647164\n",
            "Loss S1:  0.06952073888783633\n",
            "Loss S2:  0.06888021596734455\n",
            "Loss S1:  0.06952021686560604\n",
            "Loss S2:  0.068872841705839\n",
            "Loss S1:  0.0695759400554087\n",
            "Loss S2:  0.06892731189156857\n",
            "Loss S1:  0.06949275282531207\n",
            "Loss S2:  0.06881638476109593\n",
            "Loss S1:  0.06949688243643244\n",
            "Loss S2:  0.06884788735057111\n",
            "Loss S1:  0.06944963695718251\n",
            "Loss S2:  0.06876963357796374\n",
            "Loss S1:  0.06936042917972783\n",
            "Loss S2:  0.06870691266616318\n",
            "Loss S1:  0.06939552833365091\n",
            "Loss S2:  0.06876704301625203\n",
            "Loss S1:  0.06941406622621872\n",
            "Loss S2:  0.06876982900537434\n",
            "Loss S1:  0.06931178899780141\n",
            "Loss S2:  0.06867559883304593\n",
            "Loss S1:  0.06941493655229944\n",
            "Loss S2:  0.06875356650553491\n",
            "Loss S1:  0.06935320983500222\n",
            "Loss S2:  0.06876614601736056\n",
            "Loss S1:  0.0694193860046421\n",
            "Loss S2:  0.06880320072008962\n",
            "Loss S1:  0.06937475297650236\n",
            "Loss S2:  0.06877826330957387\n",
            "Loss S1:  0.06930951637233024\n",
            "Loss S2:  0.06867500287028436\n",
            "Loss S1:  0.06925348235327569\n",
            "Loss S2:  0.0685948436922582\n",
            "Loss S1:  0.06931399075579167\n",
            "Loss S2:  0.06863016375988498\n",
            "Loss S1:  0.06930166998665119\n",
            "Loss S2:  0.06854261343714095\n",
            "Loss S1:  0.06929322616186108\n",
            "Loss S2:  0.06852437641622051\n",
            "Loss S1:  0.06932205757830646\n",
            "Loss S2:  0.068514256796469\n",
            "Loss S1:  0.06929412152187354\n",
            "Loss S2:  0.06848283023351714\n",
            "Loss S1:  0.06932715091135708\n",
            "Loss S2:  0.06853511086159164\n",
            "Loss S1:  0.06930834731317394\n",
            "Loss S2:  0.06848602527813126\n",
            "Loss S1:  0.06930921064598798\n",
            "Loss S2:  0.06850507669546295\n",
            "Loss S1:  0.06934470928849153\n",
            "Loss S2:  0.06852865555244275\n",
            "Loss S1:  0.06932471119732332\n",
            "Loss S2:  0.06848706145696873\n",
            "Validation: \n",
            " Loss S1:  1.581992506980896\n",
            " Loss S2:  1.5207427740097046\n",
            " Loss S1:  1.5453557684308006\n",
            " Loss S2:  1.5212796075003487\n",
            " Loss S1:  1.5370236925962495\n",
            " Loss S2:  1.5140199225123336\n",
            " Loss S1:  1.5335389473399177\n",
            " Loss S2:  1.5056359670201287\n",
            " Loss S1:  1.5356818673051433\n",
            " Loss S2:  1.5067708300955502\n",
            "\n",
            "Epoch: 70\n",
            "Loss S1:  0.06774069368839264\n",
            "Loss S2:  0.0704299733042717\n",
            "Loss S1:  0.06900611350482161\n",
            "Loss S2:  0.06901310858401385\n",
            "Loss S1:  0.06816203218130838\n",
            "Loss S2:  0.06872454995200747\n",
            "Loss S1:  0.06870818570736915\n",
            "Loss S2:  0.06860673271359936\n",
            "Loss S1:  0.06927207112312317\n",
            "Loss S2:  0.06922076506222166\n",
            "Loss S1:  0.06937562177578609\n",
            "Loss S2:  0.06882971028486888\n",
            "Loss S1:  0.06959765697600412\n",
            "Loss S2:  0.06926992572233324\n",
            "Loss S1:  0.06960797782095385\n",
            "Loss S2:  0.06933293119072914\n",
            "Loss S1:  0.06998613688312931\n",
            "Loss S2:  0.06951188870970114\n",
            "Loss S1:  0.06993536041169376\n",
            "Loss S2:  0.06921754560464032\n",
            "Loss S1:  0.0696640470904289\n",
            "Loss S2:  0.06894744401521022\n",
            "Loss S1:  0.06932815892605095\n",
            "Loss S2:  0.0687200543848244\n",
            "Loss S1:  0.06911407826730043\n",
            "Loss S2:  0.06847501293686796\n",
            "Loss S1:  0.06895692131787766\n",
            "Loss S2:  0.06857065913331417\n",
            "Loss S1:  0.06900133192539215\n",
            "Loss S2:  0.06847561948688317\n",
            "Loss S1:  0.06890697084021884\n",
            "Loss S2:  0.06841107924154263\n",
            "Loss S1:  0.06886359248657405\n",
            "Loss S2:  0.06837037858200369\n",
            "Loss S1:  0.06892864817851468\n",
            "Loss S2:  0.0684715871922454\n",
            "Loss S1:  0.06906442391526633\n",
            "Loss S2:  0.06867077444469072\n",
            "Loss S1:  0.06897804018168549\n",
            "Loss S2:  0.06874017413530051\n",
            "Loss S1:  0.06905460872905171\n",
            "Loss S2:  0.06880006394279536\n",
            "Loss S1:  0.06911482224108484\n",
            "Loss S2:  0.06867788357757279\n",
            "Loss S1:  0.06919692541004846\n",
            "Loss S2:  0.06866901748860044\n",
            "Loss S1:  0.06918594749136404\n",
            "Loss S2:  0.06861710058146225\n",
            "Loss S1:  0.06930234903060055\n",
            "Loss S2:  0.06867003085331304\n",
            "Loss S1:  0.06926490367943072\n",
            "Loss S2:  0.06867291675917656\n",
            "Loss S1:  0.06925603571808202\n",
            "Loss S2:  0.0686496198605532\n",
            "Loss S1:  0.06929246945189814\n",
            "Loss S2:  0.06860889311591198\n",
            "Loss S1:  0.06935019198526692\n",
            "Loss S2:  0.06868581830447679\n",
            "Loss S1:  0.06932600378938966\n",
            "Loss S2:  0.06867162200951904\n",
            "Loss S1:  0.06922124794056249\n",
            "Loss S2:  0.06863603468650203\n",
            "Loss S1:  0.06927731512204244\n",
            "Loss S2:  0.06865809100808822\n",
            "Loss S1:  0.06930626403087768\n",
            "Loss S2:  0.06862008881485351\n",
            "Loss S1:  0.06921193283895351\n",
            "Loss S2:  0.06858487893177304\n",
            "Loss S1:  0.06919086928809842\n",
            "Loss S2:  0.06871049593358446\n",
            "Loss S1:  0.06925342173615412\n",
            "Loss S2:  0.06873764542390479\n",
            "Loss S1:  0.06926441170956289\n",
            "Loss S2:  0.06874604118662858\n",
            "Loss S1:  0.06928121948218088\n",
            "Loss S2:  0.0688008285275688\n",
            "Loss S1:  0.06925519742560513\n",
            "Loss S2:  0.06874560172672034\n",
            "Loss S1:  0.06922930328513655\n",
            "Loss S2:  0.06871766051101258\n",
            "Loss S1:  0.06924314800640591\n",
            "Loss S2:  0.06869163436633988\n",
            "Loss S1:  0.06923353324877665\n",
            "Loss S2:  0.06865358355380323\n",
            "Loss S1:  0.06923316474871227\n",
            "Loss S2:  0.06863661227203605\n",
            "Loss S1:  0.06926986299403305\n",
            "Loss S2:  0.06865227163667745\n",
            "Loss S1:  0.06928069072684734\n",
            "Loss S2:  0.0685979769377211\n",
            "Loss S1:  0.06935125173889348\n",
            "Loss S2:  0.06860548986365685\n",
            "Loss S1:  0.06931225020527322\n",
            "Loss S2:  0.06857451979967881\n",
            "Loss S1:  0.06933364976047204\n",
            "Loss S2:  0.0685988264019955\n",
            "Loss S1:  0.06937432741424894\n",
            "Loss S2:  0.06864514032266492\n",
            "Loss S1:  0.06931427021770759\n",
            "Loss S2:  0.06855612020567818\n",
            "Validation: \n",
            " Loss S1:  1.596598744392395\n",
            " Loss S2:  1.5149035453796387\n",
            " Loss S1:  1.5517398629869734\n",
            " Loss S2:  1.51242196559906\n",
            " Loss S1:  1.5422644760550521\n",
            " Loss S2:  1.5032235354911991\n",
            " Loss S1:  1.5376774287614665\n",
            " Loss S2:  1.4947920666366328\n",
            " Loss S1:  1.540532675790198\n",
            " Loss S2:  1.4965026643541124\n",
            "\n",
            "Epoch: 71\n",
            "Loss S1:  0.06830891221761703\n",
            "Loss S2:  0.06532999128103256\n",
            "Loss S1:  0.06884059106761758\n",
            "Loss S2:  0.06896199252117764\n",
            "Loss S1:  0.06785492528052557\n",
            "Loss S2:  0.0667099216509433\n",
            "Loss S1:  0.06831947377612514\n",
            "Loss S2:  0.06673154138749646\n",
            "Loss S1:  0.06894392447500694\n",
            "Loss S2:  0.06703407689929008\n",
            "Loss S1:  0.06878786329545226\n",
            "Loss S2:  0.06669524217060968\n",
            "Loss S1:  0.06931922029032082\n",
            "Loss S2:  0.06723153633905239\n",
            "Loss S1:  0.06959754380751663\n",
            "Loss S2:  0.06736535285140427\n",
            "Loss S1:  0.06960927478877114\n",
            "Loss S2:  0.06752123738880511\n",
            "Loss S1:  0.06941214437176893\n",
            "Loss S2:  0.06778067436355811\n",
            "Loss S1:  0.06917882917246015\n",
            "Loss S2:  0.06772847894099679\n",
            "Loss S1:  0.06900820441476933\n",
            "Loss S2:  0.0677003926924757\n",
            "Loss S1:  0.06901000079906676\n",
            "Loss S2:  0.0675731526056597\n",
            "Loss S1:  0.0689569300197008\n",
            "Loss S2:  0.0678343661413848\n",
            "Loss S1:  0.06891661483450984\n",
            "Loss S2:  0.06788916811875417\n",
            "Loss S1:  0.06894048567343232\n",
            "Loss S2:  0.06792264333031825\n",
            "Loss S1:  0.06877346713998303\n",
            "Loss S2:  0.06779548373096478\n",
            "Loss S1:  0.06885828350109664\n",
            "Loss S2:  0.06796703975625902\n",
            "Loss S1:  0.06888146630555227\n",
            "Loss S2:  0.06803716225189399\n",
            "Loss S1:  0.0687983076024742\n",
            "Loss S2:  0.06808414448696282\n",
            "Loss S1:  0.0688063924325936\n",
            "Loss S2:  0.06817205986054382\n",
            "Loss S1:  0.06881559454787399\n",
            "Loss S2:  0.06828113325752354\n",
            "Loss S1:  0.06886561626222878\n",
            "Loss S2:  0.06821272255878104\n",
            "Loss S1:  0.0689246275208213\n",
            "Loss S2:  0.06822273235171389\n",
            "Loss S1:  0.06898262486232762\n",
            "Loss S2:  0.06823792716650548\n",
            "Loss S1:  0.06893893363941238\n",
            "Loss S2:  0.06818914745908335\n",
            "Loss S1:  0.06899948612255155\n",
            "Loss S2:  0.0682024404948927\n",
            "Loss S1:  0.06895472360877973\n",
            "Loss S2:  0.06816113063879999\n",
            "Loss S1:  0.06898610996500029\n",
            "Loss S2:  0.06821892361464874\n",
            "Loss S1:  0.06892404400605925\n",
            "Loss S2:  0.06820534361699193\n",
            "Loss S1:  0.06886177939483494\n",
            "Loss S2:  0.06814419947242815\n",
            "Loss S1:  0.06892034750419798\n",
            "Loss S2:  0.06816442516839005\n",
            "Loss S1:  0.06896078014438768\n",
            "Loss S2:  0.0681794396140308\n",
            "Loss S1:  0.06888542188123274\n",
            "Loss S2:  0.06817859614948851\n",
            "Loss S1:  0.06897578350216413\n",
            "Loss S2:  0.06824546570608343\n",
            "Loss S1:  0.068979183822191\n",
            "Loss S2:  0.06824885726909012\n",
            "Loss S1:  0.06901621165326757\n",
            "Loss S2:  0.06827560110518147\n",
            "Loss S1:  0.06903555312487636\n",
            "Loss S2:  0.06821379870818953\n",
            "Loss S1:  0.06902028393401248\n",
            "Loss S2:  0.06814950862972755\n",
            "Loss S1:  0.06899131136134153\n",
            "Loss S2:  0.06813352257775529\n",
            "Loss S1:  0.06902234872194597\n",
            "Loss S2:  0.06822695687153095\n",
            "Loss S1:  0.06899331586204306\n",
            "Loss S2:  0.06820670036721403\n",
            "Loss S1:  0.06898629835269231\n",
            "Loss S2:  0.06814327652869202\n",
            "Loss S1:  0.0690333653595774\n",
            "Loss S2:  0.06814083643689786\n",
            "Loss S1:  0.06902001004100117\n",
            "Loss S2:  0.06812886204443822\n",
            "Loss S1:  0.0690328002702899\n",
            "Loss S2:  0.06813114782394697\n",
            "Loss S1:  0.06895358316648809\n",
            "Loss S2:  0.06807022296910431\n",
            "Loss S1:  0.06895614446253534\n",
            "Loss S2:  0.06809223404514056\n",
            "Loss S1:  0.06898144166335743\n",
            "Loss S2:  0.06810635350753017\n",
            "Loss S1:  0.0689569182936747\n",
            "Loss S2:  0.06801256361415575\n",
            "Validation: \n",
            " Loss S1:  1.5937023162841797\n",
            " Loss S2:  1.51359224319458\n",
            " Loss S1:  1.5491110654104323\n",
            " Loss S2:  1.513831201053801\n",
            " Loss S1:  1.540464965308585\n",
            " Loss S2:  1.5051847260172775\n",
            " Loss S1:  1.535912687661218\n",
            " Loss S2:  1.4975269583405042\n",
            " Loss S1:  1.538543403884511\n",
            " Loss S2:  1.499225965252629\n",
            "\n",
            "Epoch: 72\n",
            "Loss S1:  0.07010328769683838\n",
            "Loss S2:  0.07378017902374268\n",
            "Loss S1:  0.06933238966898485\n",
            "Loss S2:  0.06757523593577472\n",
            "Loss S1:  0.06781588778609321\n",
            "Loss S2:  0.0671806028556256\n",
            "Loss S1:  0.0677339491103926\n",
            "Loss S2:  0.0668591394299461\n",
            "Loss S1:  0.06866357720843176\n",
            "Loss S2:  0.06718045036967207\n",
            "Loss S1:  0.06834681427069739\n",
            "Loss S2:  0.06713025089280278\n",
            "Loss S1:  0.06821916710402144\n",
            "Loss S2:  0.06735084291364325\n",
            "Loss S1:  0.06815515196239444\n",
            "Loss S2:  0.06728057857130615\n",
            "Loss S1:  0.06853847573568791\n",
            "Loss S2:  0.06737395063226606\n",
            "Loss S1:  0.06856270028012139\n",
            "Loss S2:  0.06749459933284875\n",
            "Loss S1:  0.06840435935571643\n",
            "Loss S2:  0.06760232851351841\n",
            "Loss S1:  0.06827220220018078\n",
            "Loss S2:  0.06752490453623436\n",
            "Loss S1:  0.0683479306003279\n",
            "Loss S2:  0.06740540887944954\n",
            "Loss S1:  0.06834101563191596\n",
            "Loss S2:  0.06748238848593399\n",
            "Loss S1:  0.0681841013068003\n",
            "Loss S2:  0.0674180751468273\n",
            "Loss S1:  0.06813258719661378\n",
            "Loss S2:  0.06742216922984218\n",
            "Loss S1:  0.06798358819221859\n",
            "Loss S2:  0.06742727020697564\n",
            "Loss S1:  0.0679422652250842\n",
            "Loss S2:  0.0673890651928054\n",
            "Loss S1:  0.06809330138876953\n",
            "Loss S2:  0.0676477763451924\n",
            "Loss S1:  0.06812645366210587\n",
            "Loss S2:  0.06775595628076199\n",
            "Loss S1:  0.06815050520114045\n",
            "Loss S2:  0.06769523424888725\n",
            "Loss S1:  0.0681555326586651\n",
            "Loss S2:  0.06772235627352344\n",
            "Loss S1:  0.06819271117583658\n",
            "Loss S2:  0.06768447386598156\n",
            "Loss S1:  0.06818905555002101\n",
            "Loss S2:  0.06766445697708563\n",
            "Loss S1:  0.06821914802448384\n",
            "Loss S2:  0.06771952490403445\n",
            "Loss S1:  0.06822485246684447\n",
            "Loss S2:  0.06776102292407081\n",
            "Loss S1:  0.06830947898984868\n",
            "Loss S2:  0.06780761551697136\n",
            "Loss S1:  0.06835289966191313\n",
            "Loss S2:  0.0677716570229328\n",
            "Loss S1:  0.06838521866452651\n",
            "Loss S2:  0.06784388116723278\n",
            "Loss S1:  0.06834798005387135\n",
            "Loss S2:  0.06782187202695719\n",
            "Loss S1:  0.0683888062908998\n",
            "Loss S2:  0.06782887234937313\n",
            "Loss S1:  0.06842847411607622\n",
            "Loss S2:  0.06783745297712912\n",
            "Loss S1:  0.06841025283310644\n",
            "Loss S2:  0.06782228427698307\n",
            "Loss S1:  0.06832628277358331\n",
            "Loss S2:  0.06772821323434032\n",
            "Loss S1:  0.06841582251040816\n",
            "Loss S2:  0.06778024149037176\n",
            "Loss S1:  0.06844027762823975\n",
            "Loss S2:  0.06774877781244765\n",
            "Loss S1:  0.06848764715721402\n",
            "Loss S2:  0.06778158435283275\n",
            "Loss S1:  0.06846284863320644\n",
            "Loss S2:  0.06776689157653047\n",
            "Loss S1:  0.06839196834703443\n",
            "Loss S2:  0.06773069504834223\n",
            "Loss S1:  0.06840877978088301\n",
            "Loss S2:  0.06776319241241725\n",
            "Loss S1:  0.06842712603713806\n",
            "Loss S2:  0.06777987658903188\n",
            "Loss S1:  0.06837006651970882\n",
            "Loss S2:  0.06773759177240142\n",
            "Loss S1:  0.0683521605461765\n",
            "Loss S2:  0.0676686100113137\n",
            "Loss S1:  0.06835434175249872\n",
            "Loss S2:  0.06765676914712115\n",
            "Loss S1:  0.06836604949534615\n",
            "Loss S2:  0.06767165271434383\n",
            "Loss S1:  0.06836499880891946\n",
            "Loss S2:  0.06769432917964167\n",
            "Loss S1:  0.06834425620427617\n",
            "Loss S2:  0.06761966133389194\n",
            "Loss S1:  0.06838170780687575\n",
            "Loss S2:  0.06766117979513359\n",
            "Loss S1:  0.06839516423565187\n",
            "Loss S2:  0.06765738252759476\n",
            "Loss S1:  0.06832250332850545\n",
            "Loss S2:  0.06755918513247534\n",
            "Validation: \n",
            " Loss S1:  1.5775699615478516\n",
            " Loss S2:  1.518318772315979\n",
            " Loss S1:  1.5480296668552218\n",
            " Loss S2:  1.5161594152450562\n",
            " Loss S1:  1.537734139256361\n",
            " Loss S2:  1.5073078987075061\n",
            " Loss S1:  1.532440046795079\n",
            " Loss S2:  1.4983596489077708\n",
            " Loss S1:  1.534806567945598\n",
            " Loss S2:  1.4996775771364754\n",
            "\n",
            "Epoch: 73\n",
            "Loss S1:  0.06192108616232872\n",
            "Loss S2:  0.06877434253692627\n",
            "Loss S1:  0.06542282585393298\n",
            "Loss S2:  0.06661281836303798\n",
            "Loss S1:  0.06634462252259254\n",
            "Loss S2:  0.06642748646083332\n",
            "Loss S1:  0.06707957831601943\n",
            "Loss S2:  0.06700560883168251\n",
            "Loss S1:  0.06763810755276098\n",
            "Loss S2:  0.06755973580406933\n",
            "Loss S1:  0.06777804644376624\n",
            "Loss S2:  0.06746191093150307\n",
            "Loss S1:  0.06817832576935409\n",
            "Loss S2:  0.06804159949304628\n",
            "Loss S1:  0.06813746821922316\n",
            "Loss S2:  0.06795396073393419\n",
            "Loss S1:  0.0683125026156137\n",
            "Loss S2:  0.06823689210010164\n",
            "Loss S1:  0.06827586574050096\n",
            "Loss S2:  0.06822153876770984\n",
            "Loss S1:  0.06805028157806632\n",
            "Loss S2:  0.06778463890942016\n",
            "Loss S1:  0.06802608821171897\n",
            "Loss S2:  0.06758575406563175\n",
            "Loss S1:  0.06804798768202136\n",
            "Loss S2:  0.06774035699603971\n",
            "Loss S1:  0.06811608197807356\n",
            "Loss S2:  0.06773863732814789\n",
            "Loss S1:  0.06799053173538641\n",
            "Loss S2:  0.06766783152805998\n",
            "Loss S1:  0.06798788984090287\n",
            "Loss S2:  0.06767692493385827\n",
            "Loss S1:  0.06791972327306404\n",
            "Loss S2:  0.06758891036791831\n",
            "Loss S1:  0.06808439058344266\n",
            "Loss S2:  0.06760482823378161\n",
            "Loss S1:  0.06817133874935999\n",
            "Loss S2:  0.06773562640857302\n",
            "Loss S1:  0.06823852295494828\n",
            "Loss S2:  0.06779677415003327\n",
            "Loss S1:  0.06821491441397524\n",
            "Loss S2:  0.06771832226372476\n",
            "Loss S1:  0.06821855039322545\n",
            "Loss S2:  0.06772031290765622\n",
            "Loss S1:  0.06820256102880741\n",
            "Loss S2:  0.06769850071334192\n",
            "Loss S1:  0.06820731626186537\n",
            "Loss S2:  0.06767038541374268\n",
            "Loss S1:  0.06822113297044984\n",
            "Loss S2:  0.06770488326109296\n",
            "Loss S1:  0.06818599644232082\n",
            "Loss S2:  0.06772597297373045\n",
            "Loss S1:  0.06825108903235402\n",
            "Loss S2:  0.06775676680307735\n",
            "Loss S1:  0.06829613221747409\n",
            "Loss S2:  0.06772377950953822\n",
            "Loss S1:  0.06843854578417391\n",
            "Loss S2:  0.06781054772038901\n",
            "Loss S1:  0.06839836340179968\n",
            "Loss S2:  0.06775799871156715\n",
            "Loss S1:  0.06844581901235042\n",
            "Loss S2:  0.06779981256026366\n",
            "Loss S1:  0.06853101213262013\n",
            "Loss S2:  0.06786148003347434\n",
            "Loss S1:  0.06856747096944078\n",
            "Loss S2:  0.06789515921817019\n",
            "Loss S1:  0.06850450222585856\n",
            "Loss S2:  0.06787165937511942\n",
            "Loss S1:  0.0685573738516251\n",
            "Loss S2:  0.06794183907751813\n",
            "Loss S1:  0.06853373968533301\n",
            "Loss S2:  0.06793485958649223\n",
            "Loss S1:  0.06859724214326311\n",
            "Loss S2:  0.06796215606693416\n",
            "Loss S1:  0.06859793954822574\n",
            "Loss S2:  0.06793106295108152\n",
            "Loss S1:  0.0685989196434265\n",
            "Loss S2:  0.06791047990478556\n",
            "Loss S1:  0.06854507930176643\n",
            "Loss S2:  0.0678385170676824\n",
            "Loss S1:  0.06858697763070501\n",
            "Loss S2:  0.06782752562527943\n",
            "Loss S1:  0.06851271051378725\n",
            "Loss S2:  0.0677554821754169\n",
            "Loss S1:  0.06855297769085812\n",
            "Loss S2:  0.06775858905387604\n",
            "Loss S1:  0.06857205043155744\n",
            "Loss S2:  0.0677800229350818\n",
            "Loss S1:  0.0685640301918632\n",
            "Loss S2:  0.06776873212383717\n",
            "Loss S1:  0.06859135587272253\n",
            "Loss S2:  0.067801559951049\n",
            "Loss S1:  0.0685589820421311\n",
            "Loss S2:  0.06776378912257269\n",
            "Loss S1:  0.06857320735124266\n",
            "Loss S2:  0.06778915719210216\n",
            "Loss S1:  0.06857872558389781\n",
            "Loss S2:  0.06776792108393014\n",
            "Loss S1:  0.06851798304320596\n",
            "Loss S2:  0.06771868028793704\n",
            "Validation: \n",
            " Loss S1:  1.5777233839035034\n",
            " Loss S2:  1.5235230922698975\n",
            " Loss S1:  1.5491522209984916\n",
            " Loss S2:  1.517378256434486\n",
            " Loss S1:  1.5422296233293487\n",
            " Loss S2:  1.5082685075155118\n",
            " Loss S1:  1.5377424271380316\n",
            " Loss S2:  1.5010587074717536\n",
            " Loss S1:  1.540348619590571\n",
            " Loss S2:  1.5027001124841195\n",
            "\n",
            "Epoch: 74\n",
            "Loss S1:  0.07031736522912979\n",
            "Loss S2:  0.0731152817606926\n",
            "Loss S1:  0.06594846025109291\n",
            "Loss S2:  0.06720859252593735\n",
            "Loss S1:  0.06626048258372716\n",
            "Loss S2:  0.06649451525438399\n",
            "Loss S1:  0.06735397767155402\n",
            "Loss S2:  0.06659056234263605\n",
            "Loss S1:  0.0675266279679973\n",
            "Loss S2:  0.06648256593361133\n",
            "Loss S1:  0.06723231581204078\n",
            "Loss S2:  0.06636877825447157\n",
            "Loss S1:  0.06777797706547331\n",
            "Loss S2:  0.06664205544063302\n",
            "Loss S1:  0.06791088042754523\n",
            "Loss S2:  0.06701134502048224\n",
            "Loss S1:  0.06798136340063295\n",
            "Loss S2:  0.06724698367862054\n",
            "Loss S1:  0.06786902183359796\n",
            "Loss S2:  0.06709269295026968\n",
            "Loss S1:  0.06771515885202012\n",
            "Loss S2:  0.0671001763848385\n",
            "Loss S1:  0.06758621261195019\n",
            "Loss S2:  0.06680533387236767\n",
            "Loss S1:  0.06755788519609073\n",
            "Loss S2:  0.0668128174999036\n",
            "Loss S1:  0.0677811591566064\n",
            "Loss S2:  0.06687186342728046\n",
            "Loss S1:  0.06776434965167485\n",
            "Loss S2:  0.06678417885134406\n",
            "Loss S1:  0.0676867128750741\n",
            "Loss S2:  0.06674486497380086\n",
            "Loss S1:  0.06764050522205993\n",
            "Loss S2:  0.06662391288124997\n",
            "Loss S1:  0.06779953872251232\n",
            "Loss S2:  0.06685401619090671\n",
            "Loss S1:  0.06795652770535063\n",
            "Loss S2:  0.0670147745031349\n",
            "Loss S1:  0.0679726531992408\n",
            "Loss S2:  0.06703729106415629\n",
            "Loss S1:  0.06798820141980898\n",
            "Loss S2:  0.0670933826682876\n",
            "Loss S1:  0.06812597917154502\n",
            "Loss S2:  0.06713683995025418\n",
            "Loss S1:  0.06826783345835241\n",
            "Loss S2:  0.06718272789975636\n",
            "Loss S1:  0.06828594678666168\n",
            "Loss S2:  0.06720727849832345\n",
            "Loss S1:  0.06839531686357937\n",
            "Loss S2:  0.06727415524206716\n",
            "Loss S1:  0.06837107302005073\n",
            "Loss S2:  0.06734729222804901\n",
            "Loss S1:  0.06848608295636616\n",
            "Loss S2:  0.06741408221566357\n",
            "Loss S1:  0.06847648675866233\n",
            "Loss S2:  0.0673902578019568\n",
            "Loss S1:  0.06856874717247019\n",
            "Loss S2:  0.06747016536065267\n",
            "Loss S1:  0.06854102763113697\n",
            "Loss S2:  0.06742070879006304\n",
            "Loss S1:  0.06855728393080227\n",
            "Loss S2:  0.06742097698523357\n",
            "Loss S1:  0.06851692749277188\n",
            "Loss S2:  0.06744245337951221\n",
            "Loss S1:  0.06855162574309055\n",
            "Loss S2:  0.06747687221548268\n",
            "Loss S1:  0.06844657189086124\n",
            "Loss S2:  0.06745724660257198\n",
            "Loss S1:  0.0685174755883182\n",
            "Loss S2:  0.06749733043643387\n",
            "Loss S1:  0.06851989492519289\n",
            "Loss S2:  0.06752242067260959\n",
            "Loss S1:  0.06855810637949576\n",
            "Loss S2:  0.0675463310115225\n",
            "Loss S1:  0.06858410331799657\n",
            "Loss S2:  0.06755654120220328\n",
            "Loss S1:  0.0685025570019493\n",
            "Loss S2:  0.06747145023871595\n",
            "Loss S1:  0.06849849307933427\n",
            "Loss S2:  0.06742630324438405\n",
            "Loss S1:  0.0685629653848912\n",
            "Loss S2:  0.06744455176398641\n",
            "Loss S1:  0.06853004060051157\n",
            "Loss S2:  0.0673882134820713\n",
            "Loss S1:  0.06846962285155073\n",
            "Loss S2:  0.06734794237586778\n",
            "Loss S1:  0.06847897923082598\n",
            "Loss S2:  0.06733632155990103\n",
            "Loss S1:  0.06843463874626862\n",
            "Loss S2:  0.06730056418076394\n",
            "Loss S1:  0.06847396797754547\n",
            "Loss S2:  0.06730886075455704\n",
            "Loss S1:  0.06843330275566356\n",
            "Loss S2:  0.06729149597812113\n",
            "Loss S1:  0.06843905346250585\n",
            "Loss S2:  0.06734662866041918\n",
            "Loss S1:  0.06843998026017588\n",
            "Loss S2:  0.0673773191451035\n",
            "Loss S1:  0.06838902535937469\n",
            "Loss S2:  0.0673247668626595\n",
            "Validation: \n",
            " Loss S1:  1.5839409828186035\n",
            " Loss S2:  1.5272926092147827\n",
            " Loss S1:  1.5591279041199457\n",
            " Loss S2:  1.5239402850468953\n",
            " Loss S1:  1.5512358182814063\n",
            " Loss S2:  1.5151584293784164\n",
            " Loss S1:  1.5469712859294453\n",
            " Loss S2:  1.5071713044995168\n",
            " Loss S1:  1.5484415292739868\n",
            " Loss S2:  1.508317658930649\n",
            "\n",
            "Epoch: 75\n",
            "Loss S1:  0.07364822924137115\n",
            "Loss S2:  0.07879424095153809\n",
            "Loss S1:  0.0682940726930445\n",
            "Loss S2:  0.0683555467562242\n",
            "Loss S1:  0.06819510601815723\n",
            "Loss S2:  0.06681597942397707\n",
            "Loss S1:  0.06806509088604681\n",
            "Loss S2:  0.06700139360562447\n",
            "Loss S1:  0.0686734581320751\n",
            "Loss S2:  0.06747076624050373\n",
            "Loss S1:  0.06840421916807399\n",
            "Loss S2:  0.06698534688821026\n",
            "Loss S1:  0.0684392945688279\n",
            "Loss S2:  0.06724159315717025\n",
            "Loss S1:  0.06864991385332296\n",
            "Loss S2:  0.06760472662641968\n",
            "Loss S1:  0.06853685523440808\n",
            "Loss S2:  0.06783631007060592\n",
            "Loss S1:  0.06862298174055068\n",
            "Loss S2:  0.06766914813728123\n",
            "Loss S1:  0.06846976608480557\n",
            "Loss S2:  0.06733370269879256\n",
            "Loss S1:  0.06821702051538604\n",
            "Loss S2:  0.06724901571198627\n",
            "Loss S1:  0.06807706742124124\n",
            "Loss S2:  0.06713993400951063\n",
            "Loss S1:  0.06817446057578079\n",
            "Loss S2:  0.06726835850085922\n",
            "Loss S1:  0.06813068875501342\n",
            "Loss S2:  0.06718053002940848\n",
            "Loss S1:  0.06810337090433038\n",
            "Loss S2:  0.0671068507166493\n",
            "Loss S1:  0.06795447784828844\n",
            "Loss S2:  0.06690780302474958\n",
            "Loss S1:  0.06797066786222988\n",
            "Loss S2:  0.06697059093773017\n",
            "Loss S1:  0.0681228051939722\n",
            "Loss S2:  0.06707659806253502\n",
            "Loss S1:  0.06804057924535262\n",
            "Loss S2:  0.067103701677778\n",
            "Loss S1:  0.06807914720987206\n",
            "Loss S2:  0.06711065094565871\n",
            "Loss S1:  0.06804532575381311\n",
            "Loss S2:  0.06712359020495302\n",
            "Loss S1:  0.06812849436882394\n",
            "Loss S2:  0.06713095695050054\n",
            "Loss S1:  0.06809818401158631\n",
            "Loss S2:  0.067062432131855\n",
            "Loss S1:  0.06817074616234826\n",
            "Loss S2:  0.06717936312818429\n",
            "Loss S1:  0.06817467988605994\n",
            "Loss S2:  0.06716093735628394\n",
            "Loss S1:  0.06822238410055866\n",
            "Loss S2:  0.06720593107785758\n",
            "Loss S1:  0.06815852985005977\n",
            "Loss S2:  0.06714422867471881\n",
            "Loss S1:  0.068276849041736\n",
            "Loss S2:  0.06725702515647505\n",
            "Loss S1:  0.06823635700437211\n",
            "Loss S2:  0.0671085971199565\n",
            "Loss S1:  0.0681618828561615\n",
            "Loss S2:  0.06708296699034812\n",
            "Loss S1:  0.06814343832025957\n",
            "Loss S2:  0.0671574219438424\n",
            "Loss S1:  0.06812823669422081\n",
            "Loss S2:  0.06714190934955888\n",
            "Loss S1:  0.06806555057220229\n",
            "Loss S2:  0.06709390932699705\n",
            "Loss S1:  0.06810038005851231\n",
            "Loss S2:  0.06718656318016415\n",
            "Loss S1:  0.06807746830531675\n",
            "Loss S2:  0.06719487396060911\n",
            "Loss S1:  0.06804371737666078\n",
            "Loss S2:  0.06721565555626335\n",
            "Loss S1:  0.06802406570700943\n",
            "Loss S2:  0.06719398146088554\n",
            "Loss S1:  0.06797347324374779\n",
            "Loss S2:  0.06717384554778184\n",
            "Loss S1:  0.06785397244917463\n",
            "Loss S2:  0.06710643886262194\n",
            "Loss S1:  0.0678655432625453\n",
            "Loss S2:  0.06715686737264778\n",
            "Loss S1:  0.0678498502050293\n",
            "Loss S2:  0.06710488231367728\n",
            "Loss S1:  0.06786948548643153\n",
            "Loss S2:  0.06712389984245538\n",
            "Loss S1:  0.06787403616751707\n",
            "Loss S2:  0.06714123993109385\n",
            "Loss S1:  0.06787984496375331\n",
            "Loss S2:  0.06712360397116397\n",
            "Loss S1:  0.06789858395568284\n",
            "Loss S2:  0.06714047899597764\n",
            "Loss S1:  0.0678576370721396\n",
            "Loss S2:  0.067104252633071\n",
            "Loss S1:  0.06785279218969578\n",
            "Loss S2:  0.06709629572500848\n",
            "Loss S1:  0.0678094672019789\n",
            "Loss S2:  0.06711193995856198\n",
            "Loss S1:  0.06780130680187657\n",
            "Loss S2:  0.06708897188943175\n",
            "Validation: \n",
            " Loss S1:  1.584568738937378\n",
            " Loss S2:  1.5342499017715454\n",
            " Loss S1:  1.5452567168644495\n",
            " Loss S2:  1.5248030480884371\n",
            " Loss S1:  1.5368927688133427\n",
            " Loss S2:  1.5166581549295566\n",
            " Loss S1:  1.5317803230441984\n",
            " Loss S2:  1.5084050737443517\n",
            " Loss S1:  1.5345237416985593\n",
            " Loss S2:  1.5089254040776947\n",
            "\n",
            "Epoch: 76\n",
            "Loss S1:  0.07403949648141861\n",
            "Loss S2:  0.07009804248809814\n",
            "Loss S1:  0.06683905321088704\n",
            "Loss S2:  0.06700637902725827\n",
            "Loss S1:  0.06679445700276465\n",
            "Loss S2:  0.06599773448847589\n",
            "Loss S1:  0.06685070164742009\n",
            "Loss S2:  0.06549523890979829\n",
            "Loss S1:  0.0675206391549692\n",
            "Loss S2:  0.06602548335383578\n",
            "Loss S1:  0.06734885238841468\n",
            "Loss S2:  0.06559836455419951\n",
            "Loss S1:  0.06767470957558663\n",
            "Loss S2:  0.0663562220383863\n",
            "Loss S1:  0.06810858231824889\n",
            "Loss S2:  0.06682090886252028\n",
            "Loss S1:  0.0680434910327564\n",
            "Loss S2:  0.06708583553080205\n",
            "Loss S1:  0.06798376081572784\n",
            "Loss S2:  0.06710494174570827\n",
            "Loss S1:  0.06776237425090063\n",
            "Loss S2:  0.06703062595264746\n",
            "Loss S1:  0.06756048434757972\n",
            "Loss S2:  0.0667746705380646\n",
            "Loss S1:  0.06765612750506599\n",
            "Loss S2:  0.0669938131612687\n",
            "Loss S1:  0.06767729206280854\n",
            "Loss S2:  0.06693900319683643\n",
            "Loss S1:  0.06762466632516671\n",
            "Loss S2:  0.06680161550852425\n",
            "Loss S1:  0.06772740153188737\n",
            "Loss S2:  0.06680488162088079\n",
            "Loss S1:  0.06768602579369308\n",
            "Loss S2:  0.06685760249354825\n",
            "Loss S1:  0.06767672729509616\n",
            "Loss S2:  0.06702895702151528\n",
            "Loss S1:  0.06771211258202627\n",
            "Loss S2:  0.06708818287151294\n",
            "Loss S1:  0.06772588386507558\n",
            "Loss S2:  0.06706493158693089\n",
            "Loss S1:  0.0677716691065487\n",
            "Loss S2:  0.06698552126166832\n",
            "Loss S1:  0.06784238327312243\n",
            "Loss S2:  0.06696746369461877\n",
            "Loss S1:  0.06780020230158962\n",
            "Loss S2:  0.0669265757420214\n",
            "Loss S1:  0.06785440333671384\n",
            "Loss S2:  0.06688683520341332\n",
            "Loss S1:  0.06791815478586558\n",
            "Loss S2:  0.06697029046930712\n",
            "Loss S1:  0.06787632971230256\n",
            "Loss S2:  0.06696636718048518\n",
            "Loss S1:  0.06792247856314156\n",
            "Loss S2:  0.06695726246954838\n",
            "Loss S1:  0.06796523322423446\n",
            "Loss S2:  0.06691164183825585\n",
            "Loss S1:  0.06808523241152119\n",
            "Loss S2:  0.0669477025703179\n",
            "Loss S1:  0.06811914125719841\n",
            "Loss S2:  0.06691079462567966\n",
            "Loss S1:  0.0680779870785154\n",
            "Loss S2:  0.06680749265161463\n",
            "Loss S1:  0.06805259970367146\n",
            "Loss S2:  0.06683110807486285\n",
            "Loss S1:  0.06808104425250927\n",
            "Loss S2:  0.06684454567614374\n",
            "Loss S1:  0.06800099579319133\n",
            "Loss S2:  0.06686312647879664\n",
            "Loss S1:  0.06807533494276036\n",
            "Loss S2:  0.06694874784682503\n",
            "Loss S1:  0.0680283139382842\n",
            "Loss S2:  0.06697598281197059\n",
            "Loss S1:  0.06808081689262324\n",
            "Loss S2:  0.06699751757105962\n",
            "Loss S1:  0.06805042489720484\n",
            "Loss S2:  0.06701244173867683\n",
            "Loss S1:  0.06805013128974306\n",
            "Loss S2:  0.06701747776837799\n",
            "Loss S1:  0.06801172966123237\n",
            "Loss S2:  0.0669776931062074\n",
            "Loss S1:  0.0680192619041909\n",
            "Loss S2:  0.06698480362718243\n",
            "Loss S1:  0.06795835872962527\n",
            "Loss S2:  0.0669413784251451\n",
            "Loss S1:  0.06791002363532853\n",
            "Loss S2:  0.06691581884182264\n",
            "Loss S1:  0.06791702884394166\n",
            "Loss S2:  0.06693179284820977\n",
            "Loss S1:  0.06790726427453446\n",
            "Loss S2:  0.06690018174667207\n",
            "Loss S1:  0.06793827911273075\n",
            "Loss S2:  0.06688291069012788\n",
            "Loss S1:  0.06789595286266922\n",
            "Loss S2:  0.06684464284919865\n",
            "Loss S1:  0.0678607488165742\n",
            "Loss S2:  0.06685002269164013\n",
            "Loss S1:  0.06786805453270736\n",
            "Loss S2:  0.06688903058393086\n",
            "Loss S1:  0.06783265601502174\n",
            "Loss S2:  0.06682326659074626\n",
            "Validation: \n",
            " Loss S1:  1.5836422443389893\n",
            " Loss S2:  1.5218312740325928\n",
            " Loss S1:  1.5483339230219524\n",
            " Loss S2:  1.523417694228036\n",
            " Loss S1:  1.541038271857471\n",
            " Loss S2:  1.5136409154752406\n",
            " Loss S1:  1.5358268315674828\n",
            " Loss S2:  1.5049353740254388\n",
            " Loss S1:  1.5382885653295635\n",
            " Loss S2:  1.5055090188980103\n",
            "\n",
            "Epoch: 77\n",
            "Loss S1:  0.07388860732316971\n",
            "Loss S2:  0.07805847376585007\n",
            "Loss S1:  0.06808708675883034\n",
            "Loss S2:  0.0678377513858405\n",
            "Loss S1:  0.06710165171396165\n",
            "Loss S2:  0.0666987228961218\n",
            "Loss S1:  0.06689910843007026\n",
            "Loss S2:  0.06647371608884103\n",
            "Loss S1:  0.06684270864579736\n",
            "Loss S2:  0.06672679869140066\n",
            "Loss S1:  0.06671096910448636\n",
            "Loss S2:  0.06617920232169769\n",
            "Loss S1:  0.06724751050599286\n",
            "Loss S2:  0.06650919754241334\n",
            "Loss S1:  0.0673538847813304\n",
            "Loss S2:  0.06667662249274657\n",
            "Loss S1:  0.06722271851735351\n",
            "Loss S2:  0.0665447657674919\n",
            "Loss S1:  0.0673743932211137\n",
            "Loss S2:  0.06653874211913936\n",
            "Loss S1:  0.06708485640511655\n",
            "Loss S2:  0.06643202657451724\n",
            "Loss S1:  0.06691232686107224\n",
            "Loss S2:  0.06614498713532009\n",
            "Loss S1:  0.06670407131930028\n",
            "Loss S2:  0.06621915810118037\n",
            "Loss S1:  0.06686174431602464\n",
            "Loss S2:  0.06627869304582362\n",
            "Loss S1:  0.0666904905480696\n",
            "Loss S2:  0.0662221166417531\n",
            "Loss S1:  0.066659736268173\n",
            "Loss S2:  0.06633000902288796\n",
            "Loss S1:  0.06662690225317612\n",
            "Loss S2:  0.06622266135415676\n",
            "Loss S1:  0.06677691163549647\n",
            "Loss S2:  0.06645067685354523\n",
            "Loss S1:  0.06693343492996627\n",
            "Loss S2:  0.06655447598731978\n",
            "Loss S1:  0.06698464448149291\n",
            "Loss S2:  0.06669126037527753\n",
            "Loss S1:  0.06699416647428896\n",
            "Loss S2:  0.06662118371891144\n",
            "Loss S1:  0.06712385851413152\n",
            "Loss S2:  0.06656668029760862\n",
            "Loss S1:  0.06716172041464176\n",
            "Loss S2:  0.06652178296743474\n",
            "Loss S1:  0.06713167989215293\n",
            "Loss S2:  0.06651038231400701\n",
            "Loss S1:  0.06718055727694539\n",
            "Loss S2:  0.06665275431396556\n",
            "Loss S1:  0.06714544601115098\n",
            "Loss S2:  0.06666206903904083\n",
            "Loss S1:  0.06717495409008187\n",
            "Loss S2:  0.06664936951008336\n",
            "Loss S1:  0.06726543019728468\n",
            "Loss S2:  0.06666724238118563\n",
            "Loss S1:  0.06731186853556022\n",
            "Loss S2:  0.06669253523186433\n",
            "Loss S1:  0.06729162627786296\n",
            "Loss S2:  0.0666970184322485\n",
            "Loss S1:  0.06732026956653674\n",
            "Loss S2:  0.06669799234730461\n",
            "Loss S1:  0.06731597762974129\n",
            "Loss S2:  0.06666804064316766\n",
            "Loss S1:  0.06733830424019852\n",
            "Loss S2:  0.06668127257365304\n",
            "Loss S1:  0.0673008619771083\n",
            "Loss S2:  0.06661907681390598\n",
            "Loss S1:  0.06735751341995605\n",
            "Loss S2:  0.0666765704466975\n",
            "Loss S1:  0.06736793574954031\n",
            "Loss S2:  0.06668622360864596\n",
            "Loss S1:  0.06743925922341294\n",
            "Loss S2:  0.06672869528801158\n",
            "Loss S1:  0.06741300973328297\n",
            "Loss S2:  0.06669938558275809\n",
            "Loss S1:  0.06743023636459991\n",
            "Loss S2:  0.06667181386018363\n",
            "Loss S1:  0.06738764694546495\n",
            "Loss S2:  0.0666295639465532\n",
            "Loss S1:  0.06739580233644071\n",
            "Loss S2:  0.0666530653927243\n",
            "Loss S1:  0.06734218820929527\n",
            "Loss S2:  0.06654244156468234\n",
            "Loss S1:  0.06737137259362429\n",
            "Loss S2:  0.06655275984501328\n",
            "Loss S1:  0.06736046851995096\n",
            "Loss S2:  0.06654943658898989\n",
            "Loss S1:  0.06735074428879485\n",
            "Loss S2:  0.06652424872225644\n",
            "Loss S1:  0.06738359038356667\n",
            "Loss S2:  0.06655850306054177\n",
            "Loss S1:  0.06734636678095754\n",
            "Loss S2:  0.06651988866336951\n",
            "Loss S1:  0.06734696331170967\n",
            "Loss S2:  0.06657035950958855\n",
            "Loss S1:  0.06734659936348762\n",
            "Loss S2:  0.06660750465711536\n",
            "Loss S1:  0.06731830952336249\n",
            "Loss S2:  0.06656863414464317\n",
            "Validation: \n",
            " Loss S1:  1.6085255146026611\n",
            " Loss S2:  1.526175856590271\n",
            " Loss S1:  1.5607712041764032\n",
            " Loss S2:  1.5212878329413277\n",
            " Loss S1:  1.552238412019683\n",
            " Loss S2:  1.5130183144313534\n",
            " Loss S1:  1.547180226591767\n",
            " Loss S2:  1.5044991540127113\n",
            " Loss S1:  1.5493205623862185\n",
            " Loss S2:  1.505670388539632\n",
            "\n",
            "Epoch: 78\n",
            "Loss S1:  0.07027722150087357\n",
            "Loss S2:  0.06848171353340149\n",
            "Loss S1:  0.0675640380518003\n",
            "Loss S2:  0.06691883368925615\n",
            "Loss S1:  0.06609899940944854\n",
            "Loss S2:  0.06515247782781011\n",
            "Loss S1:  0.06623372207245519\n",
            "Loss S2:  0.06533276013308956\n",
            "Loss S1:  0.0667232603016423\n",
            "Loss S2:  0.06587370893940693\n",
            "Loss S1:  0.06661057764408636\n",
            "Loss S2:  0.06575396751948431\n",
            "Loss S1:  0.0670703073627636\n",
            "Loss S2:  0.06620767333957016\n",
            "Loss S1:  0.06703579515008859\n",
            "Loss S2:  0.06662249974381755\n",
            "Loss S1:  0.06700580879861925\n",
            "Loss S2:  0.0668263192989944\n",
            "Loss S1:  0.0670210033409543\n",
            "Loss S2:  0.06686521440253153\n",
            "Loss S1:  0.06680437343397944\n",
            "Loss S2:  0.06669936594691607\n",
            "Loss S1:  0.06671427740707053\n",
            "Loss S2:  0.06641415728105081\n",
            "Loss S1:  0.06674624619281981\n",
            "Loss S2:  0.06638408135160927\n",
            "Loss S1:  0.06694247670301044\n",
            "Loss S2:  0.06636836510578184\n",
            "Loss S1:  0.0669252559648338\n",
            "Loss S2:  0.06622932074551886\n",
            "Loss S1:  0.06701553506863038\n",
            "Loss S2:  0.06631529358345152\n",
            "Loss S1:  0.06681070238444375\n",
            "Loss S2:  0.06623238505598922\n",
            "Loss S1:  0.06684973450344905\n",
            "Loss S2:  0.06625084276174942\n",
            "Loss S1:  0.06698881005302318\n",
            "Loss S2:  0.06645511026952149\n",
            "Loss S1:  0.06702879463268824\n",
            "Loss S2:  0.06644159613947594\n",
            "Loss S1:  0.06706250938980733\n",
            "Loss S2:  0.06645406641770359\n",
            "Loss S1:  0.06714250398049422\n",
            "Loss S2:  0.06657448079066254\n",
            "Loss S1:  0.06723867016275543\n",
            "Loss S2:  0.06658063351905724\n",
            "Loss S1:  0.06722264357911044\n",
            "Loss S2:  0.06656540593221075\n",
            "Loss S1:  0.06727992690697746\n",
            "Loss S2:  0.06663123028221467\n",
            "Loss S1:  0.06731659485643128\n",
            "Loss S2:  0.06662692953331537\n",
            "Loss S1:  0.06740038907619272\n",
            "Loss S2:  0.06662662245932667\n",
            "Loss S1:  0.06736732491487947\n",
            "Loss S2:  0.06653886801147373\n",
            "Loss S1:  0.0674358871483718\n",
            "Loss S2:  0.06659109956747272\n",
            "Loss S1:  0.06741701633608628\n",
            "Loss S2:  0.06655812111148719\n",
            "Loss S1:  0.06739072781365575\n",
            "Loss S2:  0.06660396297558202\n",
            "Loss S1:  0.06738901056852371\n",
            "Loss S2:  0.06670041921341918\n",
            "Loss S1:  0.06737949254681759\n",
            "Loss S2:  0.06669465190153627\n",
            "Loss S1:  0.06733073363941602\n",
            "Loss S2:  0.06663923230010937\n",
            "Loss S1:  0.06744627066808712\n",
            "Loss S2:  0.06668249822475693\n",
            "Loss S1:  0.06742897025837517\n",
            "Loss S2:  0.06664840151102115\n",
            "Loss S1:  0.06749003680830518\n",
            "Loss S2:  0.06666809171850992\n",
            "Loss S1:  0.06749592129030639\n",
            "Loss S2:  0.06663315554433114\n",
            "Loss S1:  0.06746169549273694\n",
            "Loss S2:  0.06662312269289036\n",
            "Loss S1:  0.06744499071060545\n",
            "Loss S2:  0.0665461737138536\n",
            "Loss S1:  0.06746652275509668\n",
            "Loss S2:  0.06655507291344336\n",
            "Loss S1:  0.06741203093978321\n",
            "Loss S2:  0.06650734533286153\n",
            "Loss S1:  0.06742706953275798\n",
            "Loss S2:  0.0664747070848517\n",
            "Loss S1:  0.06741322029010326\n",
            "Loss S2:  0.06646543343631409\n",
            "Loss S1:  0.06734378676430709\n",
            "Loss S2:  0.06640962387969435\n",
            "Loss S1:  0.06741110101639564\n",
            "Loss S2:  0.0664667007234567\n",
            "Loss S1:  0.06736969320737876\n",
            "Loss S2:  0.06643584414249905\n",
            "Loss S1:  0.06736111160043445\n",
            "Loss S2:  0.06647964561409504\n",
            "Loss S1:  0.06734355080586213\n",
            "Loss S2:  0.06647247083004944\n",
            "Loss S1:  0.06730195187229489\n",
            "Loss S2:  0.06640829982770921\n",
            "Validation: \n",
            " Loss S1:  1.598368763923645\n",
            " Loss S2:  1.5285392999649048\n",
            " Loss S1:  1.5668640704382033\n",
            " Loss S2:  1.5251307146889823\n",
            " Loss S1:  1.5596126899486635\n",
            " Loss S2:  1.5166061273435267\n",
            " Loss S1:  1.5548117141254614\n",
            " Loss S2:  1.5074471528412865\n",
            " Loss S1:  1.5577075819910309\n",
            " Loss S2:  1.5082840110048836\n",
            "\n",
            "Epoch: 79\n",
            "Loss S1:  0.07507213950157166\n",
            "Loss S2:  0.07372675836086273\n",
            "Loss S1:  0.067493332719261\n",
            "Loss S2:  0.06674286147410219\n",
            "Loss S1:  0.06686849253518241\n",
            "Loss S2:  0.06570140999697503\n",
            "Loss S1:  0.06744137454417444\n",
            "Loss S2:  0.06574843379278336\n",
            "Loss S1:  0.06778362084452699\n",
            "Loss S2:  0.06640206640813409\n",
            "Loss S1:  0.0672848976271994\n",
            "Loss S2:  0.06612412865255393\n",
            "Loss S1:  0.06742285833251281\n",
            "Loss S2:  0.06625631854670946\n",
            "Loss S1:  0.06729286993053597\n",
            "Loss S2:  0.06651664703664645\n",
            "Loss S1:  0.06726450091342867\n",
            "Loss S2:  0.06657791289466398\n",
            "Loss S1:  0.0671459504372471\n",
            "Loss S2:  0.06669710506940936\n",
            "Loss S1:  0.06689890448262195\n",
            "Loss S2:  0.06654766814248396\n",
            "Loss S1:  0.06667847458172489\n",
            "Loss S2:  0.06612771474294835\n",
            "Loss S1:  0.06642070748219805\n",
            "Loss S2:  0.06602465666153214\n",
            "Loss S1:  0.06654066234144546\n",
            "Loss S2:  0.06612197031046597\n",
            "Loss S1:  0.06658223244949435\n",
            "Loss S2:  0.06607033476127801\n",
            "Loss S1:  0.06659653558340294\n",
            "Loss S2:  0.06606438158955795\n",
            "Loss S1:  0.06654511086689019\n",
            "Loss S2:  0.06610248804740283\n",
            "Loss S1:  0.06669662727249993\n",
            "Loss S2:  0.06617603030556823\n",
            "Loss S1:  0.06675018520398035\n",
            "Loss S2:  0.06622713275294936\n",
            "Loss S1:  0.06669944167839295\n",
            "Loss S2:  0.06625798192479848\n",
            "Loss S1:  0.06673340890463905\n",
            "Loss S2:  0.06620232293854898\n",
            "Loss S1:  0.0668291437541139\n",
            "Loss S2:  0.06616773782960046\n",
            "Loss S1:  0.06685336910154485\n",
            "Loss S2:  0.06616647823026817\n",
            "Loss S1:  0.06689132509745045\n",
            "Loss S2:  0.06617371908901058\n",
            "Loss S1:  0.06696030228588096\n",
            "Loss S2:  0.06618963449389607\n",
            "Loss S1:  0.0669922129922654\n",
            "Loss S2:  0.06617309672542777\n",
            "Loss S1:  0.0670578585959029\n",
            "Loss S2:  0.06626131573970291\n",
            "Loss S1:  0.06716906089624355\n",
            "Loss S2:  0.06631002351091797\n",
            "Loss S1:  0.06726530941992044\n",
            "Loss S2:  0.06641039091868332\n",
            "Loss S1:  0.06715034507855107\n",
            "Loss S2:  0.06631949179104923\n",
            "Loss S1:  0.06708268338560662\n",
            "Loss S2:  0.0663258980996783\n",
            "Loss S1:  0.06711110290607072\n",
            "Loss S2:  0.06637129761350499\n",
            "Loss S1:  0.06711201607670368\n",
            "Loss S2:  0.06635802284440148\n",
            "Loss S1:  0.06706093500433011\n",
            "Loss S2:  0.06633836663471608\n",
            "Loss S1:  0.06722315161590003\n",
            "Loss S2:  0.06644479239965814\n",
            "Loss S1:  0.06716517085193568\n",
            "Loss S2:  0.06642596403460557\n",
            "Loss S1:  0.06722540900606527\n",
            "Loss S2:  0.06648804748900379\n",
            "Loss S1:  0.0672092998445516\n",
            "Loss S2:  0.06645950261312675\n",
            "Loss S1:  0.06720317694849855\n",
            "Loss S2:  0.06640809214138609\n",
            "Loss S1:  0.06716528014682444\n",
            "Loss S2:  0.06634415878587976\n",
            "Loss S1:  0.06717327957104269\n",
            "Loss S2:  0.06636863382082628\n",
            "Loss S1:  0.06714118544891513\n",
            "Loss S2:  0.06635712873906695\n",
            "Loss S1:  0.06714302679245092\n",
            "Loss S2:  0.06638606796107496\n",
            "Loss S1:  0.06719241473659562\n",
            "Loss S2:  0.06641062038784237\n",
            "Loss S1:  0.06716558886290677\n",
            "Loss S2:  0.06637403683192065\n",
            "Loss S1:  0.0672125314959137\n",
            "Loss S2:  0.06636386277967704\n",
            "Loss S1:  0.06716680458853923\n",
            "Loss S2:  0.06630834019106537\n",
            "Loss S1:  0.06715689254667602\n",
            "Loss S2:  0.06632797389969451\n",
            "Loss S1:  0.06718320141618069\n",
            "Loss S2:  0.06634601110325286\n",
            "Loss S1:  0.06713006431483445\n",
            "Loss S2:  0.0662708071851439\n",
            "Validation: \n",
            " Loss S1:  1.578593134880066\n",
            " Loss S2:  1.5139336585998535\n",
            " Loss S1:  1.5423338753836495\n",
            " Loss S2:  1.517997111592974\n",
            " Loss S1:  1.5318582435933554\n",
            " Loss S2:  1.509244517582219\n",
            " Loss S1:  1.5269302794190704\n",
            " Loss S2:  1.500436354856022\n",
            " Loss S1:  1.529267939520471\n",
            " Loss S2:  1.5013798828478213\n",
            "\n",
            "Epoch: 80\n",
            "Loss S1:  0.07290494441986084\n",
            "Loss S2:  0.07247547805309296\n",
            "Loss S1:  0.06642339920455759\n",
            "Loss S2:  0.06706585599617525\n",
            "Loss S1:  0.06535254791378975\n",
            "Loss S2:  0.06535816458719117\n",
            "Loss S1:  0.06606775846692824\n",
            "Loss S2:  0.06609687149044007\n",
            "Loss S1:  0.06709475279217814\n",
            "Loss S2:  0.06613027413443821\n",
            "Loss S1:  0.06687333215685452\n",
            "Loss S2:  0.06610750472720932\n",
            "Loss S1:  0.0671558924263618\n",
            "Loss S2:  0.0664960616802583\n",
            "Loss S1:  0.06704079071190995\n",
            "Loss S2:  0.06671716482706473\n",
            "Loss S1:  0.06720432505747419\n",
            "Loss S2:  0.06691943219414463\n",
            "Loss S1:  0.06683251838926431\n",
            "Loss S2:  0.06673339353157924\n",
            "Loss S1:  0.06654528620662076\n",
            "Loss S2:  0.06675866275730699\n",
            "Loss S1:  0.06634105903071326\n",
            "Loss S2:  0.06635785341128572\n",
            "Loss S1:  0.0662384223470018\n",
            "Loss S2:  0.06640915615745813\n",
            "Loss S1:  0.06610213703554095\n",
            "Loss S2:  0.06655384093978023\n",
            "Loss S1:  0.06603631760317383\n",
            "Loss S2:  0.06642200520381014\n",
            "Loss S1:  0.06600578213172244\n",
            "Loss S2:  0.06643487468657903\n",
            "Loss S1:  0.06590345020619978\n",
            "Loss S2:  0.0663314165073152\n",
            "Loss S1:  0.06602180126117684\n",
            "Loss S2:  0.06632657015794202\n",
            "Loss S1:  0.06616432507887729\n",
            "Loss S2:  0.06638011620354257\n",
            "Loss S1:  0.06620779949489064\n",
            "Loss S2:  0.0663592348665155\n",
            "Loss S1:  0.06621286033916829\n",
            "Loss S2:  0.06633740917441264\n",
            "Loss S1:  0.06627649646164117\n",
            "Loss S2:  0.06636980663261142\n",
            "Loss S1:  0.0664097665391896\n",
            "Loss S2:  0.06632635680056805\n",
            "Loss S1:  0.06640107766825916\n",
            "Loss S2:  0.06627402736833601\n",
            "Loss S1:  0.06642396122031687\n",
            "Loss S2:  0.0663112260988639\n",
            "Loss S1:  0.0663931209044865\n",
            "Loss S2:  0.06624218281284272\n",
            "Loss S1:  0.06651778707559082\n",
            "Loss S2:  0.06626863031152108\n",
            "Loss S1:  0.06650185128639545\n",
            "Loss S2:  0.06622321422579544\n",
            "Loss S1:  0.06659734734330737\n",
            "Loss S2:  0.06630728560834592\n",
            "Loss S1:  0.06654675773431346\n",
            "Loss S2:  0.06625004562059629\n",
            "Loss S1:  0.0665509453123788\n",
            "Loss S2:  0.06626434405133177\n",
            "Loss S1:  0.06655836695883052\n",
            "Loss S2:  0.06625252814777795\n",
            "Loss S1:  0.06661505803373001\n",
            "Loss S2:  0.06625242166894248\n",
            "Loss S1:  0.06656602799352203\n",
            "Loss S2:  0.06624974051541072\n",
            "Loss S1:  0.06674484384199741\n",
            "Loss S2:  0.06636108405604041\n",
            "Loss S1:  0.06682436294492833\n",
            "Loss S2:  0.06641763506027368\n",
            "Loss S1:  0.06685526131386572\n",
            "Loss S2:  0.06647980852965833\n",
            "Loss S1:  0.06692909082834611\n",
            "Loss S2:  0.06650094947002005\n",
            "Loss S1:  0.06696323294612992\n",
            "Loss S2:  0.06649586090969602\n",
            "Loss S1:  0.06695049720080308\n",
            "Loss S2:  0.06644939275844322\n",
            "Loss S1:  0.06702656708564841\n",
            "Loss S2:  0.06645898173500475\n",
            "Loss S1:  0.06698823417480265\n",
            "Loss S2:  0.06636447235579328\n",
            "Loss S1:  0.06697591576468633\n",
            "Loss S2:  0.06633204397814008\n",
            "Loss S1:  0.06694773913052132\n",
            "Loss S2:  0.06630372976897211\n",
            "Loss S1:  0.0669613207379977\n",
            "Loss S2:  0.06628858788010755\n",
            "Loss S1:  0.06701008435297436\n",
            "Loss S2:  0.06633458161796804\n",
            "Loss S1:  0.06693260439564504\n",
            "Loss S2:  0.06626194292061241\n",
            "Loss S1:  0.06689413171063815\n",
            "Loss S2:  0.06621422334682917\n",
            "Loss S1:  0.06690877258560761\n",
            "Loss S2:  0.06621842514607366\n",
            "Loss S1:  0.06687392437804013\n",
            "Loss S2:  0.06618254340411203\n",
            "Validation: \n",
            " Loss S1:  1.5825172662734985\n",
            " Loss S2:  1.5330243110656738\n",
            " Loss S1:  1.5485055616923742\n",
            " Loss S2:  1.5274141970134916\n",
            " Loss S1:  1.5407314474989728\n",
            " Loss S2:  1.5191125375468557\n",
            " Loss S1:  1.5360252349103083\n",
            " Loss S2:  1.5098634430619537\n",
            " Loss S1:  1.538739760716756\n",
            " Loss S2:  1.5111366054158153\n",
            "\n",
            "Epoch: 81\n",
            "Loss S1:  0.07179617136716843\n",
            "Loss S2:  0.06636828929185867\n",
            "Loss S1:  0.06532174314964902\n",
            "Loss S2:  0.06352218782359903\n",
            "Loss S1:  0.06434670551901772\n",
            "Loss S2:  0.06373182666443643\n",
            "Loss S1:  0.06484385423602597\n",
            "Loss S2:  0.06442973979057805\n",
            "Loss S1:  0.06547426368768622\n",
            "Loss S2:  0.0648281756879353\n",
            "Loss S1:  0.06565749827845424\n",
            "Loss S2:  0.06480198799102914\n",
            "Loss S1:  0.06605130837100451\n",
            "Loss S2:  0.0654033997141924\n",
            "Loss S1:  0.06622134170062105\n",
            "Loss S2:  0.06583720335448293\n",
            "Loss S1:  0.0663302694849762\n",
            "Loss S2:  0.06596819706905035\n",
            "Loss S1:  0.06639862998010038\n",
            "Loss S2:  0.06624056832803475\n",
            "Loss S1:  0.06613908519986833\n",
            "Loss S2:  0.06610786000100694\n",
            "Loss S1:  0.06614633991911605\n",
            "Loss S2:  0.06602684911843892\n",
            "Loss S1:  0.06595442022297009\n",
            "Loss S2:  0.06583435956604225\n",
            "Loss S1:  0.06597095156330189\n",
            "Loss S2:  0.0659066972727994\n",
            "Loss S1:  0.06598511636785581\n",
            "Loss S2:  0.0658404251415256\n",
            "Loss S1:  0.06620678900192116\n",
            "Loss S2:  0.065954531068044\n",
            "Loss S1:  0.06609705981353055\n",
            "Loss S2:  0.06586881597404895\n",
            "Loss S1:  0.06624145982296843\n",
            "Loss S2:  0.06591800595933234\n",
            "Loss S1:  0.0664103009136013\n",
            "Loss S2:  0.06609280559733428\n",
            "Loss S1:  0.06652754771459789\n",
            "Loss S2:  0.06624176597017892\n",
            "Loss S1:  0.06647636035262648\n",
            "Loss S2:  0.06616114159647506\n",
            "Loss S1:  0.06648202962573106\n",
            "Loss S2:  0.06606838213041495\n",
            "Loss S1:  0.06656311074802779\n",
            "Loss S2:  0.06610085540430038\n",
            "Loss S1:  0.06657897058503452\n",
            "Loss S2:  0.06600512980253666\n",
            "Loss S1:  0.0666254563141166\n",
            "Loss S2:  0.06613668114627051\n",
            "Loss S1:  0.06654848992468351\n",
            "Loss S2:  0.06614968435935291\n",
            "Loss S1:  0.06665044923052477\n",
            "Loss S2:  0.0661827653532284\n",
            "Loss S1:  0.06664808524765652\n",
            "Loss S2:  0.06615048190544452\n",
            "Loss S1:  0.06668304467699705\n",
            "Loss S2:  0.06615147339279541\n",
            "Loss S1:  0.06669534869568865\n",
            "Loss S2:  0.06610623246564488\n",
            "Loss S1:  0.0666616723510712\n",
            "Loss S2:  0.06607587406554095\n",
            "Loss S1:  0.06667183910966686\n",
            "Loss S2:  0.06611363272335369\n",
            "Loss S1:  0.06669748619755853\n",
            "Loss S2:  0.06607119064789695\n",
            "Loss S1:  0.06667246917133245\n",
            "Loss S2:  0.0660780301356964\n",
            "Loss S1:  0.06681907509079427\n",
            "Loss S2:  0.06619251204812981\n",
            "Loss S1:  0.0668179362142018\n",
            "Loss S2:  0.06615352703847437\n",
            "Loss S1:  0.06684969424417145\n",
            "Loss S2:  0.06613841927365254\n",
            "Loss S1:  0.0668546951544092\n",
            "Loss S2:  0.0661646837413793\n",
            "Loss S1:  0.06678290583995071\n",
            "Loss S2:  0.06612484852277388\n",
            "Loss S1:  0.06677373129960217\n",
            "Loss S2:  0.06611036198675785\n",
            "Loss S1:  0.06684077578478323\n",
            "Loss S2:  0.06614244522113455\n",
            "Loss S1:  0.06678294807382454\n",
            "Loss S2:  0.06610598682523354\n",
            "Loss S1:  0.06678612119653163\n",
            "Loss S2:  0.0660449986715498\n",
            "Loss S1:  0.06678799173325785\n",
            "Loss S2:  0.0660443348235293\n",
            "Loss S1:  0.06678544708829619\n",
            "Loss S2:  0.06601388798921017\n",
            "Loss S1:  0.06682013639325313\n",
            "Loss S2:  0.06607610005431058\n",
            "Loss S1:  0.06677789953001927\n",
            "Loss S2:  0.06605296738752833\n",
            "Loss S1:  0.06679631920725171\n",
            "Loss S2:  0.06610103490151417\n",
            "Loss S1:  0.0667798996839107\n",
            "Loss S2:  0.06612427760372538\n",
            "Loss S1:  0.06675588445028559\n",
            "Loss S2:  0.06604577783047547\n",
            "Validation: \n",
            " Loss S1:  1.583413004875183\n",
            " Loss S2:  1.5350191593170166\n",
            " Loss S1:  1.5560691583724249\n",
            " Loss S2:  1.528748455501738\n",
            " Loss S1:  1.5476977650712176\n",
            " Loss S2:  1.519735377009322\n",
            " Loss S1:  1.5425513767805257\n",
            " Loss S2:  1.510843927743005\n",
            " Loss S1:  1.5448360752176356\n",
            " Loss S2:  1.511628864723959\n",
            "\n",
            "Epoch: 82\n",
            "Loss S1:  0.06579875200986862\n",
            "Loss S2:  0.06499122828245163\n",
            "Loss S1:  0.06550292670726776\n",
            "Loss S2:  0.06426684185862541\n",
            "Loss S1:  0.06547670846893675\n",
            "Loss S2:  0.06497717116560255\n",
            "Loss S1:  0.06561425712800795\n",
            "Loss S2:  0.06504740722237094\n",
            "Loss S1:  0.06591180430316343\n",
            "Loss S2:  0.06566637000296174\n",
            "Loss S1:  0.0659821676272972\n",
            "Loss S2:  0.06559851261622765\n",
            "Loss S1:  0.06661293557921394\n",
            "Loss S2:  0.06605763226503232\n",
            "Loss S1:  0.06653956460281157\n",
            "Loss S2:  0.06624200410196479\n",
            "Loss S1:  0.06650684433586804\n",
            "Loss S2:  0.0662220774885313\n",
            "Loss S1:  0.06652357470203232\n",
            "Loss S2:  0.06633188379007382\n",
            "Loss S1:  0.06628326230710095\n",
            "Loss S2:  0.06624728411731154\n",
            "Loss S1:  0.06614320037198497\n",
            "Loss S2:  0.06616199389100075\n",
            "Loss S1:  0.06612102693516361\n",
            "Loss S2:  0.06616079006805893\n",
            "Loss S1:  0.06622642548825905\n",
            "Loss S2:  0.066194862895112\n",
            "Loss S1:  0.06610261331847374\n",
            "Loss S2:  0.06615039743218862\n",
            "Loss S1:  0.06624060323992312\n",
            "Loss S2:  0.0661209308548479\n",
            "Loss S1:  0.06618067261224948\n",
            "Loss S2:  0.06590869090294246\n",
            "Loss S1:  0.06626052479123512\n",
            "Loss S2:  0.06598402664326784\n",
            "Loss S1:  0.06644149478627832\n",
            "Loss S2:  0.06610228740790272\n",
            "Loss S1:  0.06639987821276275\n",
            "Loss S2:  0.06613879947294116\n",
            "Loss S1:  0.06638998649458387\n",
            "Loss S2:  0.06611347737819401\n",
            "Loss S1:  0.06651955934779905\n",
            "Loss S2:  0.06610489329455588\n",
            "Loss S1:  0.06650605393211227\n",
            "Loss S2:  0.0661059310196212\n",
            "Loss S1:  0.0665599855219389\n",
            "Loss S2:  0.06596575930585592\n",
            "Loss S1:  0.06668664763886405\n",
            "Loss S2:  0.06610893034588747\n",
            "Loss S1:  0.06664806466356692\n",
            "Loss S2:  0.06609967512735333\n",
            "Loss S1:  0.0667466006145395\n",
            "Loss S2:  0.06616851805898422\n",
            "Loss S1:  0.06670570368036573\n",
            "Loss S2:  0.06613967538814704\n",
            "Loss S1:  0.0667316349243142\n",
            "Loss S2:  0.06615489115441398\n",
            "Loss S1:  0.06667380302478768\n",
            "Loss S2:  0.06611295795881052\n",
            "Loss S1:  0.06659325126843199\n",
            "Loss S2:  0.0661027432849439\n",
            "Loss S1:  0.0665862280695768\n",
            "Loss S2:  0.06612395146604136\n",
            "Loss S1:  0.0666442766228569\n",
            "Loss S2:  0.0661661417754454\n",
            "Loss S1:  0.06654753535985947\n",
            "Loss S2:  0.06609227633233157\n",
            "Loss S1:  0.0666718104979859\n",
            "Loss S2:  0.06618061683569486\n",
            "Loss S1:  0.06665165816829076\n",
            "Loss S2:  0.06618150695097073\n",
            "Loss S1:  0.06669963173919106\n",
            "Loss S2:  0.06621195802109063\n",
            "Loss S1:  0.06670592794561322\n",
            "Loss S2:  0.06618647367726117\n",
            "Loss S1:  0.06671512685227268\n",
            "Loss S2:  0.06617832318769665\n",
            "Loss S1:  0.06670377016677272\n",
            "Loss S2:  0.06612927383740845\n",
            "Loss S1:  0.0667700756183289\n",
            "Loss S2:  0.06613309144453515\n",
            "Loss S1:  0.0666794409010097\n",
            "Loss S2:  0.06606409090532583\n",
            "Loss S1:  0.06669727843829969\n",
            "Loss S2:  0.06605242513951949\n",
            "Loss S1:  0.06665502251784929\n",
            "Loss S2:  0.06604517188156564\n",
            "Loss S1:  0.06663413873774394\n",
            "Loss S2:  0.06604806798385655\n",
            "Loss S1:  0.06665173243987323\n",
            "Loss S2:  0.06610670411533898\n",
            "Loss S1:  0.06660062691233164\n",
            "Loss S2:  0.06602325128823713\n",
            "Loss S1:  0.06659571173938976\n",
            "Loss S2:  0.0660096031164794\n",
            "Loss S1:  0.06663422602501827\n",
            "Loss S2:  0.06601150821809214\n",
            "Loss S1:  0.06659878084773688\n",
            "Loss S2:  0.06594675558556608\n",
            "Validation: \n",
            " Loss S1:  1.5768963098526\n",
            " Loss S2:  1.5277618169784546\n",
            " Loss S1:  1.5530572618756975\n",
            " Loss S2:  1.5270223447254725\n",
            " Loss S1:  1.5457526648916848\n",
            " Loss S2:  1.5189822330707456\n",
            " Loss S1:  1.5424048372956574\n",
            " Loss S2:  1.5100776602010257\n",
            " Loss S1:  1.5449481157608975\n",
            " Loss S2:  1.510880156799599\n",
            "\n",
            "Epoch: 83\n",
            "Loss S1:  0.07043246924877167\n",
            "Loss S2:  0.06463507562875748\n",
            "Loss S1:  0.06465366211804477\n",
            "Loss S2:  0.06530312110077251\n",
            "Loss S1:  0.06406106870798838\n",
            "Loss S2:  0.06433545248139472\n",
            "Loss S1:  0.06486200469155465\n",
            "Loss S2:  0.06467007977827903\n",
            "Loss S1:  0.06563432714561136\n",
            "Loss S2:  0.06533418686651601\n",
            "Loss S1:  0.06543117294124529\n",
            "Loss S2:  0.06467681436562071\n",
            "Loss S1:  0.06596368111547876\n",
            "Loss S2:  0.06539324353464314\n",
            "Loss S1:  0.06603643020064058\n",
            "Loss S2:  0.06545001452027911\n",
            "Loss S1:  0.06612203081632838\n",
            "Loss S2:  0.06561146386795574\n",
            "Loss S1:  0.06621601813278355\n",
            "Loss S2:  0.06556309796460383\n",
            "Loss S1:  0.06606313829669858\n",
            "Loss S2:  0.06564526499645544\n",
            "Loss S1:  0.06608045785813718\n",
            "Loss S2:  0.0654873824468604\n",
            "Loss S1:  0.06576385847793138\n",
            "Loss S2:  0.0653394680808891\n",
            "Loss S1:  0.065784614106626\n",
            "Loss S2:  0.06530851242310218\n",
            "Loss S1:  0.06579367089884501\n",
            "Loss S2:  0.0652714729943174\n",
            "Loss S1:  0.06592818657984796\n",
            "Loss S2:  0.065382411444424\n",
            "Loss S1:  0.0659791981303914\n",
            "Loss S2:  0.06539828915681158\n",
            "Loss S1:  0.0661145360671986\n",
            "Loss S2:  0.0655110482976102\n",
            "Loss S1:  0.06611968622187883\n",
            "Loss S2:  0.06560341151462075\n",
            "Loss S1:  0.06613063355859038\n",
            "Loss S2:  0.0656871700474105\n",
            "Loss S1:  0.06614211514888711\n",
            "Loss S2:  0.06558589312938315\n",
            "Loss S1:  0.06617578286765875\n",
            "Loss S2:  0.0656177838513919\n",
            "Loss S1:  0.06623840598364221\n",
            "Loss S2:  0.06559230953589823\n",
            "Loss S1:  0.06620265377354828\n",
            "Loss S2:  0.06555448154801931\n",
            "Loss S1:  0.06619336259748432\n",
            "Loss S2:  0.06564898113430288\n",
            "Loss S1:  0.06609423773222235\n",
            "Loss S2:  0.06563479254445231\n",
            "Loss S1:  0.06628680982808957\n",
            "Loss S2:  0.065691361781852\n",
            "Loss S1:  0.06625581477730916\n",
            "Loss S2:  0.06563012605718581\n",
            "Loss S1:  0.06631979262754586\n",
            "Loss S2:  0.06570912442862775\n",
            "Loss S1:  0.06630934760793787\n",
            "Loss S2:  0.06568729236107512\n",
            "Loss S1:  0.0662506717037521\n",
            "Loss S2:  0.06572582957604002\n",
            "Loss S1:  0.06629099021291426\n",
            "Loss S2:  0.06577022173636596\n",
            "Loss S1:  0.0663205585979227\n",
            "Loss S2:  0.06580118624917072\n",
            "Loss S1:  0.066278348111314\n",
            "Loss S2:  0.06573730054398678\n",
            "Loss S1:  0.0663317508580398\n",
            "Loss S2:  0.06578524672915159\n",
            "Loss S1:  0.06635924483890887\n",
            "Loss S2:  0.06582183630717786\n",
            "Loss S1:  0.06641940546539352\n",
            "Loss S2:  0.065837221442002\n",
            "Loss S1:  0.06644383406221385\n",
            "Loss S2:  0.06583249574963937\n",
            "Loss S1:  0.06640542510730701\n",
            "Loss S2:  0.06577340145827591\n",
            "Loss S1:  0.06636256855124098\n",
            "Loss S2:  0.06572282124701363\n",
            "Loss S1:  0.06645255186536961\n",
            "Loss S2:  0.06575398600458207\n",
            "Loss S1:  0.0664153876023281\n",
            "Loss S2:  0.06569954222680008\n",
            "Loss S1:  0.06644299382127096\n",
            "Loss S2:  0.06569298484192325\n",
            "Loss S1:  0.06643682774458849\n",
            "Loss S2:  0.06568397864215335\n",
            "Loss S1:  0.06640100838139214\n",
            "Loss S2:  0.06561754472435467\n",
            "Loss S1:  0.0664605163285579\n",
            "Loss S2:  0.06564272300003637\n",
            "Loss S1:  0.06639150865456804\n",
            "Loss S2:  0.06562536070830392\n",
            "Loss S1:  0.06634867268826046\n",
            "Loss S2:  0.06561727609857126\n",
            "Loss S1:  0.06637522416371318\n",
            "Loss S2:  0.06562039681290142\n",
            "Loss S1:  0.06634473179271654\n",
            "Loss S2:  0.06552894119089829\n",
            "Validation: \n",
            " Loss S1:  1.5937237739562988\n",
            " Loss S2:  1.5236775875091553\n",
            " Loss S1:  1.557905878339495\n",
            " Loss S2:  1.5172885258992512\n",
            " Loss S1:  1.5507728384762276\n",
            " Loss S2:  1.5098571573815696\n",
            " Loss S1:  1.5464803312645583\n",
            " Loss S2:  1.5012705814642984\n",
            " Loss S1:  1.5495576505307798\n",
            " Loss S2:  1.5024260326668069\n",
            "\n",
            "Epoch: 84\n",
            "Loss S1:  0.07230497151613235\n",
            "Loss S2:  0.07202872633934021\n",
            "Loss S1:  0.06487355753779411\n",
            "Loss S2:  0.0650371177629991\n",
            "Loss S1:  0.06477373661029906\n",
            "Loss S2:  0.06410034125049908\n",
            "Loss S1:  0.06521754880105296\n",
            "Loss S2:  0.06451394793487364\n",
            "Loss S1:  0.06592442667702349\n",
            "Loss S2:  0.06494349631957891\n",
            "Loss S1:  0.06593756443437408\n",
            "Loss S2:  0.06476818141984005\n",
            "Loss S1:  0.06634117094952552\n",
            "Loss S2:  0.06523894048372253\n",
            "Loss S1:  0.06637776459396726\n",
            "Loss S2:  0.06548840883122363\n",
            "Loss S1:  0.06640677945113477\n",
            "Loss S2:  0.06572734477159417\n",
            "Loss S1:  0.06644992649063959\n",
            "Loss S2:  0.06572981682288778\n",
            "Loss S1:  0.06630728002822045\n",
            "Loss S2:  0.0654881017116627\n",
            "Loss S1:  0.06611195934919624\n",
            "Loss S2:  0.0653429572378193\n",
            "Loss S1:  0.06619000373300442\n",
            "Loss S2:  0.06541162322868001\n",
            "Loss S1:  0.06613748278895407\n",
            "Loss S2:  0.06555356523235335\n",
            "Loss S1:  0.06606411151852168\n",
            "Loss S2:  0.06540498097843313\n",
            "Loss S1:  0.06608168238046154\n",
            "Loss S2:  0.0654067425489031\n",
            "Loss S1:  0.0660427629207232\n",
            "Loss S2:  0.065406993609963\n",
            "Loss S1:  0.06609289807795782\n",
            "Loss S2:  0.06540830542295299\n",
            "Loss S1:  0.06619260279019235\n",
            "Loss S2:  0.06546431504066477\n",
            "Loss S1:  0.06622653758806708\n",
            "Loss S2:  0.06552714780363113\n",
            "Loss S1:  0.06628427502527759\n",
            "Loss S2:  0.06544890097198795\n",
            "Loss S1:  0.0662873677768131\n",
            "Loss S2:  0.06553411314272768\n",
            "Loss S1:  0.06632323193941181\n",
            "Loss S2:  0.06548943427892831\n",
            "Loss S1:  0.06637086233857906\n",
            "Loss S2:  0.06546614980284786\n",
            "Loss S1:  0.06646248399902181\n",
            "Loss S2:  0.06547929898399041\n",
            "Loss S1:  0.06649641489305819\n",
            "Loss S2:  0.06542150423939959\n",
            "Loss S1:  0.06653425244478887\n",
            "Loss S2:  0.06545984782135807\n",
            "Loss S1:  0.06664339245410424\n",
            "Loss S2:  0.06544788436091255\n",
            "Loss S1:  0.06663549553563163\n",
            "Loss S2:  0.06549469264906921\n",
            "Loss S1:  0.06662383016288485\n",
            "Loss S2:  0.06541435766158644\n",
            "Loss S1:  0.06651139395478556\n",
            "Loss S2:  0.06540501281074511\n",
            "Loss S1:  0.06647136337887435\n",
            "Loss S2:  0.06540131492246769\n",
            "Loss S1:  0.0665342862430577\n",
            "Loss S2:  0.06545591036385837\n",
            "Loss S1:  0.06644261554108645\n",
            "Loss S2:  0.06543029196492856\n",
            "Loss S1:  0.06652053499108186\n",
            "Loss S2:  0.06551316188227746\n",
            "Loss S1:  0.06651124508886935\n",
            "Loss S2:  0.06556184239621855\n",
            "Loss S1:  0.06655840361052273\n",
            "Loss S2:  0.06559926168244962\n",
            "Loss S1:  0.0665652873622118\n",
            "Loss S2:  0.06557012149546988\n",
            "Loss S1:  0.06655253471899533\n",
            "Loss S2:  0.06555004097188864\n",
            "Loss S1:  0.06649665207699741\n",
            "Loss S2:  0.06551455158521147\n",
            "Loss S1:  0.06651479792676662\n",
            "Loss S2:  0.06551457774312122\n",
            "Loss S1:  0.06646247506322941\n",
            "Loss S2:  0.06546138684716248\n",
            "Loss S1:  0.06644467692689487\n",
            "Loss S2:  0.06547646457218784\n",
            "Loss S1:  0.066427376510234\n",
            "Loss S2:  0.0654773726506189\n",
            "Loss S1:  0.06641562373425955\n",
            "Loss S2:  0.06545904642836847\n",
            "Loss S1:  0.0664321544627657\n",
            "Loss S2:  0.06548407119585246\n",
            "Loss S1:  0.06636067389342894\n",
            "Loss S2:  0.06549081552991898\n",
            "Loss S1:  0.06635382075471737\n",
            "Loss S2:  0.06547243713604924\n",
            "Loss S1:  0.06634039409350209\n",
            "Loss S2:  0.06551634768903132\n",
            "Loss S1:  0.06628676022871449\n",
            "Loss S2:  0.0654559604702195\n",
            "Validation: \n",
            " Loss S1:  1.5711088180541992\n",
            " Loss S2:  1.5328259468078613\n",
            " Loss S1:  1.5459502254213606\n",
            " Loss S2:  1.5281237193516322\n",
            " Loss S1:  1.538271130585089\n",
            " Loss S2:  1.5194840489364252\n",
            " Loss S1:  1.5339448373825824\n",
            " Loss S2:  1.510730444407854\n",
            " Loss S1:  1.5365737308690577\n",
            " Loss S2:  1.5117801913508662\n",
            "\n",
            "Epoch: 85\n",
            "Loss S1:  0.06446640193462372\n",
            "Loss S2:  0.06797858327627182\n",
            "Loss S1:  0.06435070965777744\n",
            "Loss S2:  0.06451579759066756\n",
            "Loss S1:  0.06405219772741907\n",
            "Loss S2:  0.06361639499664307\n",
            "Loss S1:  0.06485182155043848\n",
            "Loss S2:  0.06372002476165371\n",
            "Loss S1:  0.06513751252758794\n",
            "Loss S2:  0.06396198054639304\n",
            "Loss S1:  0.06496510092242092\n",
            "Loss S2:  0.0641035097194653\n",
            "Loss S1:  0.06536354356613315\n",
            "Loss S2:  0.06444877141811808\n",
            "Loss S1:  0.06542963644778224\n",
            "Loss S2:  0.06483031573220038\n",
            "Loss S1:  0.06554576285459378\n",
            "Loss S2:  0.06482966046458409\n",
            "Loss S1:  0.0655181188095402\n",
            "Loss S2:  0.06468529434322001\n",
            "Loss S1:  0.06520144988109569\n",
            "Loss S2:  0.06453895229514282\n",
            "Loss S1:  0.06506645223041913\n",
            "Loss S2:  0.06415482365467527\n",
            "Loss S1:  0.06508648780382369\n",
            "Loss S2:  0.06424466939258182\n",
            "Loss S1:  0.06522957434636036\n",
            "Loss S2:  0.06440686295164451\n",
            "Loss S1:  0.06533245102944948\n",
            "Loss S2:  0.06446092092293373\n",
            "Loss S1:  0.06536527927841572\n",
            "Loss S2:  0.0645028840972493\n",
            "Loss S1:  0.06535574583255727\n",
            "Loss S2:  0.06448580218212945\n",
            "Loss S1:  0.065416522846933\n",
            "Loss S2:  0.06458360614658099\n",
            "Loss S1:  0.06550866357200054\n",
            "Loss S2:  0.06464330226347591\n",
            "Loss S1:  0.06556718915466862\n",
            "Loss S2:  0.0647746145686242\n",
            "Loss S1:  0.06558910694288377\n",
            "Loss S2:  0.0648194492530467\n",
            "Loss S1:  0.06575026620903286\n",
            "Loss S2:  0.06486380898274517\n",
            "Loss S1:  0.06585070784140497\n",
            "Loss S2:  0.06487617682143035\n",
            "Loss S1:  0.06577123304853191\n",
            "Loss S2:  0.0649056862636562\n",
            "Loss S1:  0.06587512475820992\n",
            "Loss S2:  0.06504081712100515\n",
            "Loss S1:  0.0658737869257946\n",
            "Loss S2:  0.06510580420137876\n",
            "Loss S1:  0.06597203826995646\n",
            "Loss S2:  0.06521080928173102\n",
            "Loss S1:  0.06600028192590084\n",
            "Loss S2:  0.06517010065297359\n",
            "Loss S1:  0.06606639836818723\n",
            "Loss S2:  0.06523107610033076\n",
            "Loss S1:  0.06603520011522926\n",
            "Loss S2:  0.06524062769877952\n",
            "Loss S1:  0.06601007483231269\n",
            "Loss S2:  0.06526975950034354\n",
            "Loss S1:  0.0660797209386082\n",
            "Loss S2:  0.06531619463846615\n",
            "Loss S1:  0.06605900269338275\n",
            "Loss S2:  0.06532021639781578\n",
            "Loss S1:  0.06600621089429294\n",
            "Loss S2:  0.06530270623071316\n",
            "Loss S1:  0.06607488881457936\n",
            "Loss S2:  0.06536581715749155\n",
            "Loss S1:  0.06605762226289494\n",
            "Loss S2:  0.0653580510696964\n",
            "Loss S1:  0.06608354183651734\n",
            "Loss S2:  0.06535857757992031\n",
            "Loss S1:  0.06611980001801107\n",
            "Loss S2:  0.0653642681791294\n",
            "Loss S1:  0.06611150386804358\n",
            "Loss S2:  0.06537906748298898\n",
            "Loss S1:  0.06605732005537318\n",
            "Loss S2:  0.06534833920276378\n",
            "Loss S1:  0.0660886413271439\n",
            "Loss S2:  0.06533168267430807\n",
            "Loss S1:  0.06607211528033236\n",
            "Loss S2:  0.06526037987203784\n",
            "Loss S1:  0.06607276280886204\n",
            "Loss S2:  0.06527384104220431\n",
            "Loss S1:  0.06610202965631286\n",
            "Loss S2:  0.06526576126050507\n",
            "Loss S1:  0.0660661931599484\n",
            "Loss S2:  0.06524674841910262\n",
            "Loss S1:  0.06610491201116346\n",
            "Loss S2:  0.0653066265659031\n",
            "Loss S1:  0.06605759326877667\n",
            "Loss S2:  0.06524133089921738\n",
            "Loss S1:  0.06602984208874642\n",
            "Loss S2:  0.06525955800829167\n",
            "Loss S1:  0.066055599305771\n",
            "Loss S2:  0.06527783847927056\n",
            "Loss S1:  0.06602697122971055\n",
            "Loss S2:  0.06522984348252689\n",
            "Validation: \n",
            " Loss S1:  1.5979591608047485\n",
            " Loss S2:  1.525848388671875\n",
            " Loss S1:  1.5605059805370511\n",
            " Loss S2:  1.5200167894363403\n",
            " Loss S1:  1.5525772949544394\n",
            " Loss S2:  1.5099879648627303\n",
            " Loss S1:  1.5480264679330293\n",
            " Loss S2:  1.5011202546416735\n",
            " Loss S1:  1.5503262605196164\n",
            " Loss S2:  1.5023299853007\n",
            "\n",
            "Epoch: 86\n",
            "Loss S1:  0.06910757720470428\n",
            "Loss S2:  0.0695791020989418\n",
            "Loss S1:  0.06558218090371652\n",
            "Loss S2:  0.06381934508681297\n",
            "Loss S1:  0.06482005580550149\n",
            "Loss S2:  0.06306676797214009\n",
            "Loss S1:  0.06569799107889976\n",
            "Loss S2:  0.06386046195703168\n",
            "Loss S1:  0.06662032026343229\n",
            "Loss S2:  0.06478537537339257\n",
            "Loss S1:  0.06634932449635338\n",
            "Loss S2:  0.06499040893772069\n",
            "Loss S1:  0.06632918198821974\n",
            "Loss S2:  0.06512783223488292\n",
            "Loss S1:  0.06649043940952126\n",
            "Loss S2:  0.06535776715043565\n",
            "Loss S1:  0.06668140615026157\n",
            "Loss S2:  0.06557162161226626\n",
            "Loss S1:  0.06645756325387693\n",
            "Loss S2:  0.0654710183268065\n",
            "Loss S1:  0.06612414276540869\n",
            "Loss S2:  0.06528250400972839\n",
            "Loss S1:  0.06591191760323069\n",
            "Loss S2:  0.06513520883950027\n",
            "Loss S1:  0.06574002453241466\n",
            "Loss S2:  0.06500166155828917\n",
            "Loss S1:  0.06557603479000448\n",
            "Loss S2:  0.06500806204462779\n",
            "Loss S1:  0.06548778403629647\n",
            "Loss S2:  0.06485767564769332\n",
            "Loss S1:  0.06562816366454624\n",
            "Loss S2:  0.0649181589089482\n",
            "Loss S1:  0.06560447171145345\n",
            "Loss S2:  0.06490562661834386\n",
            "Loss S1:  0.06571691679327112\n",
            "Loss S2:  0.06495288057982573\n",
            "Loss S1:  0.06570867492266781\n",
            "Loss S2:  0.0650084216085916\n",
            "Loss S1:  0.06573133114042706\n",
            "Loss S2:  0.06514129848146313\n",
            "Loss S1:  0.06571472317573443\n",
            "Loss S2:  0.06517319833461326\n",
            "Loss S1:  0.0658165196495316\n",
            "Loss S2:  0.06514412994492111\n",
            "Loss S1:  0.0659304564793455\n",
            "Loss S2:  0.06514238146432924\n",
            "Loss S1:  0.06591900905747435\n",
            "Loss S2:  0.06515157937164946\n",
            "Loss S1:  0.06605082682306836\n",
            "Loss S2:  0.06523561537945914\n",
            "Loss S1:  0.06605320652405104\n",
            "Loss S2:  0.06524756641323823\n",
            "Loss S1:  0.06612903095239424\n",
            "Loss S2:  0.06532361249928273\n",
            "Loss S1:  0.06609831902978605\n",
            "Loss S2:  0.06525889326340598\n",
            "Loss S1:  0.06611883128559037\n",
            "Loss S2:  0.06524814557933722\n",
            "Loss S1:  0.06609883782841086\n",
            "Loss S2:  0.06521778416746261\n",
            "Loss S1:  0.06605670729843881\n",
            "Loss S2:  0.06513650896046248\n",
            "Loss S1:  0.0660437406474946\n",
            "Loss S2:  0.0652125702026956\n",
            "Loss S1:  0.06602808789022244\n",
            "Loss S2:  0.06526827433863161\n",
            "Loss S1:  0.06598747241398\n",
            "Loss S2:  0.06523199419960875\n",
            "Loss S1:  0.06611919279628135\n",
            "Loss S2:  0.06532743278224447\n",
            "Loss S1:  0.0660823404555477\n",
            "Loss S2:  0.06534272342113688\n",
            "Loss S1:  0.06608528340457219\n",
            "Loss S2:  0.06534370391983074\n",
            "Loss S1:  0.06611564145938084\n",
            "Loss S2:  0.06536390750877298\n",
            "Loss S1:  0.06607247554645763\n",
            "Loss S2:  0.06533440625847481\n",
            "Loss S1:  0.06600136508988907\n",
            "Loss S2:  0.0652735112687511\n",
            "Loss S1:  0.06605662721350901\n",
            "Loss S2:  0.06527308440602331\n",
            "Loss S1:  0.06599694736066236\n",
            "Loss S2:  0.06523321431193618\n",
            "Loss S1:  0.06598097798270737\n",
            "Loss S2:  0.06520815131712412\n",
            "Loss S1:  0.06601211696586587\n",
            "Loss S2:  0.06514294614060295\n",
            "Loss S1:  0.06592722317360156\n",
            "Loss S2:  0.0650688581793757\n",
            "Loss S1:  0.06597176374260444\n",
            "Loss S2:  0.06510389282341278\n",
            "Loss S1:  0.06591237219224266\n",
            "Loss S2:  0.06504708311142736\n",
            "Loss S1:  0.06595826536543557\n",
            "Loss S2:  0.06509931001124108\n",
            "Loss S1:  0.0659900394862754\n",
            "Loss S2:  0.06508772100734364\n",
            "Loss S1:  0.06595150992850908\n",
            "Loss S2:  0.06503058353139281\n",
            "Validation: \n",
            " Loss S1:  1.5935872793197632\n",
            " Loss S2:  1.5385098457336426\n",
            " Loss S1:  1.5613916771752494\n",
            " Loss S2:  1.5263026668911888\n",
            " Loss S1:  1.5541664710858973\n",
            " Loss S2:  1.5177308960658749\n",
            " Loss S1:  1.5499201309485513\n",
            " Loss S2:  1.5093309058517705\n",
            " Loss S1:  1.5526359257874665\n",
            " Loss S2:  1.5103785535435619\n",
            "\n",
            "Epoch: 87\n",
            "Loss S1:  0.06714149564504623\n",
            "Loss S2:  0.07585503160953522\n",
            "Loss S1:  0.06479586186734113\n",
            "Loss S2:  0.06564842232249\n",
            "Loss S1:  0.06427449041179248\n",
            "Loss S2:  0.06449750846340543\n",
            "Loss S1:  0.06477868328652074\n",
            "Loss S2:  0.06458966554172578\n",
            "Loss S1:  0.06517476379507925\n",
            "Loss S2:  0.06511324753121632\n",
            "Loss S1:  0.06514641043602251\n",
            "Loss S2:  0.0649267336463227\n",
            "Loss S1:  0.06562121548369283\n",
            "Loss S2:  0.06537829657069973\n",
            "Loss S1:  0.06591399087452553\n",
            "Loss S2:  0.06544180455761896\n",
            "Loss S1:  0.06610454306190397\n",
            "Loss S2:  0.06551875831720269\n",
            "Loss S1:  0.06611721684317012\n",
            "Loss S2:  0.06557640945027163\n",
            "Loss S1:  0.06588794847968782\n",
            "Loss S2:  0.06540522816600186\n",
            "Loss S1:  0.0655587059152019\n",
            "Loss S2:  0.06512734354347796\n",
            "Loss S1:  0.06539718545049675\n",
            "Loss S2:  0.06510889012951496\n",
            "Loss S1:  0.06532279842796217\n",
            "Loss S2:  0.06514595357971337\n",
            "Loss S1:  0.06520993365569318\n",
            "Loss S2:  0.06495364750107975\n",
            "Loss S1:  0.0651697213483962\n",
            "Loss S2:  0.06489343916540903\n",
            "Loss S1:  0.06497582617383565\n",
            "Loss S2:  0.06486923476646406\n",
            "Loss S1:  0.06511718137135282\n",
            "Loss S2:  0.06496730773595341\n",
            "Loss S1:  0.06537556189282165\n",
            "Loss S2:  0.0650763118958605\n",
            "Loss S1:  0.06535742078854151\n",
            "Loss S2:  0.06502922179655254\n",
            "Loss S1:  0.06538132996998024\n",
            "Loss S2:  0.06509593105064103\n",
            "Loss S1:  0.06541324774094667\n",
            "Loss S2:  0.06508612934658878\n",
            "Loss S1:  0.0654073322014841\n",
            "Loss S2:  0.0650416607568167\n",
            "Loss S1:  0.06541973605622976\n",
            "Loss S2:  0.06496978715642707\n",
            "Loss S1:  0.06551173943021485\n",
            "Loss S2:  0.06507539984101576\n",
            "Loss S1:  0.06553152187233902\n",
            "Loss S2:  0.06513296024673489\n",
            "Loss S1:  0.06559862379141694\n",
            "Loss S2:  0.06518888195184455\n",
            "Loss S1:  0.06563220120883956\n",
            "Loss S2:  0.0651296375807361\n",
            "Loss S1:  0.06562517485266478\n",
            "Loss S2:  0.0651710871805924\n",
            "Loss S1:  0.06561994311940629\n",
            "Loss S2:  0.06511386329007313\n",
            "Loss S1:  0.06561681448769728\n",
            "Loss S2:  0.06515005315063008\n",
            "Loss S1:  0.06557191163301468\n",
            "Loss S2:  0.06514130351867323\n",
            "Loss S1:  0.06558371717081263\n",
            "Loss S2:  0.0651654057274355\n",
            "Loss S1:  0.06556252188687958\n",
            "Loss S2:  0.06515174772128958\n",
            "Loss S1:  0.06566664753509058\n",
            "Loss S2:  0.06524382597190544\n",
            "Loss S1:  0.06567208640850507\n",
            "Loss S2:  0.06524656450858822\n",
            "Loss S1:  0.0657577087736361\n",
            "Loss S2:  0.06523582762380717\n",
            "Loss S1:  0.06579175976047297\n",
            "Loss S2:  0.06522501429738703\n",
            "Loss S1:  0.06573889120005247\n",
            "Loss S2:  0.06520282377408246\n",
            "Loss S1:  0.0656752732708631\n",
            "Loss S2:  0.06518902381896363\n",
            "Loss S1:  0.06572352434297155\n",
            "Loss S2:  0.06517743054507974\n",
            "Loss S1:  0.06568421229466324\n",
            "Loss S2:  0.0651003785993351\n",
            "Loss S1:  0.06567626267742374\n",
            "Loss S2:  0.06508386253673504\n",
            "Loss S1:  0.06563851611554761\n",
            "Loss S2:  0.0650633477677187\n",
            "Loss S1:  0.06564643219653982\n",
            "Loss S2:  0.06502876543944655\n",
            "Loss S1:  0.06571057960060642\n",
            "Loss S2:  0.06506802221789064\n",
            "Loss S1:  0.065686627745693\n",
            "Loss S2:  0.06500173658294171\n",
            "Loss S1:  0.06569508070051037\n",
            "Loss S2:  0.06500387965304077\n",
            "Loss S1:  0.06570081815526292\n",
            "Loss S2:  0.06499782345584922\n",
            "Loss S1:  0.06567308002449344\n",
            "Loss S2:  0.06492857952922765\n",
            "Validation: \n",
            " Loss S1:  1.5767508745193481\n",
            " Loss S2:  1.5307531356811523\n",
            " Loss S1:  1.5459550959723336\n",
            " Loss S2:  1.5227040903908866\n",
            " Loss S1:  1.5385982961189457\n",
            " Loss S2:  1.5142777605754574\n",
            " Loss S1:  1.534154739536223\n",
            " Loss S2:  1.506554476550368\n",
            " Loss S1:  1.536536658251727\n",
            " Loss S2:  1.5080565405480655\n",
            "\n",
            "Epoch: 88\n",
            "Loss S1:  0.06945636868476868\n",
            "Loss S2:  0.06501799076795578\n",
            "Loss S1:  0.06643042916601355\n",
            "Loss S2:  0.06500140509822151\n",
            "Loss S1:  0.06469599636537689\n",
            "Loss S2:  0.0639659824470679\n",
            "Loss S1:  0.064842815120374\n",
            "Loss S2:  0.06396413450279544\n",
            "Loss S1:  0.06492447998465561\n",
            "Loss S2:  0.06420666679013066\n",
            "Loss S1:  0.0646611077382284\n",
            "Loss S2:  0.06381783188850272\n",
            "Loss S1:  0.06531055381552117\n",
            "Loss S2:  0.0643895796698625\n",
            "Loss S1:  0.06549585893960067\n",
            "Loss S2:  0.06455020317938966\n",
            "Loss S1:  0.06553281714886795\n",
            "Loss S2:  0.06464928549564915\n",
            "Loss S1:  0.06546287091223749\n",
            "Loss S2:  0.064688242153152\n",
            "Loss S1:  0.06549305892964401\n",
            "Loss S2:  0.06444985168700171\n",
            "Loss S1:  0.0654766137401263\n",
            "Loss S2:  0.06434062734112009\n",
            "Loss S1:  0.06523242244912573\n",
            "Loss S2:  0.06425001330612119\n",
            "Loss S1:  0.0651354054060146\n",
            "Loss S2:  0.06430794199118177\n",
            "Loss S1:  0.06493955768697651\n",
            "Loss S2:  0.06421230152143655\n",
            "Loss S1:  0.06498413676062957\n",
            "Loss S2:  0.06420321034773296\n",
            "Loss S1:  0.0648057833313942\n",
            "Loss S2:  0.06413558211108172\n",
            "Loss S1:  0.0647690962586138\n",
            "Loss S2:  0.0642362401455815\n",
            "Loss S1:  0.06486555405002273\n",
            "Loss S2:  0.06435737293728148\n",
            "Loss S1:  0.06491632264165978\n",
            "Loss S2:  0.06441204782321815\n",
            "Loss S1:  0.06497292593121529\n",
            "Loss S2:  0.06435733169910327\n",
            "Loss S1:  0.06501163340999051\n",
            "Loss S2:  0.06441391217058869\n",
            "Loss S1:  0.06509990900822354\n",
            "Loss S2:  0.06448695621541722\n",
            "Loss S1:  0.06514122104877001\n",
            "Loss S2:  0.06451874802058394\n",
            "Loss S1:  0.06518872661894783\n",
            "Loss S2:  0.06463893155153856\n",
            "Loss S1:  0.06517926311172337\n",
            "Loss S2:  0.06466814378402623\n",
            "Loss S1:  0.06523270576735565\n",
            "Loss S2:  0.06463807973996433\n",
            "Loss S1:  0.06524808515151928\n",
            "Loss S2:  0.06455931500480183\n",
            "Loss S1:  0.06532184270565196\n",
            "Loss S2:  0.06465149180320658\n",
            "Loss S1:  0.06520938262650647\n",
            "Loss S2:  0.06458177372710812\n",
            "Loss S1:  0.06521001278710524\n",
            "Loss S2:  0.06452441756362931\n",
            "Loss S1:  0.06523467473374302\n",
            "Loss S2:  0.06452219584699229\n",
            "Loss S1:  0.06525805706268531\n",
            "Loss S2:  0.06444548897886201\n",
            "Loss S1:  0.06517794110398638\n",
            "Loss S2:  0.06445097061172353\n",
            "Loss S1:  0.06528607387740241\n",
            "Loss S2:  0.06457094402432093\n",
            "Loss S1:  0.06529493301979497\n",
            "Loss S2:  0.06460509735804337\n",
            "Loss S1:  0.06532772290946044\n",
            "Loss S2:  0.06467751908913213\n",
            "Loss S1:  0.06530076724218552\n",
            "Loss S2:  0.06471671593799745\n",
            "Loss S1:  0.06528160274224332\n",
            "Loss S2:  0.06470042675381571\n",
            "Loss S1:  0.06523984476276067\n",
            "Loss S2:  0.06468898105575606\n",
            "Loss S1:  0.06526792118153965\n",
            "Loss S2:  0.06473435690984167\n",
            "Loss S1:  0.06522708407936305\n",
            "Loss S2:  0.06466483002995342\n",
            "Loss S1:  0.06522154961126433\n",
            "Loss S2:  0.06459531318086627\n",
            "Loss S1:  0.06521081283965133\n",
            "Loss S2:  0.06455804015187819\n",
            "Loss S1:  0.06518691150543371\n",
            "Loss S2:  0.06451041891731643\n",
            "Loss S1:  0.06524000917365176\n",
            "Loss S2:  0.06452610782304254\n",
            "Loss S1:  0.0651771541757594\n",
            "Loss S2:  0.06446421025987562\n",
            "Loss S1:  0.06521747456646017\n",
            "Loss S2:  0.06449194556999864\n",
            "Loss S1:  0.06523193096792376\n",
            "Loss S2:  0.06449406968116017\n",
            "Loss S1:  0.06519124947503482\n",
            "Loss S2:  0.06446200743499696\n",
            "Validation: \n",
            " Loss S1:  1.5944548845291138\n",
            " Loss S2:  1.542901873588562\n",
            " Loss S1:  1.5579323711849393\n",
            " Loss S2:  1.531502473922003\n",
            " Loss S1:  1.551507170607404\n",
            " Loss S2:  1.5232071033338221\n",
            " Loss S1:  1.547598746956372\n",
            " Loss S2:  1.5148691681564832\n",
            " Loss S1:  1.5500139601436662\n",
            " Loss S2:  1.5158435430055783\n",
            "\n",
            "Epoch: 89\n",
            "Loss S1:  0.07341331243515015\n",
            "Loss S2:  0.07037684321403503\n",
            "Loss S1:  0.06546691093932498\n",
            "Loss S2:  0.06525643508542668\n",
            "Loss S1:  0.06490261178641092\n",
            "Loss S2:  0.06357751201306071\n",
            "Loss S1:  0.06558781549815208\n",
            "Loss S2:  0.06344059698524014\n",
            "Loss S1:  0.06558316991460032\n",
            "Loss S2:  0.06357969934257066\n",
            "Loss S1:  0.06526381544330541\n",
            "Loss S2:  0.06324488662329375\n",
            "Loss S1:  0.06572690300765585\n",
            "Loss S2:  0.06368009037658816\n",
            "Loss S1:  0.06562845576816881\n",
            "Loss S2:  0.06386861630098921\n",
            "Loss S1:  0.06564782500083063\n",
            "Loss S2:  0.06384183259473906\n",
            "Loss S1:  0.06569184383356964\n",
            "Loss S2:  0.06395233062269924\n",
            "Loss S1:  0.06562036618885428\n",
            "Loss S2:  0.06381124818679129\n",
            "Loss S1:  0.06540991433031924\n",
            "Loss S2:  0.06369135740238267\n",
            "Loss S1:  0.06554496294457066\n",
            "Loss S2:  0.06366357659874869\n",
            "Loss S1:  0.06560311378299735\n",
            "Loss S2:  0.06375929826998529\n",
            "Loss S1:  0.06541777927930473\n",
            "Loss S2:  0.06374534544475535\n",
            "Loss S1:  0.0654790083205463\n",
            "Loss S2:  0.06386912806539347\n",
            "Loss S1:  0.06546815096036247\n",
            "Loss S2:  0.06387763359346744\n",
            "Loss S1:  0.06540046706359987\n",
            "Loss S2:  0.06392108489982566\n",
            "Loss S1:  0.06540565577577491\n",
            "Loss S2:  0.06401927339026282\n",
            "Loss S1:  0.06533239600468056\n",
            "Loss S2:  0.06393143265503239\n",
            "Loss S1:  0.06535893737973265\n",
            "Loss S2:  0.06396727873688907\n",
            "Loss S1:  0.06539475381091872\n",
            "Loss S2:  0.06393863638571654\n",
            "Loss S1:  0.06542120386774723\n",
            "Loss S2:  0.06389099140377606\n",
            "Loss S1:  0.06539615893583277\n",
            "Loss S2:  0.06388890964192745\n",
            "Loss S1:  0.06550921572380046\n",
            "Loss S2:  0.06398227524893413\n",
            "Loss S1:  0.06550535436467346\n",
            "Loss S2:  0.0640162658317393\n",
            "Loss S1:  0.06558543877583353\n",
            "Loss S2:  0.0641357962513107\n",
            "Loss S1:  0.06554684203764169\n",
            "Loss S2:  0.06411955798409082\n",
            "Loss S1:  0.0655770087714085\n",
            "Loss S2:  0.06420820780443127\n",
            "Loss S1:  0.06552733409240893\n",
            "Loss S2:  0.06423285186188328\n",
            "Loss S1:  0.06548710754048388\n",
            "Loss S2:  0.06420940579072977\n",
            "Loss S1:  0.0654509734254558\n",
            "Loss S2:  0.06422869577237264\n",
            "Loss S1:  0.06545605454767976\n",
            "Loss S2:  0.06422635377892452\n",
            "Loss S1:  0.06533550402414043\n",
            "Loss S2:  0.0642146055679487\n",
            "Loss S1:  0.06541227500153776\n",
            "Loss S2:  0.0641819287386458\n",
            "Loss S1:  0.0654478126611465\n",
            "Loss S2:  0.0642369391126001\n",
            "Loss S1:  0.06554740310293156\n",
            "Loss S2:  0.06428394341774264\n",
            "Loss S1:  0.06553964011592042\n",
            "Loss S2:  0.06428871227483544\n",
            "Loss S1:  0.06546309164033474\n",
            "Loss S2:  0.06428638319643777\n",
            "Loss S1:  0.06541543976996866\n",
            "Loss S2:  0.06425655310225609\n",
            "Loss S1:  0.06548561194814054\n",
            "Loss S2:  0.06429527920120374\n",
            "Loss S1:  0.06545226403722798\n",
            "Loss S2:  0.06425463237829163\n",
            "Loss S1:  0.06542039305113273\n",
            "Loss S2:  0.06424271420635407\n",
            "Loss S1:  0.06546514838277048\n",
            "Loss S2:  0.06425615996468095\n",
            "Loss S1:  0.06544671353805903\n",
            "Loss S2:  0.06424692753795323\n",
            "Loss S1:  0.0654887500943868\n",
            "Loss S2:  0.06430708651664252\n",
            "Loss S1:  0.06547293382941255\n",
            "Loss S2:  0.06425546819986594\n",
            "Loss S1:  0.06545719892704056\n",
            "Loss S2:  0.06429489090595782\n",
            "Loss S1:  0.06544694560821983\n",
            "Loss S2:  0.06430514346568103\n",
            "Loss S1:  0.06539375608874193\n",
            "Loss S2:  0.06421470175480162\n",
            "Validation: \n",
            " Loss S1:  1.59844172000885\n",
            " Loss S2:  1.541678786277771\n",
            " Loss S1:  1.5559581858771188\n",
            " Loss S2:  1.5289758273533411\n",
            " Loss S1:  1.549137903422844\n",
            " Loss S2:  1.5208995225952893\n",
            " Loss S1:  1.5453353573064335\n",
            " Loss S2:  1.513251453149514\n",
            " Loss S1:  1.5479814535305825\n",
            " Loss S2:  1.5146090969627286\n",
            "\n",
            "Epoch: 90\n",
            "Loss S1:  0.06526456773281097\n",
            "Loss S2:  0.06596136093139648\n",
            "Loss S1:  0.06476223401047966\n",
            "Loss S2:  0.06502695720304143\n",
            "Loss S1:  0.0644484715802329\n",
            "Loss S2:  0.06487494778065454\n",
            "Loss S1:  0.06451861199832731\n",
            "Loss S2:  0.06424172306733747\n",
            "Loss S1:  0.06483147193382426\n",
            "Loss S2:  0.06451917703195316\n",
            "Loss S1:  0.06476759340833216\n",
            "Loss S2:  0.06434165934721629\n",
            "Loss S1:  0.06503872664981201\n",
            "Loss S2:  0.06470316569091844\n",
            "Loss S1:  0.06504250302071303\n",
            "Loss S2:  0.06482529010571225\n",
            "Loss S1:  0.06520005076387782\n",
            "Loss S2:  0.06466924274961154\n",
            "Loss S1:  0.06534021020263106\n",
            "Loss S2:  0.06479330962667099\n",
            "Loss S1:  0.06525651077822883\n",
            "Loss S2:  0.0647602304816246\n",
            "Loss S1:  0.06501791025469969\n",
            "Loss S2:  0.0644985981084205\n",
            "Loss S1:  0.06497738036242398\n",
            "Loss S2:  0.06446556146602986\n",
            "Loss S1:  0.06491580975192193\n",
            "Loss S2:  0.06458421850591216\n",
            "Loss S1:  0.0647990567781401\n",
            "Loss S2:  0.06447369121807686\n",
            "Loss S1:  0.06478879894345801\n",
            "Loss S2:  0.06445307097095528\n",
            "Loss S1:  0.0646741980952876\n",
            "Loss S2:  0.06439035381219402\n",
            "Loss S1:  0.0648008521363052\n",
            "Loss S2:  0.06445833125658203\n",
            "Loss S1:  0.06492478330490997\n",
            "Loss S2:  0.06459576782682983\n",
            "Loss S1:  0.06500507161723382\n",
            "Loss S2:  0.06468251604996426\n",
            "Loss S1:  0.0649923162710904\n",
            "Loss S2:  0.06464237632665468\n",
            "Loss S1:  0.06506237641894987\n",
            "Loss S2:  0.06460525102524961\n",
            "Loss S1:  0.065110433256734\n",
            "Loss S2:  0.0645191562256662\n",
            "Loss S1:  0.06506751632535612\n",
            "Loss S2:  0.06445218996826188\n",
            "Loss S1:  0.06507139669970853\n",
            "Loss S2:  0.06452250423458603\n",
            "Loss S1:  0.06505749923774445\n",
            "Loss S2:  0.06448989455027884\n",
            "Loss S1:  0.06509272808430296\n",
            "Loss S2:  0.06451723340656108\n",
            "Loss S1:  0.06518655490281397\n",
            "Loss S2:  0.0645020634375815\n",
            "Loss S1:  0.06519210623623638\n",
            "Loss S2:  0.06448148066356937\n",
            "Loss S1:  0.06517474978035669\n",
            "Loss S2:  0.06447509649334494\n",
            "Loss S1:  0.06510898417660169\n",
            "Loss S2:  0.06443327393048626\n",
            "Loss S1:  0.06506884405898511\n",
            "Loss S2:  0.0644727869983463\n",
            "Loss S1:  0.06507799857966254\n",
            "Loss S2:  0.06448910717280855\n",
            "Loss S1:  0.06501224768711\n",
            "Loss S2:  0.06448923293332319\n",
            "Loss S1:  0.06507071662595894\n",
            "Loss S2:  0.06453573226360632\n",
            "Loss S1:  0.06514975620683103\n",
            "Loss S2:  0.06459182099654125\n",
            "Loss S1:  0.06517434702190336\n",
            "Loss S2:  0.06463256128483201\n",
            "Loss S1:  0.06518724116915962\n",
            "Loss S2:  0.06463509610279229\n",
            "Loss S1:  0.06512645041457624\n",
            "Loss S2:  0.06459789403428243\n",
            "Loss S1:  0.0650984471582848\n",
            "Loss S2:  0.06457457095003494\n",
            "Loss S1:  0.06512600162119937\n",
            "Loss S2:  0.06458506482349072\n",
            "Loss S1:  0.06512429532560988\n",
            "Loss S2:  0.06454215366438648\n",
            "Loss S1:  0.06508901233910948\n",
            "Loss S2:  0.06448050179560791\n",
            "Loss S1:  0.06505635608065433\n",
            "Loss S2:  0.06446192127473116\n",
            "Loss S1:  0.06504322280114741\n",
            "Loss S2:  0.0644358559817819\n",
            "Loss S1:  0.06508696156072775\n",
            "Loss S2:  0.06444053790488687\n",
            "Loss S1:  0.06506810014069468\n",
            "Loss S2:  0.06441991663095491\n",
            "Loss S1:  0.06507007382224826\n",
            "Loss S2:  0.06443455785259826\n",
            "Loss S1:  0.06504906930044882\n",
            "Loss S2:  0.0644231417648138\n",
            "Loss S1:  0.06501212281358947\n",
            "Loss S2:  0.06432974189064654\n",
            "Validation: \n",
            " Loss S1:  1.568934440612793\n",
            " Loss S2:  1.5298707485198975\n",
            " Loss S1:  1.5394369477317447\n",
            " Loss S2:  1.5227001962207614\n",
            " Loss S1:  1.5324455470573612\n",
            " Loss S2:  1.5135631386826678\n",
            " Loss S1:  1.5288672173609499\n",
            " Loss S2:  1.506033664844075\n",
            " Loss S1:  1.5319881189016649\n",
            " Loss S2:  1.5076853772740306\n",
            "\n",
            "Epoch: 91\n",
            "Loss S1:  0.0633103996515274\n",
            "Loss S2:  0.06120235472917557\n",
            "Loss S1:  0.06592172011733055\n",
            "Loss S2:  0.06513088061050935\n",
            "Loss S1:  0.06534251375567346\n",
            "Loss S2:  0.06392066943503562\n",
            "Loss S1:  0.06468672233243142\n",
            "Loss S2:  0.06398269389906237\n",
            "Loss S1:  0.0654308103570124\n",
            "Loss S2:  0.06450607255101204\n",
            "Loss S1:  0.06532559974812994\n",
            "Loss S2:  0.06411270724207747\n",
            "Loss S1:  0.06559730412774399\n",
            "Loss S2:  0.06441047225819259\n",
            "Loss S1:  0.06579281951130277\n",
            "Loss S2:  0.06428847823974113\n",
            "Loss S1:  0.06587024764330299\n",
            "Loss S2:  0.06428827579926562\n",
            "Loss S1:  0.0657121469931943\n",
            "Loss S2:  0.06425225366275389\n",
            "Loss S1:  0.06553472744503824\n",
            "Loss S2:  0.06408878091243234\n",
            "Loss S1:  0.06535233815645312\n",
            "Loss S2:  0.06395928638878169\n",
            "Loss S1:  0.06520315204277512\n",
            "Loss S2:  0.06404080672943888\n",
            "Loss S1:  0.06520954653165723\n",
            "Loss S2:  0.06405028646797624\n",
            "Loss S1:  0.06513447982939423\n",
            "Loss S2:  0.06398641246747463\n",
            "Loss S1:  0.06502780328128512\n",
            "Loss S2:  0.06381226261917329\n",
            "Loss S1:  0.06485378420593575\n",
            "Loss S2:  0.06368099851823002\n",
            "Loss S1:  0.06487929644553285\n",
            "Loss S2:  0.06373076805332947\n",
            "Loss S1:  0.06500147898470499\n",
            "Loss S2:  0.06384545185947946\n",
            "Loss S1:  0.0649785284167497\n",
            "Loss S2:  0.06397402366496506\n",
            "Loss S1:  0.06497620648486697\n",
            "Loss S2:  0.06399092267253506\n",
            "Loss S1:  0.06501063166000831\n",
            "Loss S2:  0.06397251219828547\n",
            "Loss S1:  0.06506984063942509\n",
            "Loss S2:  0.06395030810552485\n",
            "Loss S1:  0.0650545840687824\n",
            "Loss S2:  0.06393994270013524\n",
            "Loss S1:  0.06508656557170188\n",
            "Loss S2:  0.06401076028703159\n",
            "Loss S1:  0.06511372050263017\n",
            "Loss S2:  0.06408595638089921\n",
            "Loss S1:  0.0651780692610704\n",
            "Loss S2:  0.06418770897091577\n",
            "Loss S1:  0.06515892622545637\n",
            "Loss S2:  0.0641700045319061\n",
            "Loss S1:  0.06518141009258206\n",
            "Loss S2:  0.06418760965356199\n",
            "Loss S1:  0.0650876645523658\n",
            "Loss S2:  0.06411843950447348\n",
            "Loss S1:  0.06503370549047112\n",
            "Loss S2:  0.06408181803864102\n",
            "Loss S1:  0.06506866531692133\n",
            "Loss S2:  0.06409428588494993\n",
            "Loss S1:  0.06511258803934694\n",
            "Loss S2:  0.0641029746245558\n",
            "Loss S1:  0.06505593158912443\n",
            "Loss S2:  0.06409358000242098\n",
            "Loss S1:  0.06512554044510263\n",
            "Loss S2:  0.06410466549683176\n",
            "Loss S1:  0.06507063914950077\n",
            "Loss S2:  0.06411993900510321\n",
            "Loss S1:  0.06511859422410295\n",
            "Loss S2:  0.06414977507536761\n",
            "Loss S1:  0.06510915512826565\n",
            "Loss S2:  0.06415958028798797\n",
            "Loss S1:  0.06507921637356125\n",
            "Loss S2:  0.06414046343855971\n",
            "Loss S1:  0.064994529670919\n",
            "Loss S2:  0.06408657747156479\n",
            "Loss S1:  0.06499734691848184\n",
            "Loss S2:  0.06412431259553629\n",
            "Loss S1:  0.06499431785332026\n",
            "Loss S2:  0.06410460327264746\n",
            "Loss S1:  0.06495796043133792\n",
            "Loss S2:  0.0640461370791364\n",
            "Loss S1:  0.06497143350731746\n",
            "Loss S2:  0.06403616184839793\n",
            "Loss S1:  0.06499557799391466\n",
            "Loss S2:  0.06401813456835119\n",
            "Loss S1:  0.0650164263029717\n",
            "Loss S2:  0.06404923307327368\n",
            "Loss S1:  0.06501078353528086\n",
            "Loss S2:  0.06401737225029835\n",
            "Loss S1:  0.06498990933986226\n",
            "Loss S2:  0.06403639464146772\n",
            "Loss S1:  0.06502777112651764\n",
            "Loss S2:  0.06408932873990829\n",
            "Loss S1:  0.06496434199743262\n",
            "Loss S2:  0.06401604032352597\n",
            "Validation: \n",
            " Loss S1:  1.5853314399719238\n",
            " Loss S2:  1.5306408405303955\n",
            " Loss S1:  1.5542176905132474\n",
            " Loss S2:  1.5293101299376715\n",
            " Loss S1:  1.548214284385123\n",
            " Loss S2:  1.5212017210518443\n",
            " Loss S1:  1.5452320868851708\n",
            " Loss S2:  1.5126704348892461\n",
            " Loss S1:  1.5479592438097354\n",
            " Loss S2:  1.513893218688023\n",
            "\n",
            "Epoch: 92\n",
            "Loss S1:  0.06567049771547318\n",
            "Loss S2:  0.0706065446138382\n",
            "Loss S1:  0.06324947015805678\n",
            "Loss S2:  0.06416890228336508\n",
            "Loss S1:  0.0632504437650953\n",
            "Loss S2:  0.06374152873953183\n",
            "Loss S1:  0.06354736969355614\n",
            "Loss S2:  0.06365953686256562\n",
            "Loss S1:  0.06443900387825036\n",
            "Loss S2:  0.06377626446689047\n",
            "Loss S1:  0.0643123394864447\n",
            "Loss S2:  0.06354013801205392\n",
            "Loss S1:  0.0649202709681675\n",
            "Loss S2:  0.06378905120931688\n",
            "Loss S1:  0.06492656626751725\n",
            "Loss S2:  0.06384431402867949\n",
            "Loss S1:  0.06506457796067368\n",
            "Loss S2:  0.06393883266934643\n",
            "Loss S1:  0.06493426969909406\n",
            "Loss S2:  0.06393039672748073\n",
            "Loss S1:  0.06476280222287273\n",
            "Loss S2:  0.06381790291997466\n",
            "Loss S1:  0.06457005541872333\n",
            "Loss S2:  0.063625104557555\n",
            "Loss S1:  0.06450378482253098\n",
            "Loss S2:  0.06361731908414975\n",
            "Loss S1:  0.06457117489958537\n",
            "Loss S2:  0.06380157485490537\n",
            "Loss S1:  0.06468183935639706\n",
            "Loss S2:  0.06381999875636811\n",
            "Loss S1:  0.0647446337026476\n",
            "Loss S2:  0.0639056810578763\n",
            "Loss S1:  0.06464323440786474\n",
            "Loss S2:  0.06390455071322666\n",
            "Loss S1:  0.06474103782957757\n",
            "Loss S2:  0.06397039282043078\n",
            "Loss S1:  0.06486555635517473\n",
            "Loss S2:  0.0640614344969968\n",
            "Loss S1:  0.06482303177655055\n",
            "Loss S2:  0.06412795227001475\n",
            "Loss S1:  0.06475132575898028\n",
            "Loss S2:  0.06407664147256618\n",
            "Loss S1:  0.06477683874385617\n",
            "Loss S2:  0.06406826589486045\n",
            "Loss S1:  0.06480203536908012\n",
            "Loss S2:  0.0640490362547102\n",
            "Loss S1:  0.06477970652508014\n",
            "Loss S2:  0.06399459433762027\n",
            "Loss S1:  0.06486533324068018\n",
            "Loss S2:  0.06402905252897394\n",
            "Loss S1:  0.06488290743761328\n",
            "Loss S2:  0.06396954378996238\n",
            "Loss S1:  0.06504174259088048\n",
            "Loss S2:  0.06408617686894205\n",
            "Loss S1:  0.06507381121885733\n",
            "Loss S2:  0.06404093916895645\n",
            "Loss S1:  0.06504627611501361\n",
            "Loss S2:  0.06404247527338856\n",
            "Loss S1:  0.06502174408114243\n",
            "Loss S2:  0.06401661367402044\n",
            "Loss S1:  0.06499629768471782\n",
            "Loss S2:  0.06400878696495116\n",
            "Loss S1:  0.06498704647040444\n",
            "Loss S2:  0.0640740208687123\n",
            "Loss S1:  0.06500161236088224\n",
            "Loss S2:  0.0640590146903494\n",
            "Loss S1:  0.06502381555758574\n",
            "Loss S2:  0.06412187639590117\n",
            "Loss S1:  0.06512872744733987\n",
            "Loss S2:  0.06413415394439376\n",
            "Loss S1:  0.06515343117917705\n",
            "Loss S2:  0.06411384144782001\n",
            "Loss S1:  0.06515556254999459\n",
            "Loss S2:  0.06414643344869243\n",
            "Loss S1:  0.06512533750816818\n",
            "Loss S2:  0.06415671189478787\n",
            "Loss S1:  0.06506428387495164\n",
            "Loss S2:  0.06415559699529112\n",
            "Loss S1:  0.06504195740879953\n",
            "Loss S2:  0.06417260189419208\n",
            "Loss S1:  0.0650609364440465\n",
            "Loss S2:  0.06419473007655797\n",
            "Loss S1:  0.06503556257254306\n",
            "Loss S2:  0.06414595853600769\n",
            "Loss S1:  0.06504067496314751\n",
            "Loss S2:  0.0641312394866326\n",
            "Loss S1:  0.06503249882891117\n",
            "Loss S2:  0.06408311220063134\n",
            "Loss S1:  0.06502609574943442\n",
            "Loss S2:  0.06405171137852193\n",
            "Loss S1:  0.06505627389955944\n",
            "Loss S2:  0.06408581970271937\n",
            "Loss S1:  0.06501903823716262\n",
            "Loss S2:  0.06403355394164807\n",
            "Loss S1:  0.0650011159305613\n",
            "Loss S2:  0.06398743306407251\n",
            "Loss S1:  0.06498179165743245\n",
            "Loss S2:  0.06398417322805419\n",
            "Loss S1:  0.06495296857993131\n",
            "Loss S2:  0.06399955401716796\n",
            "Validation: \n",
            " Loss S1:  1.5776805877685547\n",
            " Loss S2:  1.529826283454895\n",
            " Loss S1:  1.5534814369110834\n",
            " Loss S2:  1.530045060884385\n",
            " Loss S1:  1.545326119515954\n",
            " Loss S2:  1.5206808724054477\n",
            " Loss S1:  1.5415700341834397\n",
            " Loss S2:  1.5121611982095438\n",
            " Loss S1:  1.5433833496070202\n",
            " Loss S2:  1.5134781157528912\n",
            "\n",
            "Epoch: 93\n",
            "Loss S1:  0.07112358510494232\n",
            "Loss S2:  0.06675487011671066\n",
            "Loss S1:  0.06450151042504744\n",
            "Loss S2:  0.06563281674276698\n",
            "Loss S1:  0.06319380533837136\n",
            "Loss S2:  0.06505367177582923\n",
            "Loss S1:  0.06351536981040432\n",
            "Loss S2:  0.06464725985161719\n",
            "Loss S1:  0.0642237840447484\n",
            "Loss S2:  0.0646222611752952\n",
            "Loss S1:  0.06402313241771623\n",
            "Loss S2:  0.064140638153927\n",
            "Loss S1:  0.06411300416364045\n",
            "Loss S2:  0.06470042919037772\n",
            "Loss S1:  0.06412416219081678\n",
            "Loss S2:  0.06477634251957208\n",
            "Loss S1:  0.06445111541284455\n",
            "Loss S2:  0.06474961528991476\n",
            "Loss S1:  0.06436956063895435\n",
            "Loss S2:  0.06485810081709872\n",
            "Loss S1:  0.06416705512616894\n",
            "Loss S2:  0.06453866426749985\n",
            "Loss S1:  0.06403128097991685\n",
            "Loss S2:  0.06440858042857668\n",
            "Loss S1:  0.06403608703293091\n",
            "Loss S2:  0.0643051966778503\n",
            "Loss S1:  0.06403384724532375\n",
            "Loss S2:  0.06422274477267993\n",
            "Loss S1:  0.06409591856472036\n",
            "Loss S2:  0.0640562246454523\n",
            "Loss S1:  0.06402256778928618\n",
            "Loss S2:  0.06395982240406883\n",
            "Loss S1:  0.06394237217418156\n",
            "Loss S2:  0.06390097885398391\n",
            "Loss S1:  0.06408565994678882\n",
            "Loss S2:  0.06394730634682359\n",
            "Loss S1:  0.06431836293217885\n",
            "Loss S2:  0.06406059058496306\n",
            "Loss S1:  0.06437425481165267\n",
            "Loss S2:  0.06415473422064831\n",
            "Loss S1:  0.06436784357870397\n",
            "Loss S2:  0.06404662006233462\n",
            "Loss S1:  0.06442046024223075\n",
            "Loss S2:  0.06407843260050385\n",
            "Loss S1:  0.06448600692379529\n",
            "Loss S2:  0.06402249214538622\n",
            "Loss S1:  0.06439803710038011\n",
            "Loss S2:  0.06395860342107294\n",
            "Loss S1:  0.0644569531763243\n",
            "Loss S2:  0.06398633093074645\n",
            "Loss S1:  0.064404044211267\n",
            "Loss S2:  0.0639672890246152\n",
            "Loss S1:  0.06451332296505286\n",
            "Loss S2:  0.06410333838213907\n",
            "Loss S1:  0.06443829241988844\n",
            "Loss S2:  0.06403913166085293\n",
            "Loss S1:  0.06448969325826262\n",
            "Loss S2:  0.06410556351533989\n",
            "Loss S1:  0.06443933259273313\n",
            "Loss S2:  0.06411027791829862\n",
            "Loss S1:  0.06444192289117959\n",
            "Loss S2:  0.06411710879038339\n",
            "Loss S1:  0.06447943987570391\n",
            "Loss S2:  0.0641643184078468\n",
            "Loss S1:  0.06452693603352594\n",
            "Loss S2:  0.06418082930402964\n",
            "Loss S1:  0.06446039454949587\n",
            "Loss S2:  0.06417040428951427\n",
            "Loss S1:  0.06457239997920053\n",
            "Loss S2:  0.06424821923074485\n",
            "Loss S1:  0.06456252575939538\n",
            "Loss S2:  0.06425089710670659\n",
            "Loss S1:  0.06455556458053166\n",
            "Loss S2:  0.06424948897140509\n",
            "Loss S1:  0.06456762383568962\n",
            "Loss S2:  0.06425504703727372\n",
            "Loss S1:  0.0645789851647193\n",
            "Loss S2:  0.06425385114558413\n",
            "Loss S1:  0.06450233194033814\n",
            "Loss S2:  0.06420831782433688\n",
            "Loss S1:  0.06455144109645687\n",
            "Loss S2:  0.06424861080814478\n",
            "Loss S1:  0.06456598404731484\n",
            "Loss S2:  0.06421902776670863\n",
            "Loss S1:  0.0645749405238804\n",
            "Loss S2:  0.06420680543412222\n",
            "Loss S1:  0.06457153388802524\n",
            "Loss S2:  0.06419401236414356\n",
            "Loss S1:  0.06455538179429751\n",
            "Loss S2:  0.06414921219352962\n",
            "Loss S1:  0.06458238883980626\n",
            "Loss S2:  0.06414587469461752\n",
            "Loss S1:  0.06454409115064946\n",
            "Loss S2:  0.06410596587588109\n",
            "Loss S1:  0.06454936570770675\n",
            "Loss S2:  0.06412399756364255\n",
            "Loss S1:  0.06455132799714866\n",
            "Loss S2:  0.06417076097580598\n",
            "Loss S1:  0.06452367490426829\n",
            "Loss S2:  0.064136598737201\n",
            "Validation: \n",
            " Loss S1:  1.5780019760131836\n",
            " Loss S2:  1.5372695922851562\n",
            " Loss S1:  1.5498039779208956\n",
            " Loss S2:  1.5278506392524356\n",
            " Loss S1:  1.542776017654233\n",
            " Loss S2:  1.5192289497794174\n",
            " Loss S1:  1.5385875057001583\n",
            " Loss S2:  1.5108320732585718\n",
            " Loss S1:  1.5410492817560832\n",
            " Loss S2:  1.5119748674793008\n",
            "\n",
            "Epoch: 94\n",
            "Loss S1:  0.06258224695920944\n",
            "Loss S2:  0.06478362530469894\n",
            "Loss S1:  0.06169679490002719\n",
            "Loss S2:  0.06288883191618053\n",
            "Loss S1:  0.0628088368546395\n",
            "Loss S2:  0.06293858516783941\n",
            "Loss S1:  0.06349404372515217\n",
            "Loss S2:  0.06327818766716987\n",
            "Loss S1:  0.06377784735182436\n",
            "Loss S2:  0.06356766656404589\n",
            "Loss S1:  0.06303227575970631\n",
            "Loss S2:  0.06354543544790324\n",
            "Loss S1:  0.06331132792058539\n",
            "Loss S2:  0.06392936970366807\n",
            "Loss S1:  0.06351415130873801\n",
            "Loss S2:  0.0641255124249089\n",
            "Loss S1:  0.06352291559731518\n",
            "Loss S2:  0.06390892396922465\n",
            "Loss S1:  0.0635901696898125\n",
            "Loss S2:  0.063994802657392\n",
            "Loss S1:  0.06341510745558408\n",
            "Loss S2:  0.06381048844887478\n",
            "Loss S1:  0.06338060080065383\n",
            "Loss S2:  0.06356491296140997\n",
            "Loss S1:  0.06320014272716419\n",
            "Loss S2:  0.06339562373343578\n",
            "Loss S1:  0.06320833443230345\n",
            "Loss S2:  0.06336319025691228\n",
            "Loss S1:  0.06319906599555455\n",
            "Loss S2:  0.06319916932612446\n",
            "Loss S1:  0.06328522734689397\n",
            "Loss S2:  0.06317413761126285\n",
            "Loss S1:  0.06331943988985156\n",
            "Loss S2:  0.06317715106173331\n",
            "Loss S1:  0.06347781380540446\n",
            "Loss S2:  0.06337702086968729\n",
            "Loss S1:  0.06361850027962285\n",
            "Loss S2:  0.06355633601052327\n",
            "Loss S1:  0.06367390437041902\n",
            "Loss S2:  0.06357437930025979\n",
            "Loss S1:  0.06378507688270872\n",
            "Loss S2:  0.0635670532486332\n",
            "Loss S1:  0.06390339335629726\n",
            "Loss S2:  0.0636700564968925\n",
            "Loss S1:  0.0641034390794206\n",
            "Loss S2:  0.06364675425835864\n",
            "Loss S1:  0.06403253362937407\n",
            "Loss S2:  0.06356281688783592\n",
            "Loss S1:  0.06410909300337689\n",
            "Loss S2:  0.06363076974803976\n",
            "Loss S1:  0.06408710540290848\n",
            "Loss S2:  0.06366754057575982\n",
            "Loss S1:  0.06419584319223846\n",
            "Loss S2:  0.06376104817593692\n",
            "Loss S1:  0.06426320400955052\n",
            "Loss S2:  0.06383518832031211\n",
            "Loss S1:  0.06428207769936939\n",
            "Loss S2:  0.0638500326097648\n",
            "Loss S1:  0.06424761078355648\n",
            "Loss S2:  0.06377603792140574\n",
            "Loss S1:  0.06418934756537212\n",
            "Loss S2:  0.06372063995595786\n",
            "Loss S1:  0.06421946644351796\n",
            "Loss S2:  0.0637474267644706\n",
            "Loss S1:  0.06420172963326222\n",
            "Loss S2:  0.06372229743319509\n",
            "Loss S1:  0.06417780469686603\n",
            "Loss S2:  0.06364605717608576\n",
            "Loss S1:  0.06427419942713553\n",
            "Loss S2:  0.0637129270894961\n",
            "Loss S1:  0.06427205045126443\n",
            "Loss S2:  0.06372065310422172\n",
            "Loss S1:  0.06434282474735767\n",
            "Loss S2:  0.06376811293394942\n",
            "Loss S1:  0.06427996676927307\n",
            "Loss S2:  0.06375285165730833\n",
            "Loss S1:  0.06424008607082167\n",
            "Loss S2:  0.06378657677824416\n",
            "Loss S1:  0.06422380497083639\n",
            "Loss S2:  0.06373620614447557\n",
            "Loss S1:  0.06429131080087581\n",
            "Loss S2:  0.0637640704649345\n",
            "Loss S1:  0.06426672785205273\n",
            "Loss S2:  0.06371600886947337\n",
            "Loss S1:  0.06431003086569756\n",
            "Loss S2:  0.06371166685379316\n",
            "Loss S1:  0.06435561405706572\n",
            "Loss S2:  0.06372059611149841\n",
            "Loss S1:  0.06438254946821671\n",
            "Loss S2:  0.06374247618813633\n",
            "Loss S1:  0.06441076905229139\n",
            "Loss S2:  0.06373868262325053\n",
            "Loss S1:  0.0644107167881766\n",
            "Loss S2:  0.06368481703183397\n",
            "Loss S1:  0.06439970379928621\n",
            "Loss S2:  0.06371232006706756\n",
            "Loss S1:  0.06439822232215171\n",
            "Loss S2:  0.06371235234078151\n",
            "Loss S1:  0.06434366365708791\n",
            "Loss S2:  0.06366529775491314\n",
            "Validation: \n",
            " Loss S1:  1.5697487592697144\n",
            " Loss S2:  1.5293537378311157\n",
            " Loss S1:  1.5474790561766851\n",
            " Loss S2:  1.526634352547782\n",
            " Loss S1:  1.540832618387734\n",
            " Loss S2:  1.5172054186099913\n",
            " Loss S1:  1.537044204649378\n",
            " Loss S2:  1.508902536063898\n",
            " Loss S1:  1.539617202900074\n",
            " Loss S2:  1.5102585127324233\n",
            "\n",
            "Epoch: 95\n",
            "Loss S1:  0.06894225627183914\n",
            "Loss S2:  0.06895771622657776\n",
            "Loss S1:  0.06469293548302217\n",
            "Loss S2:  0.06505728513002396\n",
            "Loss S1:  0.06385111524945214\n",
            "Loss S2:  0.06422801777010873\n",
            "Loss S1:  0.06396740182272849\n",
            "Loss S2:  0.0641304659506967\n",
            "Loss S1:  0.06450272933012102\n",
            "Loss S2:  0.06422518520820432\n",
            "Loss S1:  0.06438426220533895\n",
            "Loss S2:  0.06399516235379611\n",
            "Loss S1:  0.06447275495920025\n",
            "Loss S2:  0.0640320346736517\n",
            "Loss S1:  0.06461808090688477\n",
            "Loss S2:  0.06391656078713041\n",
            "Loss S1:  0.06448505846438585\n",
            "Loss S2:  0.06394472129550981\n",
            "Loss S1:  0.06443779059982561\n",
            "Loss S2:  0.06383694790221833\n",
            "Loss S1:  0.06411407109681923\n",
            "Loss S2:  0.06344614159500245\n",
            "Loss S1:  0.06389295296357558\n",
            "Loss S2:  0.06323165953293577\n",
            "Loss S1:  0.0637882281064002\n",
            "Loss S2:  0.06323894424256214\n",
            "Loss S1:  0.06375705180618599\n",
            "Loss S2:  0.06330593740780845\n",
            "Loss S1:  0.06363115568004601\n",
            "Loss S2:  0.0630741352677768\n",
            "Loss S1:  0.06375008224454147\n",
            "Loss S2:  0.06312451151427843\n",
            "Loss S1:  0.06378056377357577\n",
            "Loss S2:  0.0631235958487721\n",
            "Loss S1:  0.06378794372168897\n",
            "Loss S2:  0.06319862473429295\n",
            "Loss S1:  0.06392334920266715\n",
            "Loss S2:  0.06324096583859037\n",
            "Loss S1:  0.06400508568158948\n",
            "Loss S2:  0.06330241994317913\n",
            "Loss S1:  0.06401891848860096\n",
            "Loss S2:  0.06322930556772953\n",
            "Loss S1:  0.0639960143838731\n",
            "Loss S2:  0.06327924755582877\n",
            "Loss S1:  0.06407533972039482\n",
            "Loss S2:  0.063288429288438\n",
            "Loss S1:  0.06404066498661454\n",
            "Loss S2:  0.06335403850842349\n",
            "Loss S1:  0.06404266967071043\n",
            "Loss S2:  0.06346366032030573\n",
            "Loss S1:  0.06404146945393419\n",
            "Loss S2:  0.06346432860213447\n",
            "Loss S1:  0.06408713011506417\n",
            "Loss S2:  0.06358796468011721\n",
            "Loss S1:  0.06411792950986496\n",
            "Loss S2:  0.06363080782863927\n",
            "Loss S1:  0.06412965417226438\n",
            "Loss S2:  0.06361936898471197\n",
            "Loss S1:  0.06409966242835693\n",
            "Loss S2:  0.06359647754643791\n",
            "Loss S1:  0.06405346119077103\n",
            "Loss S2:  0.06364368902290382\n",
            "Loss S1:  0.0640440394089728\n",
            "Loss S2:  0.06368409831472148\n",
            "Loss S1:  0.06409913271294204\n",
            "Loss S2:  0.0637384726784868\n",
            "Loss S1:  0.06406184835106224\n",
            "Loss S2:  0.06368528287351312\n",
            "Loss S1:  0.06412136236812013\n",
            "Loss S2:  0.06370402552471482\n",
            "Loss S1:  0.06418681223253239\n",
            "Loss S2:  0.06372127244700054\n",
            "Loss S1:  0.06426748045180973\n",
            "Loss S2:  0.06374531677844121\n",
            "Loss S1:  0.06422432635551836\n",
            "Loss S2:  0.06370822741898564\n",
            "Loss S1:  0.06417900895861191\n",
            "Loss S2:  0.06367242322584463\n",
            "Loss S1:  0.06412140743049514\n",
            "Loss S2:  0.0636219479277006\n",
            "Loss S1:  0.0641436067937021\n",
            "Loss S2:  0.06366662875263768\n",
            "Loss S1:  0.06409518033664882\n",
            "Loss S2:  0.06360484898960503\n",
            "Loss S1:  0.0640635341582842\n",
            "Loss S2:  0.06354994251616211\n",
            "Loss S1:  0.06406553407694514\n",
            "Loss S2:  0.06349370155803288\n",
            "Loss S1:  0.06403185966975834\n",
            "Loss S2:  0.06349006749672684\n",
            "Loss S1:  0.06407530748692161\n",
            "Loss S2:  0.06355661422245518\n",
            "Loss S1:  0.0640645602358396\n",
            "Loss S2:  0.06353335703951418\n",
            "Loss S1:  0.06411059421433765\n",
            "Loss S2:  0.06356738401762746\n",
            "Loss S1:  0.06413621442264678\n",
            "Loss S2:  0.06359108615658883\n",
            "Loss S1:  0.06409670980059202\n",
            "Loss S2:  0.0635354846806973\n",
            "Validation: \n",
            " Loss S1:  1.5644367933273315\n",
            " Loss S2:  1.530066967010498\n",
            " Loss S1:  1.5507928019478208\n",
            " Loss S2:  1.5232790822074527\n",
            " Loss S1:  1.5435621098774235\n",
            " Loss S2:  1.5152146729027354\n",
            " Loss S1:  1.5399722603500867\n",
            " Loss S2:  1.506993049480876\n",
            " Loss S1:  1.5422921298462668\n",
            " Loss S2:  1.5078333954752228\n",
            "\n",
            "Epoch: 96\n",
            "Loss S1:  0.0709637925028801\n",
            "Loss S2:  0.06976146250963211\n",
            "Loss S1:  0.06350624832240018\n",
            "Loss S2:  0.06357562880624425\n",
            "Loss S1:  0.06312780561191696\n",
            "Loss S2:  0.06220211514404842\n",
            "Loss S1:  0.06303127061936163\n",
            "Loss S2:  0.06225153683654724\n",
            "Loss S1:  0.06371962778815408\n",
            "Loss S2:  0.06259606860396338\n",
            "Loss S1:  0.06372070648506575\n",
            "Loss S2:  0.06255400012813363\n",
            "Loss S1:  0.06421041891711657\n",
            "Loss S2:  0.06305781935082107\n",
            "Loss S1:  0.06391917410450922\n",
            "Loss S2:  0.06301409703954844\n",
            "Loss S1:  0.06402671245145208\n",
            "Loss S2:  0.06316662556779237\n",
            "Loss S1:  0.06398846049393926\n",
            "Loss S2:  0.06318297838935485\n",
            "Loss S1:  0.06392140273410495\n",
            "Loss S2:  0.06301097084980199\n",
            "Loss S1:  0.06376312867746697\n",
            "Loss S2:  0.06275013971838865\n",
            "Loss S1:  0.06378247511904102\n",
            "Loss S2:  0.0627513793874378\n",
            "Loss S1:  0.06381383023530472\n",
            "Loss S2:  0.06270878554414247\n",
            "Loss S1:  0.06372372607918496\n",
            "Loss S2:  0.06273673842033596\n",
            "Loss S1:  0.06364920602136889\n",
            "Loss S2:  0.06276874029577173\n",
            "Loss S1:  0.06364307858003593\n",
            "Loss S2:  0.06263065430688562\n",
            "Loss S1:  0.06371749872178362\n",
            "Loss S2:  0.06269100591627477\n",
            "Loss S1:  0.06367145369642348\n",
            "Loss S2:  0.06274210877757705\n",
            "Loss S1:  0.06368743341552649\n",
            "Loss S2:  0.06283100803641124\n",
            "Loss S1:  0.06364409710785643\n",
            "Loss S2:  0.0628682622565559\n",
            "Loss S1:  0.06368849749601847\n",
            "Loss S2:  0.0629402891119227\n",
            "Loss S1:  0.06371772036428365\n",
            "Loss S2:  0.06287235973984408\n",
            "Loss S1:  0.06378839316564205\n",
            "Loss S2:  0.06289632012059679\n",
            "Loss S1:  0.06388123466389803\n",
            "Loss S2:  0.06296956908591556\n",
            "Loss S1:  0.06385248854934457\n",
            "Loss S2:  0.06298137794571569\n",
            "Loss S1:  0.06390509497502754\n",
            "Loss S2:  0.06303017101182791\n",
            "Loss S1:  0.06388841808612056\n",
            "Loss S2:  0.06301114640827549\n",
            "Loss S1:  0.06395362625130555\n",
            "Loss S2:  0.06306743943977611\n",
            "Loss S1:  0.06387467093484099\n",
            "Loss S2:  0.06302767526038323\n",
            "Loss S1:  0.0638835315168696\n",
            "Loss S2:  0.06298276367427107\n",
            "Loss S1:  0.06383726642829429\n",
            "Loss S2:  0.0630513244934404\n",
            "Loss S1:  0.0638805821166603\n",
            "Loss S2:  0.06298485488813614\n",
            "Loss S1:  0.06382639358366005\n",
            "Loss S2:  0.06290881546197341\n",
            "Loss S1:  0.06391852907290207\n",
            "Loss S2:  0.06297493290123352\n",
            "Loss S1:  0.06394684764734361\n",
            "Loss S2:  0.06304165752779724\n",
            "Loss S1:  0.06397027580113952\n",
            "Loss S2:  0.06304474933449582\n",
            "Loss S1:  0.06401199812315544\n",
            "Loss S2:  0.06302618839830723\n",
            "Loss S1:  0.06398649326968067\n",
            "Loss S2:  0.0630262325129171\n",
            "Loss S1:  0.06395836765199062\n",
            "Loss S2:  0.06301277157519479\n",
            "Loss S1:  0.06398889689968708\n",
            "Loss S2:  0.06304013499632441\n",
            "Loss S1:  0.06400271654672866\n",
            "Loss S2:  0.06302266753756797\n",
            "Loss S1:  0.06403869756822632\n",
            "Loss S2:  0.06302469671480446\n",
            "Loss S1:  0.06401012538826382\n",
            "Loss S2:  0.06297788518349146\n",
            "Loss S1:  0.06402582152326361\n",
            "Loss S2:  0.06293518980648243\n",
            "Loss S1:  0.0640470767249157\n",
            "Loss S2:  0.06298720338557354\n",
            "Loss S1:  0.06400452436244927\n",
            "Loss S2:  0.06295304855335819\n",
            "Loss S1:  0.06398608200963896\n",
            "Loss S2:  0.06297630960647721\n",
            "Loss S1:  0.06398859735493105\n",
            "Loss S2:  0.06300909652252702\n",
            "Loss S1:  0.06397475385678034\n",
            "Loss S2:  0.06296332255612809\n",
            "Validation: \n",
            " Loss S1:  1.5877611637115479\n",
            " Loss S2:  1.525891900062561\n",
            " Loss S1:  1.5537866694586617\n",
            " Loss S2:  1.5236426932471139\n",
            " Loss S1:  1.5475086322644862\n",
            " Loss S2:  1.5144621017502575\n",
            " Loss S1:  1.543723874404782\n",
            " Loss S2:  1.506525149110888\n",
            " Loss S1:  1.5465317140390844\n",
            " Loss S2:  1.5080754138805248\n",
            "\n",
            "Epoch: 97\n",
            "Loss S1:  0.06238162890076637\n",
            "Loss S2:  0.06467816978693008\n",
            "Loss S1:  0.06306153976104477\n",
            "Loss S2:  0.06252390755848451\n",
            "Loss S1:  0.06320341303944588\n",
            "Loss S2:  0.062277709799153466\n",
            "Loss S1:  0.06336238836088488\n",
            "Loss S2:  0.06246053511577268\n",
            "Loss S1:  0.06414954164406149\n",
            "Loss S2:  0.06282213502904264\n",
            "Loss S1:  0.06364872408848182\n",
            "Loss S2:  0.06240686292157454\n",
            "Loss S1:  0.06402455502357639\n",
            "Loss S2:  0.06280641611970839\n",
            "Loss S1:  0.0638049417503283\n",
            "Loss S2:  0.06295633693815957\n",
            "Loss S1:  0.06408477712560583\n",
            "Loss S2:  0.06310817047401711\n",
            "Loss S1:  0.06410875425233946\n",
            "Loss S2:  0.06308106091487539\n",
            "Loss S1:  0.06392569592831158\n",
            "Loss S2:  0.06303684048280858\n",
            "Loss S1:  0.06383091882542447\n",
            "Loss S2:  0.0627128113698852\n",
            "Loss S1:  0.06387334301575157\n",
            "Loss S2:  0.0627745727369608\n",
            "Loss S1:  0.063937212987483\n",
            "Loss S2:  0.06280734057303604\n",
            "Loss S1:  0.06389078775301893\n",
            "Loss S2:  0.06273116144939517\n",
            "Loss S1:  0.06389955231389463\n",
            "Loss S2:  0.06276355587587451\n",
            "Loss S1:  0.06370104786864719\n",
            "Loss S2:  0.06267199720840276\n",
            "Loss S1:  0.06375972846010972\n",
            "Loss S2:  0.06274582434728829\n",
            "Loss S1:  0.06398593208325502\n",
            "Loss S2:  0.06280178313261896\n",
            "Loss S1:  0.0640401514685903\n",
            "Loss S2:  0.06294921122452352\n",
            "Loss S1:  0.06402949708981893\n",
            "Loss S2:  0.06292031521895039\n",
            "Loss S1:  0.06410164110609705\n",
            "Loss S2:  0.06294005455987714\n",
            "Loss S1:  0.06407193274856693\n",
            "Loss S2:  0.06288496827984827\n",
            "Loss S1:  0.0640809302464192\n",
            "Loss S2:  0.06286717265908852\n",
            "Loss S1:  0.06416654871212496\n",
            "Loss S2:  0.06285726994708861\n",
            "Loss S1:  0.06413555045823652\n",
            "Loss S2:  0.06289399243089783\n",
            "Loss S1:  0.06419232095167098\n",
            "Loss S2:  0.06295896791121512\n",
            "Loss S1:  0.06420791857082025\n",
            "Loss S2:  0.06293603425976095\n",
            "Loss S1:  0.06424264332736938\n",
            "Loss S2:  0.06297959404536838\n",
            "Loss S1:  0.06420267501340289\n",
            "Loss S2:  0.06300253878088341\n",
            "Loss S1:  0.06410099401376969\n",
            "Loss S2:  0.06293292547826751\n",
            "Loss S1:  0.06408517678355097\n",
            "Loss S2:  0.06298205137875686\n",
            "Loss S1:  0.06417888700033646\n",
            "Loss S2:  0.06301312347021058\n",
            "Loss S1:  0.06413307290783102\n",
            "Loss S2:  0.06301380047895397\n",
            "Loss S1:  0.0641472491154398\n",
            "Loss S2:  0.06304669570276115\n",
            "Loss S1:  0.06419510690447612\n",
            "Loss S2:  0.0630969255419708\n",
            "Loss S1:  0.06425348410274513\n",
            "Loss S2:  0.06316644706636915\n",
            "Loss S1:  0.06421359667599683\n",
            "Loss S2:  0.06315625034212423\n",
            "Loss S1:  0.06419577173830017\n",
            "Loss S2:  0.06314610844365567\n",
            "Loss S1:  0.06421105403576971\n",
            "Loss S2:  0.0631177217587638\n",
            "Loss S1:  0.06420387000663025\n",
            "Loss S2:  0.06316238684472895\n",
            "Loss S1:  0.06412315333302873\n",
            "Loss S2:  0.06310553861432992\n",
            "Loss S1:  0.06411960030819627\n",
            "Loss S2:  0.06312000582485754\n",
            "Loss S1:  0.06410430025162388\n",
            "Loss S2:  0.063111514916207\n",
            "Loss S1:  0.06411513079658927\n",
            "Loss S2:  0.06306347196875246\n",
            "Loss S1:  0.06417564398499656\n",
            "Loss S2:  0.06310334322589993\n",
            "Loss S1:  0.06412715064826342\n",
            "Loss S2:  0.06304968958983194\n",
            "Loss S1:  0.06414286335119523\n",
            "Loss S2:  0.06309285884102185\n",
            "Loss S1:  0.06411956187933993\n",
            "Loss S2:  0.06309776090999403\n",
            "Loss S1:  0.06407690234102931\n",
            "Loss S2:  0.06304026483852121\n",
            "Validation: \n",
            " Loss S1:  1.5853432416915894\n",
            " Loss S2:  1.5229878425598145\n",
            " Loss S1:  1.5569665148144676\n",
            " Loss S2:  1.5181218385696411\n",
            " Loss S1:  1.5492194890975952\n",
            " Loss S2:  1.5090493777903116\n",
            " Loss S1:  1.5448887055037452\n",
            " Loss S2:  1.5015598105602577\n",
            " Loss S1:  1.5482788512736192\n",
            " Loss S2:  1.5029334124223686\n",
            "\n",
            "Epoch: 98\n",
            "Loss S1:  0.06217364966869354\n",
            "Loss S2:  0.06621117144823074\n",
            "Loss S1:  0.06352864104238423\n",
            "Loss S2:  0.06348759579387578\n",
            "Loss S1:  0.06286104768514633\n",
            "Loss S2:  0.0627765121559302\n",
            "Loss S1:  0.06311595235620776\n",
            "Loss S2:  0.06267955502675425\n",
            "Loss S1:  0.06346415346715509\n",
            "Loss S2:  0.06319062130116834\n",
            "Loss S1:  0.063482579647326\n",
            "Loss S2:  0.06317802606260076\n",
            "Loss S1:  0.06368897012511238\n",
            "Loss S2:  0.06319998968087259\n",
            "Loss S1:  0.06381813363290169\n",
            "Loss S2:  0.06338005072214234\n",
            "Loss S1:  0.06380501568869308\n",
            "Loss S2:  0.06340695512883457\n",
            "Loss S1:  0.06382032623002817\n",
            "Loss S2:  0.06324653130966229\n",
            "Loss S1:  0.06366324837845151\n",
            "Loss S2:  0.06311169735128337\n",
            "Loss S1:  0.06356966968726467\n",
            "Loss S2:  0.062872185967527\n",
            "Loss S1:  0.06333577808392934\n",
            "Loss S2:  0.06282446053156182\n",
            "Loss S1:  0.06344278079976562\n",
            "Loss S2:  0.06288447504052679\n",
            "Loss S1:  0.06351398433565247\n",
            "Loss S2:  0.06288561375534281\n",
            "Loss S1:  0.06345427241053013\n",
            "Loss S2:  0.06287925650939247\n",
            "Loss S1:  0.06345357308569162\n",
            "Loss S2:  0.0629428245173478\n",
            "Loss S1:  0.06356222138941636\n",
            "Loss S2:  0.06300890561171442\n",
            "Loss S1:  0.06364505861510229\n",
            "Loss S2:  0.06315416953840308\n",
            "Loss S1:  0.06367849529335635\n",
            "Loss S2:  0.06315588791176911\n",
            "Loss S1:  0.06371679436878779\n",
            "Loss S2:  0.06312495245788227\n",
            "Loss S1:  0.06382606595189651\n",
            "Loss S2:  0.06311122947747673\n",
            "Loss S1:  0.0638143981878574\n",
            "Loss S2:  0.0630431472749462\n",
            "Loss S1:  0.06379695567282247\n",
            "Loss S2:  0.06296076906191839\n",
            "Loss S1:  0.06386607497374051\n",
            "Loss S2:  0.06303577033872426\n",
            "Loss S1:  0.0638648971202839\n",
            "Loss S2:  0.06305495303762386\n",
            "Loss S1:  0.06392435910772547\n",
            "Loss S2:  0.06307751349962078\n",
            "Loss S1:  0.06392044749976963\n",
            "Loss S2:  0.06296040198844738\n",
            "Loss S1:  0.06396037499251314\n",
            "Loss S2:  0.0630657352586658\n",
            "Loss S1:  0.06393297553011232\n",
            "Loss S2:  0.06303315025936697\n",
            "Loss S1:  0.06389746022877899\n",
            "Loss S2:  0.06303916344313923\n",
            "Loss S1:  0.06387826421828131\n",
            "Loss S2:  0.06307939768220834\n",
            "Loss S1:  0.06393240682339743\n",
            "Loss S2:  0.06307300546923159\n",
            "Loss S1:  0.06391015485649383\n",
            "Loss S2:  0.06301769234937843\n",
            "Loss S1:  0.06393936258956484\n",
            "Loss S2:  0.06306822133684788\n",
            "Loss S1:  0.06398230961967058\n",
            "Loss S2:  0.06307882906874361\n",
            "Loss S1:  0.06404019462930198\n",
            "Loss S2:  0.06309372804790653\n",
            "Loss S1:  0.06404967796850719\n",
            "Loss S2:  0.06309150037860292\n",
            "Loss S1:  0.06399510847771261\n",
            "Loss S2:  0.06302339930742432\n",
            "Loss S1:  0.06393778675695515\n",
            "Loss S2:  0.06297042066483852\n",
            "Loss S1:  0.06392633570288482\n",
            "Loss S2:  0.06296375074291467\n",
            "Loss S1:  0.06391672545323407\n",
            "Loss S2:  0.06291590856700918\n",
            "Loss S1:  0.06393342041310958\n",
            "Loss S2:  0.06288590043287662\n",
            "Loss S1:  0.06391064536578417\n",
            "Loss S2:  0.06283992120084242\n",
            "Loss S1:  0.06386056853259772\n",
            "Loss S2:  0.06281256053993761\n",
            "Loss S1:  0.0639266889137598\n",
            "Loss S2:  0.062869950187312\n",
            "Loss S1:  0.06388095013975063\n",
            "Loss S2:  0.06286223913157064\n",
            "Loss S1:  0.06384735827328293\n",
            "Loss S2:  0.06287471727802242\n",
            "Loss S1:  0.06385883674985901\n",
            "Loss S2:  0.0628879963809512\n",
            "Loss S1:  0.06382368882082387\n",
            "Loss S2:  0.06284060650135252\n",
            "Validation: \n",
            " Loss S1:  1.5749950408935547\n",
            " Loss S2:  1.5150508880615234\n",
            " Loss S1:  1.552465518315633\n",
            " Loss S2:  1.511671469325111\n",
            " Loss S1:  1.5456032026104811\n",
            " Loss S2:  1.5027549034211694\n",
            " Loss S1:  1.5416100709164728\n",
            " Loss S2:  1.494611744020806\n",
            " Loss S1:  1.5451876042801658\n",
            " Loss S2:  1.4954997345253274\n",
            "\n",
            "Epoch: 99\n",
            "Loss S1:  0.06546132266521454\n",
            "Loss S2:  0.06792975962162018\n",
            "Loss S1:  0.06319541721181436\n",
            "Loss S2:  0.06370742111043497\n",
            "Loss S1:  0.06230048853017035\n",
            "Loss S2:  0.06223467435865175\n",
            "Loss S1:  0.06307227717291924\n",
            "Loss S2:  0.062426029794639154\n",
            "Loss S1:  0.06327536911135767\n",
            "Loss S2:  0.06266694706751079\n",
            "Loss S1:  0.06290735194788259\n",
            "Loss S2:  0.06206375462751763\n",
            "Loss S1:  0.06337072083451709\n",
            "Loss S2:  0.06261845580378517\n",
            "Loss S1:  0.06357134448390611\n",
            "Loss S2:  0.06275804718615303\n",
            "Loss S1:  0.06373637036224943\n",
            "Loss S2:  0.06297712547727573\n",
            "Loss S1:  0.0636731326006926\n",
            "Loss S2:  0.06291811479316962\n",
            "Loss S1:  0.06340244419798993\n",
            "Loss S2:  0.06289063803482764\n",
            "Loss S1:  0.06339418780696285\n",
            "Loss S2:  0.06262732951624973\n",
            "Loss S1:  0.06344004735843209\n",
            "Loss S2:  0.06267735919307087\n",
            "Loss S1:  0.06343800850383198\n",
            "Loss S2:  0.06269366868579661\n",
            "Loss S1:  0.06339537526698823\n",
            "Loss S2:  0.06273808544303508\n",
            "Loss S1:  0.06331771219901691\n",
            "Loss S2:  0.0627871494222161\n",
            "Loss S1:  0.06311757071232944\n",
            "Loss S2:  0.0627324672543114\n",
            "Loss S1:  0.06312046879739092\n",
            "Loss S2:  0.06282842322661165\n",
            "Loss S1:  0.06318726691093234\n",
            "Loss S2:  0.06290335831803512\n",
            "Loss S1:  0.06328760684316695\n",
            "Loss S2:  0.06309910284365035\n",
            "Loss S1:  0.0633973354909254\n",
            "Loss S2:  0.0631787934854849\n",
            "Loss S1:  0.06346538099703065\n",
            "Loss S2:  0.06322545558214188\n",
            "Loss S1:  0.06357120918067871\n",
            "Loss S2:  0.06320570776770018\n",
            "Loss S1:  0.06356105243875868\n",
            "Loss S2:  0.0632357806838178\n",
            "Loss S1:  0.06360234082116131\n",
            "Loss S2:  0.0632649979310659\n",
            "Loss S1:  0.06362530992800496\n",
            "Loss S2:  0.06332851213169288\n",
            "Loss S1:  0.06370190128512766\n",
            "Loss S2:  0.06340531458171848\n",
            "Loss S1:  0.063670125284758\n",
            "Loss S2:  0.06336378377632021\n",
            "Loss S1:  0.06369370110730684\n",
            "Loss S2:  0.06339806694789289\n",
            "Loss S1:  0.0636344323658042\n",
            "Loss S2:  0.06330415822847192\n",
            "Loss S1:  0.06361064287614189\n",
            "Loss S2:  0.06327678603537851\n",
            "Loss S1:  0.06360858214054843\n",
            "Loss S2:  0.06328248487743535\n",
            "Loss S1:  0.06358014866477604\n",
            "Loss S2:  0.06322913318640347\n",
            "Loss S1:  0.06354805597545878\n",
            "Loss S2:  0.06323343313937821\n",
            "Loss S1:  0.06369084929309982\n",
            "Loss S2:  0.06332359106834333\n",
            "Loss S1:  0.06373605148023011\n",
            "Loss S2:  0.0633582189424425\n",
            "Loss S1:  0.06377944289358368\n",
            "Loss S2:  0.06335294279695548\n",
            "Loss S1:  0.06379274010778758\n",
            "Loss S2:  0.06336380845573392\n",
            "Loss S1:  0.06373922645146139\n",
            "Loss S2:  0.06333716756363553\n",
            "Loss S1:  0.06365389823723022\n",
            "Loss S2:  0.0632460826955488\n",
            "Loss S1:  0.06372358479962087\n",
            "Loss S2:  0.06328319188699758\n",
            "Loss S1:  0.06371372114241558\n",
            "Loss S2:  0.06322559170479322\n",
            "Loss S1:  0.06371925545595321\n",
            "Loss S2:  0.06319932393605805\n",
            "Loss S1:  0.06373833632420775\n",
            "Loss S2:  0.06316323536869545\n",
            "Loss S1:  0.06375166994272446\n",
            "Loss S2:  0.06314488691267242\n",
            "Loss S1:  0.06380472805541\n",
            "Loss S2:  0.06317196039693583\n",
            "Loss S1:  0.06375088830379498\n",
            "Loss S2:  0.06311525065863986\n",
            "Loss S1:  0.06376698205045833\n",
            "Loss S2:  0.06318427274308133\n",
            "Loss S1:  0.06380563582489247\n",
            "Loss S2:  0.06320136565552432\n",
            "Loss S1:  0.06378869985490127\n",
            "Loss S2:  0.06315908156896803\n",
            "Validation: \n",
            " Loss S1:  1.5947319269180298\n",
            " Loss S2:  1.5361464023590088\n",
            " Loss S1:  1.5578566562561762\n",
            " Loss S2:  1.5263889119738625\n",
            " Loss S1:  1.5512351088407563\n",
            " Loss S2:  1.5174066002775983\n",
            " Loss S1:  1.5461938947927756\n",
            " Loss S2:  1.5098178034923115\n",
            " Loss S1:  1.54942512659379\n",
            " Loss S2:  1.5111496948901517\n",
            "\n",
            "Epoch: 100\n",
            "Loss S1:  0.06830710172653198\n",
            "Loss S2:  0.06147688627243042\n",
            "Loss S1:  0.064347447658127\n",
            "Loss S2:  0.06305660849267786\n",
            "Loss S1:  0.06308638712479955\n",
            "Loss S2:  0.06166865463767733\n",
            "Loss S1:  0.06335919903170678\n",
            "Loss S2:  0.06141738809885517\n",
            "Loss S1:  0.06381309931961501\n",
            "Loss S2:  0.06185287582438167\n",
            "Loss S1:  0.06355106385022986\n",
            "Loss S2:  0.06160793782157056\n",
            "Loss S1:  0.06396551949323201\n",
            "Loss S2:  0.062286382205173615\n",
            "Loss S1:  0.06419161318893164\n",
            "Loss S2:  0.06272609678792282\n",
            "Loss S1:  0.06405835304363275\n",
            "Loss S2:  0.06288507084051768\n",
            "Loss S1:  0.06379363305129848\n",
            "Loss S2:  0.06306439093672313\n",
            "Loss S1:  0.06352223854253788\n",
            "Loss S2:  0.06289940401174055\n",
            "Loss S1:  0.06340654117164311\n",
            "Loss S2:  0.06267635742420549\n",
            "Loss S1:  0.06332422318783673\n",
            "Loss S2:  0.06272611616194741\n",
            "Loss S1:  0.06330669696890671\n",
            "Loss S2:  0.06297886621861057\n",
            "Loss S1:  0.06335022469572987\n",
            "Loss S2:  0.0629690705884433\n",
            "Loss S1:  0.06332741593880369\n",
            "Loss S2:  0.06288343445946838\n",
            "Loss S1:  0.06327520391863325\n",
            "Loss S2:  0.06272964126297406\n",
            "Loss S1:  0.06335003545496896\n",
            "Loss S2:  0.06278657754175147\n",
            "Loss S1:  0.06349461826171664\n",
            "Loss S2:  0.06297963295769955\n",
            "Loss S1:  0.06348076306712565\n",
            "Loss S2:  0.06309821796042757\n",
            "Loss S1:  0.06352062439963\n",
            "Loss S2:  0.06305740134262328\n",
            "Loss S1:  0.06358883326864355\n",
            "Loss S2:  0.0631701921540979\n",
            "Loss S1:  0.06369504844980542\n",
            "Loss S2:  0.06314547326230356\n",
            "Loss S1:  0.06358405391310716\n",
            "Loss S2:  0.06306911713052622\n",
            "Loss S1:  0.0636600846757038\n",
            "Loss S2:  0.06314850502736341\n",
            "Loss S1:  0.06355808227066975\n",
            "Loss S2:  0.06307896716423718\n",
            "Loss S1:  0.0636657015688118\n",
            "Loss S2:  0.06313515374601116\n",
            "Loss S1:  0.06369948065478863\n",
            "Loss S2:  0.06306789831921623\n",
            "Loss S1:  0.06368630228305626\n",
            "Loss S2:  0.06316029414322452\n",
            "Loss S1:  0.06360396974531236\n",
            "Loss S2:  0.063058636788138\n",
            "Loss S1:  0.06353549509497972\n",
            "Loss S2:  0.06302077752658695\n",
            "Loss S1:  0.0635088554005531\n",
            "Loss S2:  0.06304161070914897\n",
            "Loss S1:  0.06357672782701866\n",
            "Loss S2:  0.06306570750948425\n",
            "Loss S1:  0.06351707906116172\n",
            "Loss S2:  0.06299103852755354\n",
            "Loss S1:  0.06357104506048639\n",
            "Loss S2:  0.06307177257633978\n",
            "Loss S1:  0.06356731926401456\n",
            "Loss S2:  0.06304972208081147\n",
            "Loss S1:  0.06360710637226\n",
            "Loss S2:  0.06303699261965513\n",
            "Loss S1:  0.0636109898213267\n",
            "Loss S2:  0.06301317051695685\n",
            "Loss S1:  0.0636068274831678\n",
            "Loss S2:  0.06300202721920539\n",
            "Loss S1:  0.06359752357158514\n",
            "Loss S2:  0.06296880290750652\n",
            "Loss S1:  0.06361611681686077\n",
            "Loss S2:  0.06294613866660363\n",
            "Loss S1:  0.06357174302358407\n",
            "Loss S2:  0.06289989309081775\n",
            "Loss S1:  0.0636077871515179\n",
            "Loss S2:  0.06287005424251749\n",
            "Loss S1:  0.06361866433804383\n",
            "Loss S2:  0.06281331928312917\n",
            "Loss S1:  0.06360218466983901\n",
            "Loss S2:  0.0627654123789464\n",
            "Loss S1:  0.06364876688550951\n",
            "Loss S2:  0.06281330913074001\n",
            "Loss S1:  0.06360097731304014\n",
            "Loss S2:  0.06279231960240776\n",
            "Loss S1:  0.0635389218664473\n",
            "Loss S2:  0.06277385899785218\n",
            "Loss S1:  0.06355789091569718\n",
            "Loss S2:  0.06282700827488533\n",
            "Loss S1:  0.06351844632577022\n",
            "Loss S2:  0.06276133881840104\n",
            "Validation: \n",
            " Loss S1:  1.5949044227600098\n",
            " Loss S2:  1.5473058223724365\n",
            " Loss S1:  1.5626370509465535\n",
            " Loss S2:  1.528118366286868\n",
            " Loss S1:  1.55609127661077\n",
            " Loss S2:  1.5194759368896484\n",
            " Loss S1:  1.55131588802963\n",
            " Loss S2:  1.511681951460291\n",
            " Loss S1:  1.5540269775155149\n",
            " Loss S2:  1.5130605756500621\n"
          ]
        }
      ],
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+100):\n",
        "    train(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test(epoch)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model parameters\n",
        "PATH = \"/content/gdrive/MyDrive/Deep Learning/A5/s1_multistudent_model\"\n",
        "torch.save(s1.state_dict(), PATH)\n",
        "PATH = \"/content/gdrive/MyDrive/Deep Learning/A5/s2_multistudent_model\"\n",
        "torch.save(s2.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "fAfWYBJMoMWe"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eofHfplkxnd6"
      },
      "source": [
        "#Create ensamble model\n",
        "\n",
        "1.8 In this step you will create a new network class that takes s1, and s2 as perimeters. This class should initiate a new network that ensembles both s1 and s2, and have a classifier for cross-entropy. In the forward method pass the input x from both s1 and s2 and then concatenate there outputs along axis 1. Then pass this concatinated output through classifier of appropriate shape. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "BFDraDrfpFU4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfda9db2-ae37-4902-db79-263415fc407f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 64, 16, 16]               0\n",
            "            Conv2d-5           [-1, 64, 16, 16]          36,928\n",
            "       BatchNorm2d-6           [-1, 64, 16, 16]             128\n",
            "              ReLU-7           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 64, 8, 8]               0\n",
            "            Conv2d-9            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-10            [-1, 128, 8, 8]             256\n",
            "             ReLU-11            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-12            [-1, 128, 4, 4]               0\n",
            "           Conv2d-13            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-14            [-1, 128, 4, 4]             256\n",
            "             ReLU-15            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-16            [-1, 128, 2, 2]               0\n",
            "           Conv2d-17            [-1, 256, 2, 2]         295,168\n",
            "      BatchNorm2d-18            [-1, 256, 2, 2]             512\n",
            "             ReLU-19            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-20            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-21            [-1, 256, 1, 1]               0\n",
            "           Linear-22                  [-1, 256]          65,792\n",
            "              VGG-23                  [-1, 256]               0\n",
            "           Conv2d-24           [-1, 64, 32, 32]           1,792\n",
            "      BatchNorm2d-25           [-1, 64, 32, 32]             128\n",
            "             ReLU-26           [-1, 64, 32, 32]               0\n",
            "        MaxPool2d-27           [-1, 64, 16, 16]               0\n",
            "           Conv2d-28           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-29           [-1, 64, 16, 16]             128\n",
            "             ReLU-30           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-31             [-1, 64, 8, 8]               0\n",
            "           Conv2d-32            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-33            [-1, 128, 8, 8]             256\n",
            "             ReLU-34            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-35            [-1, 128, 4, 4]               0\n",
            "           Conv2d-36            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-37            [-1, 128, 4, 4]             256\n",
            "             ReLU-38            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-39            [-1, 128, 2, 2]               0\n",
            "           Conv2d-40            [-1, 256, 2, 2]         295,168\n",
            "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
            "             ReLU-42            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-43            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-44            [-1, 256, 1, 1]               0\n",
            "           Linear-45                  [-1, 256]          65,792\n",
            "              VGG-46                  [-1, 256]               0\n",
            "           Linear-47                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 1,249,930\n",
            "Trainable params: 1,249,930\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 4.63\n",
            "Params size (MB): 4.77\n",
            "Estimated Total Size (MB): 9.41\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, s1,s2):\n",
        "        super(Net, self).__init__()\n",
        "        self.s1 = s1\n",
        "        self.s2 = s2\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #Code here\n",
        "        # print(x.shape)\n",
        "        x1 = self.s1(x)\n",
        "        x2 = self.s2(x)\n",
        "        # print(\"x1 shape\", x1.shape)\n",
        "        # print(\"x2 shape\", x2.shape)\n",
        "        x = torch.cat((x1, x2),1)\n",
        "        # print(x.shape)\n",
        "        out = self.classifier(x)\n",
        "        # print(out.shape)\n",
        "        return out\n",
        "net = Net(s1,s2)\n",
        "net.to(device)\n",
        "summary(net, (3, 32, 32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udBh3ZBIxpF2"
      },
      "source": [
        "#Train The Ensambled network\n",
        "1.9 In this step you will freez all the conv layers in the ensambled network and then finetune it on orignal dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "JzDX2cJyqfND",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "146b2da2-2176-4ce4-87b7-e052a1a5d983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 -> Net(\n",
            "  (s1): VGG(\n",
            "    (features): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (6): ReLU(inplace=True)\n",
            "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (10): ReLU(inplace=True)\n",
            "      (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (14): ReLU(inplace=True)\n",
            "      (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (16): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (18): ReLU(inplace=True)\n",
            "      (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (20): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            "    )\n",
            "    (classifier): Linear(in_features=256, out_features=256, bias=True)\n",
            "  )\n",
            "  (s2): VGG(\n",
            "    (features): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (6): ReLU(inplace=True)\n",
            "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (10): ReLU(inplace=True)\n",
            "      (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (14): ReLU(inplace=True)\n",
            "      (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (16): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (18): ReLU(inplace=True)\n",
            "      (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (20): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            "    )\n",
            "    (classifier): Linear(in_features=256, out_features=256, bias=True)\n",
            "  )\n",
            "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "<generator object Module.parameters at 0x7f3349160c50>\n",
            "1 -> VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU(inplace=True)\n",
            "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (14): ReLU(inplace=True)\n",
            "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (16): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (20): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            "  )\n",
            "  (classifier): Linear(in_features=256, out_features=256, bias=True)\n",
            ")\n",
            "<generator object Module.parameters at 0x7f33480028d0>\n",
            "2 -> Sequential(\n",
            "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): ReLU(inplace=True)\n",
            "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (6): ReLU(inplace=True)\n",
            "  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (10): ReLU(inplace=True)\n",
            "  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (14): ReLU(inplace=True)\n",
            "  (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (16): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (18): ReLU(inplace=True)\n",
            "  (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (20): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            ")\n",
            "<generator object Module.parameters at 0x7f3348002950>\n",
            "3 -> Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f33480026d0>\n",
            "4 -> BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f33480026d0>\n",
            "5 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f33480026d0>\n",
            "6 -> MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "<generator object Module.parameters at 0x7f3348002150>\n",
            "7 -> Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f3348002ed0>\n",
            "8 -> BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f3348002ed0>\n",
            "9 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f3348002ed0>\n",
            "10 -> MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "<generator object Module.parameters at 0x7f3347d62850>\n",
            "11 -> Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f3347d62250>\n",
            "12 -> BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f3347d62250>\n",
            "13 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f3347d62250>\n",
            "14 -> MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "<generator object Module.parameters at 0x7f3347d62750>\n",
            "15 -> Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f3347d62a50>\n",
            "16 -> BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f3347d62a50>\n",
            "17 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f3347d62a50>\n",
            "18 -> MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "<generator object Module.parameters at 0x7f3347d627d0>\n",
            "19 -> Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f3347d62350>\n",
            "20 -> BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f3347d62350>\n",
            "21 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f3347d62350>\n",
            "22 -> MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "<generator object Module.parameters at 0x7f3347d628d0>\n",
            "23 -> AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            "<generator object Module.parameters at 0x7f3347d628d0>\n",
            "24 -> Linear(in_features=256, out_features=256, bias=True)\n",
            "<generator object Module.parameters at 0x7f3348002950>\n",
            "25 -> VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU(inplace=True)\n",
            "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (14): ReLU(inplace=True)\n",
            "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (16): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (20): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            "  )\n",
            "  (classifier): Linear(in_features=256, out_features=256, bias=True)\n",
            ")\n",
            "<generator object Module.parameters at 0x7f3348002950>\n",
            "26 -> Sequential(\n",
            "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): ReLU(inplace=True)\n",
            "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (6): ReLU(inplace=True)\n",
            "  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (10): ReLU(inplace=True)\n",
            "  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (14): ReLU(inplace=True)\n",
            "  (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (16): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (18): ReLU(inplace=True)\n",
            "  (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (20): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            ")\n",
            "<generator object Module.parameters at 0x7f33480028d0>\n",
            "27 -> Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f3347d62ed0>\n",
            "28 -> BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f3347d62ed0>\n",
            "29 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f3347d62ed0>\n",
            "30 -> MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "<generator object Module.parameters at 0x7f3347d62bd0>\n",
            "31 -> Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f3347d629d0>\n",
            "32 -> BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f3347d629d0>\n",
            "33 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f3347d629d0>\n",
            "34 -> MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "<generator object Module.parameters at 0x7f3347d626d0>\n",
            "35 -> Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f3347d624d0>\n",
            "36 -> BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f3347d624d0>\n",
            "37 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f3347d624d0>\n",
            "38 -> MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "<generator object Module.parameters at 0x7f3347d625d0>\n",
            "39 -> Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f3347d62450>\n",
            "40 -> BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f3347d62450>\n",
            "41 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f3347d62450>\n",
            "42 -> MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "<generator object Module.parameters at 0x7f3347d62950>\n",
            "43 -> Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f3347d62f50>\n",
            "44 -> BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f3347d62f50>\n",
            "45 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f3347d62f50>\n",
            "46 -> MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "<generator object Module.parameters at 0x7f3347d622d0>\n",
            "47 -> AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            "<generator object Module.parameters at 0x7f3347d622d0>\n",
            "48 -> Linear(in_features=256, out_features=256, bias=True)\n",
            "<generator object Module.parameters at 0x7f33480028d0>\n",
            "49 -> Linear(in_features=512, out_features=10, bias=True)\n",
            "<generator object Module.parameters at 0x7f3348002950>\n"
          ]
        }
      ],
      "source": [
        "for i, m in enumerate(net.modules()):\n",
        "  print(i, '->', m)\n",
        "  print(m.parameters())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is found by printing the model summary abouve using the s1.module() and then finding the index of convolutional layers\n",
        "index_of_conv_layers = [3,7,11,15,19,27,31,35,39,43]\n",
        "\n",
        "for i, m in enumerate(net.modules()):\n",
        "    # only update if the index matches index of a conv2d layer\n",
        "    if i in index_of_conv_layers:\n",
        "      for param in m.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "net = net.to(device)\n",
        "summary(net, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJHJwuXWpdAJ",
        "outputId": "857e0855-ca1e-4216-81a5-06ca0435001e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 64, 16, 16]               0\n",
            "            Conv2d-5           [-1, 64, 16, 16]          36,928\n",
            "       BatchNorm2d-6           [-1, 64, 16, 16]             128\n",
            "              ReLU-7           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 64, 8, 8]               0\n",
            "            Conv2d-9            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-10            [-1, 128, 8, 8]             256\n",
            "             ReLU-11            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-12            [-1, 128, 4, 4]               0\n",
            "           Conv2d-13            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-14            [-1, 128, 4, 4]             256\n",
            "             ReLU-15            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-16            [-1, 128, 2, 2]               0\n",
            "           Conv2d-17            [-1, 256, 2, 2]         295,168\n",
            "      BatchNorm2d-18            [-1, 256, 2, 2]             512\n",
            "             ReLU-19            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-20            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-21            [-1, 256, 1, 1]               0\n",
            "           Linear-22                  [-1, 256]          65,792\n",
            "              VGG-23                  [-1, 256]               0\n",
            "           Conv2d-24           [-1, 64, 32, 32]           1,792\n",
            "      BatchNorm2d-25           [-1, 64, 32, 32]             128\n",
            "             ReLU-26           [-1, 64, 32, 32]               0\n",
            "        MaxPool2d-27           [-1, 64, 16, 16]               0\n",
            "           Conv2d-28           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-29           [-1, 64, 16, 16]             128\n",
            "             ReLU-30           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-31             [-1, 64, 8, 8]               0\n",
            "           Conv2d-32            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-33            [-1, 128, 8, 8]             256\n",
            "             ReLU-34            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-35            [-1, 128, 4, 4]               0\n",
            "           Conv2d-36            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-37            [-1, 128, 4, 4]             256\n",
            "             ReLU-38            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-39            [-1, 128, 2, 2]               0\n",
            "           Conv2d-40            [-1, 256, 2, 2]         295,168\n",
            "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
            "             ReLU-42            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-43            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-44            [-1, 256, 1, 1]               0\n",
            "           Linear-45                  [-1, 256]          65,792\n",
            "              VGG-46                  [-1, 256]               0\n",
            "           Linear-47                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 1,249,930\n",
            "Trainable params: 139,274\n",
            "Non-trainable params: 1,110,656\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 4.63\n",
            "Params size (MB): 4.77\n",
            "Estimated Total Size (MB): 9.41\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take note that the trainable parameters are much less than the actual number of parameters. This indicates the the conv2d layers were switched off."
      ],
      "metadata": {
        "id": "etSR0iPnA5W1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Gewd8MZOqRJ8"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        inputs = Variable(inputs, requires_grad=False)\n",
        "        targets = Variable(targets)\n",
        "        net.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if(batch_idx % 200 == 0):\n",
        "          print(\"Accuracy : \",100.*correct/total,\" Loss : \", train_loss/(batch_idx+1))\n",
        "def test(epoch):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\"Accuracy : \",100.*correct/total,\" Loss : \", test_loss/(batch_idx+1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+10):\n",
        "    train(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqKfkzGipvBg",
        "outputId": "9fa888ea-a4ac-48e3-eb87-ace7de1700ed"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Accuracy :  7.0  Loss :  2.6424481868743896\n",
            "Accuracy :  72.8407960199005  Loss :  0.9775601062015514\n",
            "Accuracy :  78.95511221945137  Loss :  0.7290290362222533\n",
            "Validation: \n",
            "Accuracy :  84.0  Loss :  0.5044693946838379\n",
            "Accuracy :  83.71428571428571  Loss :  0.47616819398743765\n",
            "Accuracy :  83.60975609756098  Loss :  0.48366556516507775\n",
            "Accuracy :  83.75409836065573  Loss :  0.4782984203002492\n",
            "Accuracy :  83.39506172839506  Loss :  0.47503023015128243\n",
            "\n",
            "Epoch: 2\n",
            "Accuracy :  88.0  Loss :  0.39672794938087463\n",
            "Accuracy :  85.29353233830845  Loss :  0.4263456021227054\n",
            "Accuracy :  85.57356608478803  Loss :  0.4203893430438125\n",
            "Validation: \n",
            "Accuracy :  85.0  Loss :  0.4743480384349823\n",
            "Accuracy :  84.19047619047619  Loss :  0.45898051985672544\n",
            "Accuracy :  84.1219512195122  Loss :  0.4659295391018798\n",
            "Accuracy :  84.26229508196721  Loss :  0.46032412594459093\n",
            "Accuracy :  84.04938271604938  Loss :  0.45722021088923936\n",
            "\n",
            "Epoch: 3\n",
            "Accuracy :  89.0  Loss :  0.3189612329006195\n",
            "Accuracy :  85.88059701492537  Loss :  0.4060089639318523\n",
            "Accuracy :  86.02992518703242  Loss :  0.4017593302854576\n",
            "Validation: \n",
            "Accuracy :  85.0  Loss :  0.4495347738265991\n",
            "Accuracy :  84.28571428571429  Loss :  0.45576678003583637\n",
            "Accuracy :  84.21951219512195  Loss :  0.46276239142185305\n",
            "Accuracy :  84.39344262295081  Loss :  0.4573842062324774\n",
            "Accuracy :  84.25925925925925  Loss :  0.4544464283519321\n",
            "\n",
            "Epoch: 4\n",
            "Accuracy :  89.0  Loss :  0.3580615520477295\n",
            "Accuracy :  86.08955223880596  Loss :  0.40179184561048575\n",
            "Accuracy :  86.2793017456359  Loss :  0.3981592735447491\n",
            "Validation: \n",
            "Accuracy :  85.0  Loss :  0.43814265727996826\n",
            "Accuracy :  84.52380952380952  Loss :  0.45030606999283745\n",
            "Accuracy :  84.51219512195122  Loss :  0.4580977555455231\n",
            "Accuracy :  84.67213114754098  Loss :  0.45302928788740127\n",
            "Accuracy :  84.54320987654322  Loss :  0.4501797510885898\n",
            "\n",
            "Epoch: 5\n",
            "Accuracy :  86.0  Loss :  0.36819061636924744\n",
            "Accuracy :  86.23880597014926  Loss :  0.397931837768697\n",
            "Accuracy :  86.23690773067332  Loss :  0.39556906924134777\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.42848628759384155\n",
            "Accuracy :  84.57142857142857  Loss :  0.4474629546914782\n",
            "Accuracy :  84.53658536585365  Loss :  0.4546827288662515\n",
            "Accuracy :  84.75409836065573  Loss :  0.4489863153363838\n",
            "Accuracy :  84.60493827160494  Loss :  0.4461912994767413\n",
            "\n",
            "Epoch: 6\n",
            "Accuracy :  89.0  Loss :  0.3187131881713867\n",
            "Accuracy :  86.29353233830845  Loss :  0.3947786507914908\n",
            "Accuracy :  86.51620947630923  Loss :  0.3904981165381144\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.41792845726013184\n",
            "Accuracy :  84.47619047619048  Loss :  0.44540093200547354\n",
            "Accuracy :  84.60975609756098  Loss :  0.453707053167064\n",
            "Accuracy :  84.85245901639344  Loss :  0.44780388083614286\n",
            "Accuracy :  84.80246913580247  Loss :  0.44481729357330885\n",
            "\n",
            "Epoch: 7\n",
            "Accuracy :  89.0  Loss :  0.326315701007843\n",
            "Accuracy :  86.51243781094527  Loss :  0.38702427606974077\n",
            "Accuracy :  86.57605985037407  Loss :  0.38468475918520123\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.40970736742019653\n",
            "Accuracy :  84.71428571428571  Loss :  0.44375144725754145\n",
            "Accuracy :  84.78048780487805  Loss :  0.45186202555167965\n",
            "Accuracy :  84.9672131147541  Loss :  0.44552670003937894\n",
            "Accuracy :  84.8641975308642  Loss :  0.4427839793540813\n",
            "\n",
            "Epoch: 8\n",
            "Accuracy :  90.0  Loss :  0.30981308221817017\n",
            "Accuracy :  86.70149253731343  Loss :  0.3819033138046217\n",
            "Accuracy :  86.69825436408978  Loss :  0.38360690873608627\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.4084934890270233\n",
            "Accuracy :  84.71428571428571  Loss :  0.44152275792190004\n",
            "Accuracy :  84.82926829268293  Loss :  0.44958952469069785\n",
            "Accuracy :  84.95081967213115  Loss :  0.44399158558884605\n",
            "Accuracy :  84.87654320987654  Loss :  0.44163867739247686\n",
            "\n",
            "Epoch: 9\n",
            "Accuracy :  87.0  Loss :  0.29276660084724426\n",
            "Accuracy :  86.61194029850746  Loss :  0.38481244014863353\n",
            "Accuracy :  86.57605985037407  Loss :  0.3845551446860568\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.40389907360076904\n",
            "Accuracy :  84.95238095238095  Loss :  0.4385817221232823\n",
            "Accuracy :  84.92682926829268  Loss :  0.44684600757389537\n",
            "Accuracy :  85.06557377049181  Loss :  0.4409902721154885\n",
            "Accuracy :  84.93827160493827  Loss :  0.43831114526148196\n",
            "\n",
            "Epoch: 10\n",
            "Accuracy :  90.0  Loss :  0.2948917746543884\n",
            "Accuracy :  86.70149253731343  Loss :  0.38299568723979877\n",
            "Accuracy :  86.87032418952619  Loss :  0.37932486311901836\n",
            "Validation: \n",
            "Accuracy :  88.0  Loss :  0.39520499110221863\n",
            "Accuracy :  84.95238095238095  Loss :  0.43801026330107734\n",
            "Accuracy :  84.92682926829268  Loss :  0.446719311722895\n",
            "Accuracy :  85.08196721311475  Loss :  0.44077743638734346\n",
            "Accuracy :  84.88888888888889  Loss :  0.4381489759242093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finetuned student model** with **2 children** has even better final validation accuracy of **88.0%**. This final validation accuracy is greater than that achieved by a single student finetuned model (85%) and teacher model (83%). This suggests that the student networks were able to learn a more accurate representation of the underlying model depicted by the training set."
      ],
      "metadata": {
        "id": "ciJJubfLB3D8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cjdE_B9nO4v"
      },
      "source": [
        "#4 and 8 Students (Bonus)\n",
        "2.0 In this step try to divide dense feature into four and eight chunks and try to create respective smaller students. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_Ij2AhZyta6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "PA5_Compression_Part1_KD.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "83fd4851e4ab407281e0da49be9acf05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b39e20a0010a41f9a75c59c1e18c885a",
              "IPY_MODEL_306d6e54b9ce41dd997fb43c4a7d782b",
              "IPY_MODEL_982ea5ce6c3c465faca21c14ceb4796f"
            ],
            "layout": "IPY_MODEL_d7c4edd1f29e46d494369e8c94804163"
          }
        },
        "b39e20a0010a41f9a75c59c1e18c885a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0c90c561e1e4a46b936989fb9cb3f70",
            "placeholder": "​",
            "style": "IPY_MODEL_909ff3d5f7ec441daf830e2a247e5ce2",
            "value": ""
          }
        },
        "306d6e54b9ce41dd997fb43c4a7d782b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0c57949b101443e8f8d211e17a111c9",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c820ba4ff0cb44e296e227b4664687f8",
            "value": 170498071
          }
        },
        "982ea5ce6c3c465faca21c14ceb4796f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f5dd42c07b0486d8b4f765e2a4e212d",
            "placeholder": "​",
            "style": "IPY_MODEL_e206c15e80df4c5cb5e2fac5708c9d5d",
            "value": " 170499072/? [00:05&lt;00:00, 30405989.87it/s]"
          }
        },
        "d7c4edd1f29e46d494369e8c94804163": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0c90c561e1e4a46b936989fb9cb3f70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "909ff3d5f7ec441daf830e2a247e5ce2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0c57949b101443e8f8d211e17a111c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c820ba4ff0cb44e296e227b4664687f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f5dd42c07b0486d8b4f765e2a4e212d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e206c15e80df4c5cb5e2fac5708c9d5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}