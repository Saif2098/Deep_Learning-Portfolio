{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PA5_Compression_Part2_Structured_Pruning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e1f2c22b96294f1eb71fc3b96fa45efe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ded33a07fad44c5be0c7bd2f2d1a150",
              "IPY_MODEL_5fdb29f25e7f42ff88de06d6af5023f2",
              "IPY_MODEL_21490d32ea94469fb2173624e5f39f55"
            ],
            "layout": "IPY_MODEL_4c1eee2b01b745fa8ce30e9962757ba9"
          }
        },
        "0ded33a07fad44c5be0c7bd2f2d1a150": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7da51d20e714737badcf514ead2673d",
            "placeholder": "​",
            "style": "IPY_MODEL_b230106c25c74697b1933a7fde77664e",
            "value": ""
          }
        },
        "5fdb29f25e7f42ff88de06d6af5023f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_617bb668575441ba9b5374cb315adf1f",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a23d38add3d42b3bb1238415bdc74e5",
            "value": 170498071
          }
        },
        "21490d32ea94469fb2173624e5f39f55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c977b10cb6f14a0cbacc32026d585b1c",
            "placeholder": "​",
            "style": "IPY_MODEL_087010526f1b41df96df9f088f6c10c8",
            "value": " 170499072/? [00:05&lt;00:00, 33690075.39it/s]"
          }
        },
        "4c1eee2b01b745fa8ce30e9962757ba9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7da51d20e714737badcf514ead2673d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b230106c25c74697b1933a7fde77664e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "617bb668575441ba9b5374cb315adf1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a23d38add3d42b3bb1238415bdc74e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c977b10cb6f14a0cbacc32026d585b1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "087010526f1b41df96df9f088f6c10c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Name: Saif Ur Rahman\n",
        "## Roll Number: 2022-10-0001"
      ],
      "metadata": {
        "id": "mZeUh3ONDQkw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Structured Pruning via Gates\n",
        "In this assignment, we will implement filter pruning via gates. [relevant paper](https://arxiv.org/pdf/2010.02623.pdf) "
      ],
      "metadata": {
        "id": "Wd_UeRo0lO5m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "e1f2c22b96294f1eb71fc3b96fa45efe",
            "0ded33a07fad44c5be0c7bd2f2d1a150",
            "5fdb29f25e7f42ff88de06d6af5023f2",
            "21490d32ea94469fb2173624e5f39f55",
            "4c1eee2b01b745fa8ce30e9962757ba9",
            "f7da51d20e714737badcf514ead2673d",
            "b230106c25c74697b1933a7fde77664e",
            "617bb668575441ba9b5374cb315adf1f",
            "4a23d38add3d42b3bb1238415bdc74e5",
            "c977b10cb6f14a0cbacc32026d585b1c",
            "087010526f1b41df96df9f088f6c10c8"
          ]
        },
        "id": "5oLTKWqOb6H4",
        "outputId": "20e342db-5f63-4d10-dd0e-4a15c87db53b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1f2c22b96294f1eb71fc3b96fa45efe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from torch.autograd import Variable\n",
        "import tqdm\n",
        "from torchsummary import summary\n",
        "\n",
        "batch_size = 100\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=100, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create Gates Class\n",
        "\n",
        "2.1 In this cell you will create a gates class. This class should initiate a 1-D tensor weight of length : 'size'. Initialization value of all gates should be 1. In forward method reshape this weight tensor to (1, size, 1, 1) so that it can be broadcasted to (batch, depth, width, height) and then multiply multiply it with input 'x' and return the product."
      ],
      "metadata": {
        "id": "aiTJXJuor4iS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Gates(nn.Module):\n",
        "    def __init__(self, size):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.weight = torch.nn.Parameter(torch.ones((self.size,), device=\"cuda\"))\n",
        "\n",
        "    def forward(self, x):\n",
        "        temp = self.weight[None, :, None, None]\n",
        "        result =  torch.mul(temp, x)\n",
        "        return result"
      ],
      "metadata": {
        "id": "oF1qlOx3cMPa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Place Gates in the network\n",
        "2.2 In this cell you need to place gate layers infront of conv layers during network initialization. Pass number of filters in the conv to gates class to initiate gates. "
      ],
      "metadata": {
        "id": "fvRfqP1-v8IQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                # create a Gates() class instance\n",
        "                gates_instance = Gates(x)\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           gates_instance, # initialized gates instance \n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "net = VGG('VGG16')\n",
        "net = net.to(device)"
      ],
      "metadata": {
        "id": "EIG3VUawcUf0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The summery of network should look something like this"
      ],
      "metadata": {
        "id": "XDWaM0cSxMwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(net, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvbUL3ufHTRH",
        "outputId": "5aed7937-e479-459a-a0cd-eb88ee95e2df"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "             Gates-2           [-1, 64, 32, 32]              64\n",
            "       BatchNorm2d-3           [-1, 64, 32, 32]             128\n",
            "              ReLU-4           [-1, 64, 32, 32]               0\n",
            "            Conv2d-5           [-1, 64, 32, 32]          36,928\n",
            "             Gates-6           [-1, 64, 32, 32]              64\n",
            "       BatchNorm2d-7           [-1, 64, 32, 32]             128\n",
            "              ReLU-8           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-9           [-1, 64, 16, 16]               0\n",
            "           Conv2d-10          [-1, 128, 16, 16]          73,856\n",
            "            Gates-11          [-1, 128, 16, 16]             128\n",
            "      BatchNorm2d-12          [-1, 128, 16, 16]             256\n",
            "             ReLU-13          [-1, 128, 16, 16]               0\n",
            "           Conv2d-14          [-1, 128, 16, 16]         147,584\n",
            "            Gates-15          [-1, 128, 16, 16]             128\n",
            "      BatchNorm2d-16          [-1, 128, 16, 16]             256\n",
            "             ReLU-17          [-1, 128, 16, 16]               0\n",
            "        MaxPool2d-18            [-1, 128, 8, 8]               0\n",
            "           Conv2d-19            [-1, 256, 8, 8]         295,168\n",
            "            Gates-20            [-1, 256, 8, 8]             256\n",
            "      BatchNorm2d-21            [-1, 256, 8, 8]             512\n",
            "             ReLU-22            [-1, 256, 8, 8]               0\n",
            "           Conv2d-23            [-1, 256, 8, 8]         590,080\n",
            "            Gates-24            [-1, 256, 8, 8]             256\n",
            "      BatchNorm2d-25            [-1, 256, 8, 8]             512\n",
            "             ReLU-26            [-1, 256, 8, 8]               0\n",
            "           Conv2d-27            [-1, 256, 8, 8]         590,080\n",
            "            Gates-28            [-1, 256, 8, 8]             256\n",
            "      BatchNorm2d-29            [-1, 256, 8, 8]             512\n",
            "             ReLU-30            [-1, 256, 8, 8]               0\n",
            "        MaxPool2d-31            [-1, 256, 4, 4]               0\n",
            "           Conv2d-32            [-1, 512, 4, 4]       1,180,160\n",
            "            Gates-33            [-1, 512, 4, 4]             512\n",
            "      BatchNorm2d-34            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-35            [-1, 512, 4, 4]               0\n",
            "           Conv2d-36            [-1, 512, 4, 4]       2,359,808\n",
            "            Gates-37            [-1, 512, 4, 4]             512\n",
            "      BatchNorm2d-38            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-39            [-1, 512, 4, 4]               0\n",
            "           Conv2d-40            [-1, 512, 4, 4]       2,359,808\n",
            "            Gates-41            [-1, 512, 4, 4]             512\n",
            "      BatchNorm2d-42            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-43            [-1, 512, 4, 4]               0\n",
            "        MaxPool2d-44            [-1, 512, 2, 2]               0\n",
            "           Conv2d-45            [-1, 512, 2, 2]       2,359,808\n",
            "            Gates-46            [-1, 512, 2, 2]             512\n",
            "      BatchNorm2d-47            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-48            [-1, 512, 2, 2]               0\n",
            "           Conv2d-49            [-1, 512, 2, 2]       2,359,808\n",
            "            Gates-50            [-1, 512, 2, 2]             512\n",
            "      BatchNorm2d-51            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-52            [-1, 512, 2, 2]               0\n",
            "           Conv2d-53            [-1, 512, 2, 2]       2,359,808\n",
            "            Gates-54            [-1, 512, 2, 2]             512\n",
            "      BatchNorm2d-55            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-56            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-57            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-58            [-1, 512, 1, 1]               0\n",
            "           Linear-59                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 14,732,490\n",
            "Trainable params: 14,732,490\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 8.68\n",
            "Params size (MB): 56.20\n",
            "Estimated Total Size (MB): 64.89\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(net, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVwEATXFcYfS",
        "outputId": "59b38881-9edf-42c4-bc09-5a29742d0b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "             Gates-2           [-1, 64, 32, 32]              64\n",
            "       BatchNorm2d-3           [-1, 64, 32, 32]             128\n",
            "              ReLU-4           [-1, 64, 32, 32]               0\n",
            "            Conv2d-5           [-1, 64, 32, 32]          36,928\n",
            "             Gates-6           [-1, 64, 32, 32]              64\n",
            "       BatchNorm2d-7           [-1, 64, 32, 32]             128\n",
            "              ReLU-8           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-9           [-1, 64, 16, 16]               0\n",
            "           Conv2d-10          [-1, 128, 16, 16]          73,856\n",
            "            Gates-11          [-1, 128, 16, 16]             128\n",
            "      BatchNorm2d-12          [-1, 128, 16, 16]             256\n",
            "             ReLU-13          [-1, 128, 16, 16]               0\n",
            "           Conv2d-14          [-1, 128, 16, 16]         147,584\n",
            "            Gates-15          [-1, 128, 16, 16]             128\n",
            "      BatchNorm2d-16          [-1, 128, 16, 16]             256\n",
            "             ReLU-17          [-1, 128, 16, 16]               0\n",
            "        MaxPool2d-18            [-1, 128, 8, 8]               0\n",
            "           Conv2d-19            [-1, 256, 8, 8]         295,168\n",
            "            Gates-20            [-1, 256, 8, 8]             256\n",
            "      BatchNorm2d-21            [-1, 256, 8, 8]             512\n",
            "             ReLU-22            [-1, 256, 8, 8]               0\n",
            "           Conv2d-23            [-1, 256, 8, 8]         590,080\n",
            "            Gates-24            [-1, 256, 8, 8]             256\n",
            "      BatchNorm2d-25            [-1, 256, 8, 8]             512\n",
            "             ReLU-26            [-1, 256, 8, 8]               0\n",
            "           Conv2d-27            [-1, 256, 8, 8]         590,080\n",
            "            Gates-28            [-1, 256, 8, 8]             256\n",
            "      BatchNorm2d-29            [-1, 256, 8, 8]             512\n",
            "             ReLU-30            [-1, 256, 8, 8]               0\n",
            "        MaxPool2d-31            [-1, 256, 4, 4]               0\n",
            "           Conv2d-32            [-1, 512, 4, 4]       1,180,160\n",
            "            Gates-33            [-1, 512, 4, 4]             512\n",
            "      BatchNorm2d-34            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-35            [-1, 512, 4, 4]               0\n",
            "           Conv2d-36            [-1, 512, 4, 4]       2,359,808\n",
            "            Gates-37            [-1, 512, 4, 4]             512\n",
            "      BatchNorm2d-38            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-39            [-1, 512, 4, 4]               0\n",
            "           Conv2d-40            [-1, 512, 4, 4]       2,359,808\n",
            "            Gates-41            [-1, 512, 4, 4]             512\n",
            "      BatchNorm2d-42            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-43            [-1, 512, 4, 4]               0\n",
            "        MaxPool2d-44            [-1, 512, 2, 2]               0\n",
            "           Conv2d-45            [-1, 512, 2, 2]       2,359,808\n",
            "            Gates-46            [-1, 512, 2, 2]             512\n",
            "      BatchNorm2d-47            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-48            [-1, 512, 2, 2]               0\n",
            "           Conv2d-49            [-1, 512, 2, 2]       2,359,808\n",
            "            Gates-50            [-1, 512, 2, 2]             512\n",
            "      BatchNorm2d-51            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-52            [-1, 512, 2, 2]               0\n",
            "           Conv2d-53            [-1, 512, 2, 2]       2,359,808\n",
            "            Gates-54            [-1, 512, 2, 2]             512\n",
            "      BatchNorm2d-55            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-56            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-57            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-58            [-1, 512, 1, 1]               0\n",
            "           Linear-59                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 14,732,490\n",
            "Trainable params: 14,732,490\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 8.68\n",
            "Params size (MB): 56.20\n",
            "Estimated Total Size (MB): 64.89\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Freez Gates\n",
        "2.3 In this cell you will traverse network and make gates non-trainable.  "
      ],
      "metadata": {
        "id": "AZeWHpK9w7U3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first list the layers and later hardcode gates to make non-trainable \n",
        "for i, m in enumerate(net.modules()):\n",
        "  print(i, '->', m)\n",
        "  print(m.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUVAYU7vGf2A",
        "outputId": "0fa82434-5a52-4e06-ffe7-6dcf99f139ef"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 -> VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): Gates()\n",
            "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): Gates()\n",
            "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (9): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (10): Gates()\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (14): Gates()\n",
            "    (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (18): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (19): Gates()\n",
            "    (20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (21): ReLU(inplace=True)\n",
            "    (22): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (23): Gates()\n",
            "    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): Gates()\n",
            "    (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (31): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (32): Gates()\n",
            "    (33): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (34): ReLU(inplace=True)\n",
            "    (35): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (36): Gates()\n",
            "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (38): ReLU(inplace=True)\n",
            "    (39): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (40): Gates()\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (44): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (45): Gates()\n",
            "    (46): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (47): ReLU(inplace=True)\n",
            "    (48): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (49): Gates()\n",
            "    (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (51): ReLU(inplace=True)\n",
            "    (52): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (53): Gates()\n",
            "    (54): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (55): ReLU(inplace=True)\n",
            "    (56): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (57): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            "  )\n",
            "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "<generator object Module.parameters at 0x7f5e07724650>\n",
            "1 -> Sequential(\n",
            "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): Gates()\n",
            "  (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (3): ReLU(inplace=True)\n",
            "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (5): Gates()\n",
            "  (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (7): ReLU(inplace=True)\n",
            "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (9): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (10): Gates()\n",
            "  (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (12): ReLU(inplace=True)\n",
            "  (13): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (14): Gates()\n",
            "  (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (16): ReLU(inplace=True)\n",
            "  (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (18): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (19): Gates()\n",
            "  (20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (21): ReLU(inplace=True)\n",
            "  (22): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (23): Gates()\n",
            "  (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (25): ReLU(inplace=True)\n",
            "  (26): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (27): Gates()\n",
            "  (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (29): ReLU(inplace=True)\n",
            "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (31): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (32): Gates()\n",
            "  (33): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (34): ReLU(inplace=True)\n",
            "  (35): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (36): Gates()\n",
            "  (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (38): ReLU(inplace=True)\n",
            "  (39): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (40): Gates()\n",
            "  (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (42): ReLU(inplace=True)\n",
            "  (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (44): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (45): Gates()\n",
            "  (46): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (47): ReLU(inplace=True)\n",
            "  (48): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (49): Gates()\n",
            "  (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (51): ReLU(inplace=True)\n",
            "  (52): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (53): Gates()\n",
            "  (54): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (55): ReLU(inplace=True)\n",
            "  (56): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (57): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            ")\n",
            "<generator object Module.parameters at 0x7f5e07724d50>\n",
            "2 -> Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f5e07724ed0>\n",
            "3 -> Gates()\n",
            "<generator object Module.parameters at 0x7f5e07724c50>\n",
            "4 -> BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f5e07724c50>\n",
            "5 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f5e07724c50>\n",
            "6 -> Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f5e077243d0>\n",
            "7 -> Gates()\n",
            "<generator object Module.parameters at 0x7f5e077243d0>\n",
            "8 -> BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f5e077243d0>\n",
            "9 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f5e077243d0>\n",
            "10 -> MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "<generator object Module.parameters at 0x7f5e07724450>\n",
            "11 -> Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f5e077248d0>\n",
            "12 -> Gates()\n",
            "<generator object Module.parameters at 0x7f5e077248d0>\n",
            "13 -> BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f5e077248d0>\n",
            "14 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f5e077248d0>\n",
            "15 -> Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f5e07724350>\n",
            "16 -> Gates()\n",
            "<generator object Module.parameters at 0x7f5e07724350>\n",
            "17 -> BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f5e07724350>\n",
            "18 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f5e07724350>\n",
            "19 -> MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "<generator object Module.parameters at 0x7f5e07724850>\n",
            "20 -> Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f5e07724850>\n",
            "21 -> Gates()\n",
            "<generator object Module.parameters at 0x7f5e07724850>\n",
            "22 -> BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f5e07724850>\n",
            "23 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f5e07724850>\n",
            "24 -> Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f5e07724750>\n",
            "25 -> Gates()\n",
            "<generator object Module.parameters at 0x7f5e07724750>\n",
            "26 -> BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f5e07724750>\n",
            "27 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f5e07724750>\n",
            "28 -> Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f5e077247d0>\n",
            "29 -> Gates()\n",
            "<generator object Module.parameters at 0x7f5e077247d0>\n",
            "30 -> BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f5e077247d0>\n",
            "31 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f5e077247d0>\n",
            "32 -> MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "<generator object Module.parameters at 0x7f5e077244d0>\n",
            "33 -> Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f5e077246d0>\n",
            "34 -> Gates()\n",
            "<generator object Module.parameters at 0x7f5e077246d0>\n",
            "35 -> BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f5e077246d0>\n",
            "36 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f5e077246d0>\n",
            "37 -> Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f5e07724bd0>\n",
            "38 -> Gates()\n",
            "<generator object Module.parameters at 0x7f5e07724bd0>\n",
            "39 -> BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f5e07724bd0>\n",
            "40 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f5e07724bd0>\n",
            "41 -> Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f5e07724dd0>\n",
            "42 -> Gates()\n",
            "<generator object Module.parameters at 0x7f5e07724dd0>\n",
            "43 -> BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f5e07724dd0>\n",
            "44 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f5e07724dd0>\n",
            "45 -> MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "<generator object Module.parameters at 0x7f5e07724e50>\n",
            "46 -> Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f5e07724b50>\n",
            "47 -> Gates()\n",
            "<generator object Module.parameters at 0x7f5e07724b50>\n",
            "48 -> BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f5e07724b50>\n",
            "49 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f5e07724b50>\n",
            "50 -> Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f5e077245d0>\n",
            "51 -> Gates()\n",
            "<generator object Module.parameters at 0x7f5e077245d0>\n",
            "52 -> BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f5e077245d0>\n",
            "53 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f5e077245d0>\n",
            "54 -> Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "<generator object Module.parameters at 0x7f5e0772a050>\n",
            "55 -> Gates()\n",
            "<generator object Module.parameters at 0x7f5e0772a050>\n",
            "56 -> BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "<generator object Module.parameters at 0x7f5e0772a050>\n",
            "57 -> ReLU(inplace=True)\n",
            "<generator object Module.parameters at 0x7f5e0772a050>\n",
            "58 -> MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "<generator object Module.parameters at 0x7f5e0772a0d0>\n",
            "59 -> AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            "<generator object Module.parameters at 0x7f5e0772a0d0>\n",
            "60 -> Linear(in_features=512, out_features=10, bias=True)\n",
            "<generator object Module.parameters at 0x7f5e07724d50>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is found by printing the model summary abouve using the s1.module() and then finding the index of convolutional layers\n",
        "index_of_gates_layers = [3,7,12,16,21,25,29,34,38,42,47,51,55]\n",
        "\n",
        "for i, m in enumerate(net.modules()):\n",
        "    # only update if the index matches index of a Gates() layer\n",
        "    if i in index_of_gates_layers:\n",
        "      for param in m.parameters():\n",
        "        param.requires_grad = False\n",
        "net = net.to(device)"
      ],
      "metadata": {
        "id": "FIPGs-15cbkH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# new model summary to show gates parameters have been frozen\n",
        "summary(net, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOeIcL-4HuqZ",
        "outputId": "ddff9e40-9056-49b8-93cf-3c04eadcae42"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "             Gates-2           [-1, 64, 32, 32]              64\n",
            "       BatchNorm2d-3           [-1, 64, 32, 32]             128\n",
            "              ReLU-4           [-1, 64, 32, 32]               0\n",
            "            Conv2d-5           [-1, 64, 32, 32]          36,928\n",
            "             Gates-6           [-1, 64, 32, 32]              64\n",
            "       BatchNorm2d-7           [-1, 64, 32, 32]             128\n",
            "              ReLU-8           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-9           [-1, 64, 16, 16]               0\n",
            "           Conv2d-10          [-1, 128, 16, 16]          73,856\n",
            "            Gates-11          [-1, 128, 16, 16]             128\n",
            "      BatchNorm2d-12          [-1, 128, 16, 16]             256\n",
            "             ReLU-13          [-1, 128, 16, 16]               0\n",
            "           Conv2d-14          [-1, 128, 16, 16]         147,584\n",
            "            Gates-15          [-1, 128, 16, 16]             128\n",
            "      BatchNorm2d-16          [-1, 128, 16, 16]             256\n",
            "             ReLU-17          [-1, 128, 16, 16]               0\n",
            "        MaxPool2d-18            [-1, 128, 8, 8]               0\n",
            "           Conv2d-19            [-1, 256, 8, 8]         295,168\n",
            "            Gates-20            [-1, 256, 8, 8]             256\n",
            "      BatchNorm2d-21            [-1, 256, 8, 8]             512\n",
            "             ReLU-22            [-1, 256, 8, 8]               0\n",
            "           Conv2d-23            [-1, 256, 8, 8]         590,080\n",
            "            Gates-24            [-1, 256, 8, 8]             256\n",
            "      BatchNorm2d-25            [-1, 256, 8, 8]             512\n",
            "             ReLU-26            [-1, 256, 8, 8]               0\n",
            "           Conv2d-27            [-1, 256, 8, 8]         590,080\n",
            "            Gates-28            [-1, 256, 8, 8]             256\n",
            "      BatchNorm2d-29            [-1, 256, 8, 8]             512\n",
            "             ReLU-30            [-1, 256, 8, 8]               0\n",
            "        MaxPool2d-31            [-1, 256, 4, 4]               0\n",
            "           Conv2d-32            [-1, 512, 4, 4]       1,180,160\n",
            "            Gates-33            [-1, 512, 4, 4]             512\n",
            "      BatchNorm2d-34            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-35            [-1, 512, 4, 4]               0\n",
            "           Conv2d-36            [-1, 512, 4, 4]       2,359,808\n",
            "            Gates-37            [-1, 512, 4, 4]             512\n",
            "      BatchNorm2d-38            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-39            [-1, 512, 4, 4]               0\n",
            "           Conv2d-40            [-1, 512, 4, 4]       2,359,808\n",
            "            Gates-41            [-1, 512, 4, 4]             512\n",
            "      BatchNorm2d-42            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-43            [-1, 512, 4, 4]               0\n",
            "        MaxPool2d-44            [-1, 512, 2, 2]               0\n",
            "           Conv2d-45            [-1, 512, 2, 2]       2,359,808\n",
            "            Gates-46            [-1, 512, 2, 2]             512\n",
            "      BatchNorm2d-47            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-48            [-1, 512, 2, 2]               0\n",
            "           Conv2d-49            [-1, 512, 2, 2]       2,359,808\n",
            "            Gates-50            [-1, 512, 2, 2]             512\n",
            "      BatchNorm2d-51            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-52            [-1, 512, 2, 2]               0\n",
            "           Conv2d-53            [-1, 512, 2, 2]       2,359,808\n",
            "            Gates-54            [-1, 512, 2, 2]             512\n",
            "      BatchNorm2d-55            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-56            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-57            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-58            [-1, 512, 1, 1]               0\n",
            "           Linear-59                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 14,732,490\n",
            "Trainable params: 14,728,266\n",
            "Non-trainable params: 4,224\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 8.68\n",
            "Params size (MB): 56.20\n",
            "Estimated Total Size (MB): 64.89\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the network\n",
        "This section is not graded"
      ],
      "metadata": {
        "id": "IhenPsNAyDau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
      ],
      "metadata": {
        "id": "LHLQeDiLdamD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        inputs = Variable(inputs, requires_grad=False)\n",
        "        targets = Variable(targets)\n",
        "        net.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if(batch_idx % 200 == 0):\n",
        "          print(\"Accuracy : \",100.*correct/total,\" Loss : \", train_loss/(batch_idx+1))\n",
        "def test(epoch):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\"Accuracy : \",100.*correct/total,\" Loss : \", test_loss/(batch_idx+1))"
      ],
      "metadata": {
        "id": "IcR-W9I3dseF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+100):\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "POIZJChvdvMy",
        "outputId": "07ae61f7-67e1-4ce6-9760-49887aff917f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Accuracy :  16.0  Loss :  2.4564666748046875\n",
            "Accuracy :  41.84577114427861  Loss :  1.5765447747054977\n",
            "Accuracy :  48.428927680798004  Loss :  1.4077702582328397\n",
            "Accuracy :  57.0  Loss :  1.1009331941604614\n",
            "Accuracy :  56.76190476190476  Loss :  1.1796199565842038\n",
            "Accuracy :  58.0  Loss :  1.1869256583655752\n",
            "Accuracy :  57.91803278688525  Loss :  1.1912383749836781\n",
            "Accuracy :  58.19753086419753  Loss :  1.191373849356616\n",
            "\n",
            "Epoch: 2\n",
            "Accuracy :  65.0  Loss :  0.9885735511779785\n",
            "Accuracy :  64.95522388059702  Loss :  0.988233048227889\n",
            "Accuracy :  66.68827930174564  Loss :  0.9433682168511084\n",
            "Accuracy :  66.0  Loss :  1.0127038955688477\n",
            "Accuracy :  66.23809523809524  Loss :  1.0186041196187336\n",
            "Accuracy :  65.82926829268293  Loss :  1.01512706570509\n",
            "Accuracy :  65.72131147540983  Loss :  1.0249059112345587\n",
            "Accuracy :  65.9753086419753  Loss :  1.0140491172119424\n",
            "\n",
            "Epoch: 3\n",
            "Accuracy :  77.0  Loss :  0.7989439368247986\n",
            "Accuracy :  72.8407960199005  Loss :  0.7832491132453899\n",
            "Accuracy :  73.53117206982543  Loss :  0.7603522145242763\n",
            "Accuracy :  73.0  Loss :  0.749925434589386\n",
            "Accuracy :  73.95238095238095  Loss :  0.7422906359036764\n",
            "Accuracy :  73.53658536585365  Loss :  0.7590719010771775\n",
            "Accuracy :  73.54098360655738  Loss :  0.758463481410605\n",
            "Accuracy :  73.75308641975309  Loss :  0.7481040189295639\n",
            "\n",
            "Epoch: 4\n",
            "Accuracy :  80.0  Loss :  0.6205769181251526\n",
            "Accuracy :  76.70149253731343  Loss :  0.6663816916705364\n",
            "Accuracy :  76.95511221945137  Loss :  0.6600840916657388\n",
            "Accuracy :  81.0  Loss :  0.5627837181091309\n",
            "Accuracy :  79.19047619047619  Loss :  0.6088543974217915\n",
            "Accuracy :  78.36585365853658  Loss :  0.6274693092195\n",
            "Accuracy :  78.57377049180327  Loss :  0.620518438640188\n",
            "Accuracy :  78.65432098765432  Loss :  0.6119221989755277\n",
            "\n",
            "Epoch: 5\n",
            "Accuracy :  83.0  Loss :  0.4635070860385895\n",
            "Accuracy :  79.96019900497512  Loss :  0.5813103290043067\n",
            "Accuracy :  79.83291770573567  Loss :  0.5820020614122214\n",
            "Accuracy :  85.0  Loss :  0.566796064376831\n",
            "Accuracy :  79.61904761904762  Loss :  0.5693397082033611\n",
            "Accuracy :  79.85365853658537  Loss :  0.5793536269083256\n",
            "Accuracy :  79.88524590163935  Loss :  0.5766756993825318\n",
            "Accuracy :  79.95061728395062  Loss :  0.5771082525635943\n",
            "\n",
            "Epoch: 6\n",
            "Accuracy :  85.0  Loss :  0.5288444757461548\n",
            "Accuracy :  81.53731343283582  Loss :  0.5304514629330801\n",
            "Accuracy :  81.52119700748129  Loss :  0.531036525667457\n",
            "Accuracy :  85.0  Loss :  0.5567317605018616\n",
            "Accuracy :  81.95238095238095  Loss :  0.5296477618671599\n",
            "Accuracy :  81.2439024390244  Loss :  0.5577606478842293\n",
            "Accuracy :  81.21311475409836  Loss :  0.5546529078092731\n",
            "Accuracy :  81.58024691358025  Loss :  0.5458474666983993\n",
            "\n",
            "Epoch: 7\n",
            "Accuracy :  83.0  Loss :  0.481428861618042\n",
            "Accuracy :  83.27860696517413  Loss :  0.4831504651266544\n",
            "Accuracy :  83.13216957605985  Loss :  0.48973584814261917\n",
            "Accuracy :  79.0  Loss :  0.4943227469921112\n",
            "Accuracy :  82.0952380952381  Loss :  0.5317756647155398\n",
            "Accuracy :  82.48780487804878  Loss :  0.5292211087738595\n",
            "Accuracy :  82.8688524590164  Loss :  0.5182395592087605\n",
            "Accuracy :  83.03703703703704  Loss :  0.513879339636108\n",
            "\n",
            "Epoch: 8\n",
            "Accuracy :  92.0  Loss :  0.2146676778793335\n",
            "Accuracy :  84.60199004975124  Loss :  0.4453805372193085\n",
            "Accuracy :  84.43142144638404  Loss :  0.4492612712252467\n",
            "Accuracy :  82.0  Loss :  0.5292035937309265\n",
            "Accuracy :  81.9047619047619  Loss :  0.5209010356948489\n",
            "Accuracy :  81.6829268292683  Loss :  0.5408919562653798\n",
            "Accuracy :  82.01639344262296  Loss :  0.5299808817808745\n",
            "Accuracy :  82.14814814814815  Loss :  0.5231350549945125\n",
            "\n",
            "Epoch: 9\n",
            "Accuracy :  86.0  Loss :  0.37776896357536316\n",
            "Accuracy :  86.0049751243781  Loss :  0.40845658470742147\n",
            "Accuracy :  85.98753117206982  Loss :  0.4078692889347338\n",
            "Accuracy :  87.0  Loss :  0.39837008714675903\n",
            "Accuracy :  84.47619047619048  Loss :  0.4551666818913959\n",
            "Accuracy :  84.36585365853658  Loss :  0.47585466576785573\n",
            "Accuracy :  84.19672131147541  Loss :  0.475435665884956\n",
            "Accuracy :  84.11111111111111  Loss :  0.4692379869060752\n",
            "\n",
            "Epoch: 10\n",
            "Accuracy :  86.0  Loss :  0.38918331265449524\n",
            "Accuracy :  86.6318407960199  Loss :  0.3856517823774423\n",
            "Accuracy :  86.66832917705736  Loss :  0.3868063797157007\n",
            "Accuracy :  82.0  Loss :  0.5268206596374512\n",
            "Accuracy :  83.0952380952381  Loss :  0.5077622688951946\n",
            "Accuracy :  82.92682926829268  Loss :  0.5081049523702482\n",
            "Accuracy :  83.01639344262296  Loss :  0.5078351737045851\n",
            "Accuracy :  83.17283950617283  Loss :  0.5068542004367451\n",
            "\n",
            "Epoch: 11\n",
            "Accuracy :  93.0  Loss :  0.21336138248443604\n",
            "Accuracy :  88.07960199004975  Loss :  0.35525593302439695\n",
            "Accuracy :  87.70822942643392  Loss :  0.3620013201474549\n",
            "Accuracy :  90.0  Loss :  0.3833639919757843\n",
            "Accuracy :  85.04761904761905  Loss :  0.4288079327061063\n",
            "Accuracy :  85.34146341463415  Loss :  0.4433709840948989\n",
            "Accuracy :  85.31147540983606  Loss :  0.4372781695889645\n",
            "Accuracy :  85.48148148148148  Loss :  0.4341595658549556\n",
            "\n",
            "Epoch: 12\n",
            "Accuracy :  93.0  Loss :  0.20390233397483826\n",
            "Accuracy :  88.41293532338308  Loss :  0.3385544580903219\n",
            "Accuracy :  88.51620947630923  Loss :  0.3346731749704651\n",
            "Accuracy :  83.0  Loss :  0.4860391914844513\n",
            "Accuracy :  84.38095238095238  Loss :  0.46472925941149396\n",
            "Accuracy :  84.4390243902439  Loss :  0.4831433267128177\n",
            "Accuracy :  84.49180327868852  Loss :  0.47663287891716255\n",
            "Accuracy :  84.55555555555556  Loss :  0.4761965870857239\n",
            "\n",
            "Epoch: 13\n",
            "Accuracy :  86.0  Loss :  0.38563400506973267\n",
            "Accuracy :  89.00995024875621  Loss :  0.3101386194353673\n",
            "Accuracy :  89.02992518703242  Loss :  0.31507777617756566\n",
            "Accuracy :  86.0  Loss :  0.40564480423927307\n",
            "Accuracy :  86.66666666666667  Loss :  0.41964550954954966\n",
            "Accuracy :  85.63414634146342  Loss :  0.4462957600267922\n",
            "Accuracy :  85.57377049180327  Loss :  0.44004715270683414\n",
            "Accuracy :  85.79012345679013  Loss :  0.43434706845401244\n",
            "\n",
            "Epoch: 14\n",
            "Accuracy :  86.0  Loss :  0.41322603821754456\n",
            "Accuracy :  90.13432835820896  Loss :  0.28756992826562616\n",
            "Accuracy :  89.71571072319202  Loss :  0.2974294888185444\n",
            "Accuracy :  88.0  Loss :  0.4343438744544983\n",
            "Accuracy :  85.0  Loss :  0.46839468961670283\n",
            "Accuracy :  84.6829268292683  Loss :  0.4747051632985836\n",
            "Accuracy :  84.88524590163935  Loss :  0.4628070749220301\n",
            "Accuracy :  84.9753086419753  Loss :  0.4660594360328015\n",
            "\n",
            "Epoch: 15\n",
            "Accuracy :  88.0  Loss :  0.3034525215625763\n",
            "Accuracy :  90.5223880597015  Loss :  0.2711615551318695\n",
            "Accuracy :  90.49376558603491  Loss :  0.2747017427201283\n",
            "Accuracy :  88.0  Loss :  0.3340236246585846\n",
            "Accuracy :  86.61904761904762  Loss :  0.43999292027382625\n",
            "Accuracy :  85.70731707317073  Loss :  0.4535106871186233\n",
            "Accuracy :  85.62295081967213  Loss :  0.44759930059558056\n",
            "Accuracy :  85.34567901234568  Loss :  0.45401656186139144\n",
            "\n",
            "Epoch: 16\n",
            "Accuracy :  94.0  Loss :  0.23623506724834442\n",
            "Accuracy :  91.08955223880596  Loss :  0.2576950679519283\n",
            "Accuracy :  90.88029925187033  Loss :  0.2640683956239883\n",
            "Accuracy :  88.0  Loss :  0.4259376525878906\n",
            "Accuracy :  86.33333333333333  Loss :  0.44416587835266474\n",
            "Accuracy :  86.04878048780488  Loss :  0.4543145924079709\n",
            "Accuracy :  85.73770491803279  Loss :  0.4487617240577448\n",
            "Accuracy :  85.60493827160494  Loss :  0.4479869548921232\n",
            "\n",
            "Epoch: 17\n",
            "Accuracy :  92.0  Loss :  0.1970454454421997\n",
            "Accuracy :  91.86069651741293  Loss :  0.23684916311680382\n",
            "Accuracy :  91.53366583541147  Loss :  0.24579167209955818\n",
            "Accuracy :  88.0  Loss :  0.32966935634613037\n",
            "Accuracy :  85.14285714285714  Loss :  0.4687980328287397\n",
            "Accuracy :  84.8048780487805  Loss :  0.47835517147692236\n",
            "Accuracy :  84.68852459016394  Loss :  0.4842420418242939\n",
            "Accuracy :  84.66666666666667  Loss :  0.48335849153406824\n",
            "\n",
            "Epoch: 18\n",
            "Accuracy :  91.0  Loss :  0.20534895360469818\n",
            "Accuracy :  91.68656716417911  Loss :  0.2375626086224964\n",
            "Accuracy :  91.68079800498754  Loss :  0.23753974965757266\n",
            "Accuracy :  81.0  Loss :  0.4643273949623108\n",
            "Accuracy :  85.57142857142857  Loss :  0.44813876492636545\n",
            "Accuracy :  85.5609756097561  Loss :  0.46372899631174597\n",
            "Accuracy :  85.47540983606558  Loss :  0.45564872539434276\n",
            "Accuracy :  85.67901234567901  Loss :  0.449881919188264\n",
            "\n",
            "Epoch: 19\n",
            "Accuracy :  93.0  Loss :  0.2398308515548706\n",
            "Accuracy :  92.48258706467662  Loss :  0.21691793047670108\n",
            "Accuracy :  92.43890274314215  Loss :  0.21939529318761944\n",
            "Accuracy :  84.0  Loss :  0.4857548177242279\n",
            "Accuracy :  84.47619047619048  Loss :  0.4995277055672237\n",
            "Accuracy :  84.48780487804878  Loss :  0.511244832742505\n",
            "Accuracy :  84.59016393442623  Loss :  0.5126957077471936\n",
            "Accuracy :  84.5925925925926  Loss :  0.5133835390026187\n",
            "\n",
            "Epoch: 20\n",
            "Accuracy :  95.0  Loss :  0.198704794049263\n",
            "Accuracy :  92.43781094527363  Loss :  0.2121395831603316\n",
            "Accuracy :  92.61845386533666  Loss :  0.21048462441065663\n",
            "Accuracy :  87.0  Loss :  0.42324477434158325\n",
            "Accuracy :  85.66666666666667  Loss :  0.4859350919723511\n",
            "Accuracy :  85.41463414634147  Loss :  0.4907339142590034\n",
            "Accuracy :  85.57377049180327  Loss :  0.4858002569831786\n",
            "Accuracy :  85.58024691358025  Loss :  0.4812641883337939\n",
            "\n",
            "Epoch: 21\n",
            "Accuracy :  96.0  Loss :  0.12642674148082733\n",
            "Accuracy :  93.46766169154229  Loss :  0.18988212639123053\n",
            "Accuracy :  93.2219451371571  Loss :  0.19584849251996253\n",
            "Accuracy :  83.0  Loss :  0.49655085802078247\n",
            "Accuracy :  85.61904761904762  Loss :  0.46660455635615755\n",
            "Accuracy :  85.85365853658537  Loss :  0.4735504076248262\n",
            "Accuracy :  86.0  Loss :  0.46559649412749243\n",
            "Accuracy :  86.19753086419753  Loss :  0.45543852595635403\n",
            "\n",
            "Epoch: 22\n",
            "Accuracy :  94.0  Loss :  0.24028976261615753\n",
            "Accuracy :  93.71641791044776  Loss :  0.1797480165068783\n",
            "Accuracy :  93.50374064837905  Loss :  0.18408231613716283\n",
            "Accuracy :  88.0  Loss :  0.3632112145423889\n",
            "Accuracy :  87.47619047619048  Loss :  0.3909091360512234\n",
            "Accuracy :  87.1219512195122  Loss :  0.4182514282988339\n",
            "Accuracy :  87.42622950819673  Loss :  0.40838056167618175\n",
            "Accuracy :  87.29629629629629  Loss :  0.4130694741084252\n",
            "\n",
            "Epoch: 23\n",
            "Accuracy :  93.0  Loss :  0.29409003257751465\n",
            "Accuracy :  94.01990049751244  Loss :  0.17173277425454625\n",
            "Accuracy :  93.82793017456359  Loss :  0.17801906407213866\n",
            "Accuracy :  86.0  Loss :  0.34287741780281067\n",
            "Accuracy :  86.85714285714286  Loss :  0.4329194781326112\n",
            "Accuracy :  87.1219512195122  Loss :  0.4409942939514067\n",
            "Accuracy :  86.9672131147541  Loss :  0.43452905924593815\n",
            "Accuracy :  86.90123456790124  Loss :  0.43501084601437606\n",
            "\n",
            "Epoch: 24\n",
            "Accuracy :  96.0  Loss :  0.10805313289165497\n",
            "Accuracy :  94.35820895522389  Loss :  0.16176624673960813\n",
            "Accuracy :  94.22693266832918  Loss :  0.16734197299156403\n",
            "Accuracy :  92.0  Loss :  0.3188166320323944\n",
            "Accuracy :  87.80952380952381  Loss :  0.3838441485450381\n",
            "Accuracy :  87.92682926829268  Loss :  0.39358082559050583\n",
            "Accuracy :  87.85245901639344  Loss :  0.39083432662682455\n",
            "Accuracy :  87.71604938271605  Loss :  0.3966976915612633\n",
            "\n",
            "Epoch: 25\n",
            "Accuracy :  98.0  Loss :  0.07501719892024994\n",
            "Accuracy :  94.32835820895522  Loss :  0.15858176065173316\n",
            "Accuracy :  94.39152119700748  Loss :  0.15912103912145123\n",
            "Accuracy :  89.0  Loss :  0.37756428122520447\n",
            "Accuracy :  87.9047619047619  Loss :  0.38999536136786145\n",
            "Accuracy :  87.7560975609756  Loss :  0.3980835628945653\n",
            "Accuracy :  87.85245901639344  Loss :  0.39532214647433794\n",
            "Accuracy :  87.72839506172839  Loss :  0.3984441845505326\n",
            "\n",
            "Epoch: 26\n",
            "Accuracy :  97.0  Loss :  0.12839293479919434\n",
            "Accuracy :  95.0  Loss :  0.14337757717243474\n",
            "Accuracy :  94.71571072319202  Loss :  0.1498621826680224\n",
            "Accuracy :  92.0  Loss :  0.2561016380786896\n",
            "Accuracy :  88.23809523809524  Loss :  0.4137279107457116\n",
            "Accuracy :  87.8048780487805  Loss :  0.4295655823335415\n",
            "Accuracy :  87.57377049180327  Loss :  0.4300128167769948\n",
            "Accuracy :  87.49382716049382  Loss :  0.43080680421841\n",
            "\n",
            "Epoch: 27\n",
            "Accuracy :  95.0  Loss :  0.1473633199930191\n",
            "Accuracy :  95.18905472636816  Loss :  0.1358877615030132\n",
            "Accuracy :  95.12468827930175  Loss :  0.13978833939294863\n",
            "Accuracy :  91.0  Loss :  0.34455791115760803\n",
            "Accuracy :  87.85714285714286  Loss :  0.41677666561944143\n",
            "Accuracy :  87.7560975609756  Loss :  0.42109801638417127\n",
            "Accuracy :  87.73770491803279  Loss :  0.4127451862956657\n",
            "Accuracy :  87.69135802469135  Loss :  0.417012576887637\n",
            "\n",
            "Epoch: 28\n",
            "Accuracy :  94.0  Loss :  0.20174504816532135\n",
            "Accuracy :  95.59203980099502  Loss :  0.1290653149101568\n",
            "Accuracy :  95.23192019950125  Loss :  0.13489864177611702\n",
            "Accuracy :  85.0  Loss :  0.46424782276153564\n",
            "Accuracy :  87.76190476190476  Loss :  0.42306886258579435\n",
            "Accuracy :  87.51219512195122  Loss :  0.435646001158691\n",
            "Accuracy :  87.60655737704919  Loss :  0.4228485160675205\n",
            "Accuracy :  87.55555555555556  Loss :  0.42591080419075344\n",
            "\n",
            "Epoch: 29\n",
            "Accuracy :  98.0  Loss :  0.07453319430351257\n",
            "Accuracy :  95.59203980099502  Loss :  0.12965347067411268\n",
            "Accuracy :  95.5860349127182  Loss :  0.12911393958378461\n",
            "Accuracy :  83.0  Loss :  0.4639069437980652\n",
            "Accuracy :  87.33333333333333  Loss :  0.4431440532207489\n",
            "Accuracy :  87.07317073170732  Loss :  0.4515111257390278\n",
            "Accuracy :  86.68852459016394  Loss :  0.4569852889561262\n",
            "Accuracy :  86.82716049382717  Loss :  0.4608378553832019\n",
            "\n",
            "Epoch: 30\n",
            "Accuracy :  97.0  Loss :  0.10167908668518066\n",
            "Accuracy :  95.9950248756219  Loss :  0.1129329961108331\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-69165f3b5f56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-7bc444fd8296>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tljVmGRJItNY",
        "outputId": "9b8b837c-077e-4473-d815-cf6e7843782a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model parameters\n",
        "PATH = \"/content/gdrive/MyDrive/Deep Learning/A5/Part2_net_gates1\"\n",
        "torch.save(net.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "HUG9bkLvIp1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model\n",
        "net = VGG('VGG16')\n",
        "net = net.to(device)\n",
        "net.load_state_dict(torch.load(PATH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "couLq-PZjy9T",
        "outputId": "44a3d260-e88f-4cd7-c60a-79809686bbf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Theoratical pruning\n",
        "2.4 In this cell you will analyze network Conv layer weights, rank them based on absolute weight sums of filters and turn off 30% of lower-ranked filters by changing the corresponding gate values from one to zero. This will virtually prune corresponding filters from the network because the outputs of these filters would be zero and they would not contribute to the output decision. "
      ],
      "metadata": {
        "id": "yHzGbmd9yTQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list all the layers of the feature attribute in init() of the neural network class definition\n",
        "net.features"
      ],
      "metadata": {
        "id": "LpW7cBDlfi9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd3a9fe-e8d5-442f-8ed6-c4b9b683bc58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (1): Gates()\n",
              "  (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (3): ReLU(inplace=True)\n",
              "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (5): Gates()\n",
              "  (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (7): ReLU(inplace=True)\n",
              "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (9): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (10): Gates()\n",
              "  (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (12): ReLU(inplace=True)\n",
              "  (13): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (14): Gates()\n",
              "  (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (16): ReLU(inplace=True)\n",
              "  (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (18): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (19): Gates()\n",
              "  (20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (21): ReLU(inplace=True)\n",
              "  (22): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (23): Gates()\n",
              "  (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (25): ReLU(inplace=True)\n",
              "  (26): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (27): Gates()\n",
              "  (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (29): ReLU(inplace=True)\n",
              "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (31): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (32): Gates()\n",
              "  (33): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (34): ReLU(inplace=True)\n",
              "  (35): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (36): Gates()\n",
              "  (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (38): ReLU(inplace=True)\n",
              "  (39): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (40): Gates()\n",
              "  (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (42): ReLU(inplace=True)\n",
              "  (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (44): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (45): Gates()\n",
              "  (46): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (47): ReLU(inplace=True)\n",
              "  (48): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (49): Gates()\n",
              "  (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (51): ReLU(inplace=True)\n",
              "  (52): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (53): Gates()\n",
              "  (54): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (55): ReLU(inplace=True)\n",
              "  (56): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (57): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def modify_gate_parameter(index_conv2d, turn_off_ratio):\n",
        "  # take sum of filters\n",
        "  a = torch.sum(net.features[index_conv2d].weight, dim=[1,2,3])\n",
        "  # take the absolute value\n",
        "  b = torch.abs(a)\n",
        "\n",
        "  # find index of 30% lower weights\n",
        "  index_list = b.cpu().detach().numpy().argsort()\n",
        "  lower_index_list = index_list[ : int(np.ceil(turn_off_ratio*len(index_list))) ]\n",
        "\n",
        "  # modify gate parameters\n",
        "  for i in range(len(lower_index_list)):\n",
        "      # print(index_conv2d+1)\n",
        "      # print(net.features[index_conv2d+1])\n",
        "      net.features[index_conv2d+1].weight[lower_index_list[i]]=0\n",
        "\n",
        "  # print the new weights\n",
        "  # print(net.features[index_conv2d+1].weight)"
      ],
      "metadata": {
        "id": "qIx40d4lhiw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_conv2d_list = [0,4,9,13,18,22,26,31,35,39,44,48,52]\n",
        "# Modify gates parameters\n",
        "for i in range(len(index_conv2d_list)):\n",
        "  modify_gate_parameter(index_conv2d_list[i], 0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbyzhhNkZoyd",
        "outputId": "f696b82a-0c49-444a-a510-5ca02e76831e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 0., 1., 0.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 1.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 0., 1., 1.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 0.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
            "        0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 1.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 1., 0.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
            "        0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 0.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 1.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
            "        0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 0.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
            "        1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
            "        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 1.], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Finetune\n",
        "Finetune the network for a few epochs (not graded)"
      ],
      "metadata": {
        "id": "Ij7ViGBw5rir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        inputs = Variable(inputs, requires_grad=False)\n",
        "        targets = Variable(targets)\n",
        "        \n",
        "        net.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if(batch_idx % 200 == 0):\n",
        "          print(\"Accuracy : \",100.*correct/total,\" Loss : \", train_loss/(batch_idx+1))\n",
        "def test(epoch):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\"Accuracy : \",100.*correct/total,\" Loss : \", test_loss/(batch_idx+1))"
      ],
      "metadata": {
        "id": "25oi7_zOmBmj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+10):\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ],
      "metadata": {
        "id": "YNBiExm6mG5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03203de2-c974-4d7c-d1ad-a4b75c141cad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Accuracy :  45.0  Loss :  2.2768611907958984\n",
            "Accuracy :  48.39303482587065  Loss :  2.022524858588603\n",
            "Accuracy :  48.1072319201995  Loss :  2.0293892063107575\n",
            "Accuracy :  41.0  Loss :  2.5864901542663574\n",
            "Accuracy :  47.04761904761905  Loss :  2.157105264209566\n",
            "Accuracy :  47.4390243902439  Loss :  2.1351999974832303\n",
            "Accuracy :  47.114754098360656  Loss :  2.130868278565954\n",
            "Accuracy :  47.30864197530864  Loss :  2.1229199583147778\n",
            "\n",
            "Epoch: 2\n",
            "Accuracy :  44.0  Loss :  2.327136754989624\n",
            "Accuracy :  48.149253731343286  Loss :  2.011521842349228\n",
            "Accuracy :  47.96508728179551  Loss :  2.029207182941294\n",
            "Accuracy :  42.0  Loss :  2.5634584426879883\n",
            "Accuracy :  47.76190476190476  Loss :  2.142910463469369\n",
            "Accuracy :  47.97560975609756  Loss :  2.1244728216310826\n",
            "Accuracy :  47.59016393442623  Loss :  2.12055097447067\n",
            "Accuracy :  47.876543209876544  Loss :  2.1121511694825728\n",
            "\n",
            "Epoch: 3\n",
            "Accuracy :  51.0  Loss :  1.8707232475280762\n",
            "Accuracy :  47.82587064676617  Loss :  2.0381989390102784\n",
            "Accuracy :  47.87032418952619  Loss :  2.0387506128249324\n",
            "Accuracy :  42.0  Loss :  2.5643692016601562\n",
            "Accuracy :  47.333333333333336  Loss :  2.1369151728493825\n",
            "Accuracy :  47.80487804878049  Loss :  2.1165292989916917\n",
            "Accuracy :  47.459016393442624  Loss :  2.1116618308864656\n",
            "Accuracy :  47.72839506172839  Loss :  2.1032072364548107\n",
            "\n",
            "Epoch: 4\n",
            "Accuracy :  52.0  Loss :  1.7131454944610596\n",
            "Accuracy :  47.44776119402985  Loss :  2.0577812254132324\n",
            "Accuracy :  47.586034912718205  Loss :  2.0378395423032996\n",
            "Accuracy :  43.0  Loss :  2.568862199783325\n",
            "Accuracy :  47.333333333333336  Loss :  2.1459267196201144\n",
            "Accuracy :  47.73170731707317  Loss :  2.1229427791223294\n",
            "Accuracy :  47.42622950819672  Loss :  2.1190062292286607\n",
            "Accuracy :  47.5679012345679  Loss :  2.110751509666443\n",
            "\n",
            "Epoch: 5\n",
            "Accuracy :  45.0  Loss :  2.384467124938965\n",
            "Accuracy :  48.039800995024876  Loss :  2.0228589417329474\n",
            "Accuracy :  48.094763092269325  Loss :  2.0209655746854747\n",
            "Accuracy :  42.0  Loss :  2.585996627807617\n",
            "Accuracy :  47.42857142857143  Loss :  2.16514283702487\n",
            "Accuracy :  47.390243902439025  Loss :  2.1454591344042524\n",
            "Accuracy :  47.18032786885246  Loss :  2.142487989097345\n",
            "Accuracy :  47.45679012345679  Loss :  2.134219496338456\n",
            "\n",
            "Epoch: 6\n",
            "Accuracy :  43.0  Loss :  2.1980130672454834\n",
            "Accuracy :  47.840796019900495  Loss :  2.02898299516137\n",
            "Accuracy :  47.910224438902745  Loss :  2.0313228368759155\n",
            "Accuracy :  42.0  Loss :  2.583977460861206\n",
            "Accuracy :  47.57142857142857  Loss :  2.1427395116715204\n",
            "Accuracy :  47.829268292682926  Loss :  2.119504789026772\n",
            "Accuracy :  47.540983606557376  Loss :  2.1145945025272055\n",
            "Accuracy :  47.71604938271605  Loss :  2.1062075547230097\n",
            "\n",
            "Epoch: 7\n",
            "Accuracy :  54.0  Loss :  2.1691036224365234\n",
            "Accuracy :  48.124378109452735  Loss :  2.0236356833680946\n",
            "Accuracy :  48.047381546134666  Loss :  2.023295558598868\n",
            "Accuracy :  42.0  Loss :  2.565622568130493\n",
            "Accuracy :  47.476190476190474  Loss :  2.137072205543518\n",
            "Accuracy :  47.829268292682926  Loss :  2.1148303601799943\n",
            "Accuracy :  47.47540983606557  Loss :  2.1099287994572373\n",
            "Accuracy :  47.7037037037037  Loss :  2.101591854919622\n",
            "\n",
            "Epoch: 8\n",
            "Accuracy :  45.0  Loss :  2.277552843093872\n",
            "Accuracy :  48.25373134328358  Loss :  2.013736127027825\n",
            "Accuracy :  47.942643391521194  Loss :  2.0265542179569045\n",
            "Accuracy :  42.0  Loss :  2.589376211166382\n",
            "Accuracy :  47.04761904761905  Loss :  2.1681485800516036\n",
            "Accuracy :  47.390243902439025  Loss :  2.1464598411467017\n",
            "Accuracy :  47.26229508196721  Loss :  2.1407612718519617\n",
            "Accuracy :  47.50617283950617  Loss :  2.132774829864502\n",
            "\n",
            "Epoch: 9\n",
            "Accuracy :  52.0  Loss :  1.865997314453125\n",
            "Accuracy :  47.691542288557216  Loss :  2.0503082008504157\n",
            "Accuracy :  48.084788029925186  Loss :  2.0397812089420615\n",
            "Accuracy :  41.0  Loss :  2.5885772705078125\n",
            "Accuracy :  47.61904761904762  Loss :  2.1607972042901173\n",
            "Accuracy :  47.65853658536585  Loss :  2.140497838578573\n",
            "Accuracy :  47.34426229508197  Loss :  2.135552162029704\n",
            "Accuracy :  47.50617283950617  Loss :  2.1276875101489785\n",
            "\n",
            "Epoch: 10\n",
            "Accuracy :  57.0  Loss :  1.797592043876648\n",
            "Accuracy :  48.38308457711443  Loss :  2.0124479704235325\n",
            "Accuracy :  48.25935162094763  Loss :  2.0267811451766855\n",
            "Accuracy :  43.0  Loss :  2.575012445449829\n",
            "Accuracy :  47.19047619047619  Loss :  2.1503762233824957\n",
            "Accuracy :  47.53658536585366  Loss :  2.128482847678952\n",
            "Accuracy :  47.26229508196721  Loss :  2.123355658327947\n",
            "Accuracy :  47.5679012345679  Loss :  2.115274990046466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model parameters\n",
        "PATH = \"/content/gdrive/MyDrive/Deep Learning/A5/Part2_net_gates2\"\n",
        "torch.save(net.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "zD31WyyHpOCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model\n",
        "net = VGG('VGG16')\n",
        "PATH = \"/content/gdrive/MyDrive/Deep Learning/A5/Part2_net_gates2\"\n",
        "net = net.to(device)\n",
        "net.load_state_dict(torch.load(PATH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d1Zk7bZWdyN",
        "outputId": "272864b5-a7a3-4eec-c964-19c11da6d4c6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train model for more epochs"
      ],
      "metadata": {
        "id": "94bVyFjrWzFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 10\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+60):\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9y_t8tbSW1lE",
        "outputId": "332bcf19-8db4-4577-a15e-9b209cd08716"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 11\n",
            "Accuracy :  48.0  Loss :  1.8109130859375\n",
            "Accuracy :  48.233830845771145  Loss :  2.037980154972171\n",
            "Accuracy :  48.45635910224439  Loss :  2.0158744128862223\n",
            "Accuracy :  41.0  Loss :  2.576968193054199\n",
            "Accuracy :  47.285714285714285  Loss :  2.159459795270647\n",
            "Accuracy :  47.609756097560975  Loss :  2.1370080738532833\n",
            "Accuracy :  47.32786885245902  Loss :  2.1316490779157546\n",
            "Accuracy :  47.51851851851852  Loss :  2.1237504614724054\n",
            "\n",
            "Epoch: 12\n",
            "Accuracy :  53.0  Loss :  1.785770058631897\n",
            "Accuracy :  48.33830845771144  Loss :  1.9992235163551064\n",
            "Accuracy :  47.95511221945137  Loss :  2.024052731116811\n",
            "Accuracy :  44.0  Loss :  2.5647263526916504\n",
            "Accuracy :  47.57142857142857  Loss :  2.152383327484131\n",
            "Accuracy :  47.73170731707317  Loss :  2.13118271711396\n",
            "Accuracy :  47.47540983606557  Loss :  2.126424623317406\n",
            "Accuracy :  47.72839506172839  Loss :  2.117811158851341\n",
            "\n",
            "Epoch: 13\n",
            "Accuracy :  45.0  Loss :  2.0962471961975098\n",
            "Accuracy :  47.343283582089555  Loss :  2.0666941985562075\n",
            "Accuracy :  47.51620947630923  Loss :  2.0507935130982626\n",
            "Accuracy :  42.0  Loss :  2.585583209991455\n",
            "Accuracy :  47.04761904761905  Loss :  2.1599236669994535\n",
            "Accuracy :  47.4390243902439  Loss :  2.1378068197064284\n",
            "Accuracy :  47.22950819672131  Loss :  2.133843144432443\n",
            "Accuracy :  47.382716049382715  Loss :  2.125799161416513\n",
            "\n",
            "Epoch: 14\n",
            "Accuracy :  46.0  Loss :  1.8947670459747314\n",
            "Accuracy :  47.950248756218905  Loss :  2.0279976313386983\n",
            "Accuracy :  47.99251870324189  Loss :  2.0334336290930275\n",
            "Accuracy :  43.0  Loss :  2.605776786804199\n",
            "Accuracy :  47.0  Loss :  2.1835683697745916\n",
            "Accuracy :  47.36585365853659  Loss :  2.160732490260427\n",
            "Accuracy :  47.0655737704918  Loss :  2.1561513982835363\n",
            "Accuracy :  47.23456790123457  Loss :  2.147329342218093\n",
            "\n",
            "Epoch: 15\n",
            "Accuracy :  46.0  Loss :  2.182659864425659\n",
            "Accuracy :  48.55721393034826  Loss :  1.9989511741334527\n",
            "Accuracy :  48.204488778054866  Loss :  2.0231678167780736\n",
            "Accuracy :  43.0  Loss :  2.579984426498413\n",
            "Accuracy :  47.714285714285715  Loss :  2.139015799476987\n",
            "Accuracy :  47.80487804878049  Loss :  2.1169103471244255\n",
            "Accuracy :  47.50819672131148  Loss :  2.1130812402631416\n",
            "Accuracy :  47.81481481481482  Loss :  2.104220353526834\n",
            "\n",
            "Epoch: 16\n",
            "Accuracy :  50.0  Loss :  1.8855323791503906\n",
            "Accuracy :  48.09452736318408  Loss :  2.017891693471083\n",
            "Accuracy :  47.68329177057357  Loss :  2.035649910829311\n",
            "Accuracy :  42.0  Loss :  2.561877489089966\n",
            "Accuracy :  47.42857142857143  Loss :  2.1449306295031594\n",
            "Accuracy :  47.58536585365854  Loss :  2.1261185669317477\n",
            "Accuracy :  47.34426229508197  Loss :  2.121254919005222\n",
            "Accuracy :  47.592592592592595  Loss :  2.113717832683045\n",
            "\n",
            "Epoch: 17\n",
            "Accuracy :  41.0  Loss :  2.2398428916931152\n",
            "Accuracy :  47.62189054726368  Loss :  2.037019578378592\n",
            "Accuracy :  47.93516209476309  Loss :  2.028747797012329\n",
            "Accuracy :  43.0  Loss :  2.5424344539642334\n",
            "Accuracy :  47.76190476190476  Loss :  2.132471720377604\n",
            "Accuracy :  48.048780487804876  Loss :  2.1116616028111155\n",
            "Accuracy :  47.67213114754098  Loss :  2.106370243869844\n",
            "Accuracy :  47.98765432098765  Loss :  2.0985208470144388\n",
            "\n",
            "Epoch: 18\n",
            "Accuracy :  38.0  Loss :  2.241574287414551\n",
            "Accuracy :  47.97014925373134  Loss :  2.047972151889137\n",
            "Accuracy :  47.86783042394015  Loss :  2.0451234763399917\n",
            "Accuracy :  42.0  Loss :  2.5677568912506104\n",
            "Accuracy :  47.523809523809526  Loss :  2.140456171262832\n",
            "Accuracy :  48.0  Loss :  2.1216278163398186\n",
            "Accuracy :  47.721311475409834  Loss :  2.116244198846035\n",
            "Accuracy :  47.95061728395062  Loss :  2.1084280911787054\n",
            "\n",
            "Epoch: 19\n",
            "Accuracy :  49.0  Loss :  1.9361631870269775\n",
            "Accuracy :  48.19402985074627  Loss :  2.008406400680542\n",
            "Accuracy :  47.80798004987531  Loss :  2.029627226831907\n",
            "Accuracy :  43.0  Loss :  2.587442636489868\n",
            "Accuracy :  46.904761904761905  Loss :  2.1654119605109807\n",
            "Accuracy :  47.4390243902439  Loss :  2.1433921848855366\n",
            "Accuracy :  47.24590163934426  Loss :  2.138190511797295\n",
            "Accuracy :  47.51851851851852  Loss :  2.1290989967040073\n",
            "\n",
            "Epoch: 20\n",
            "Accuracy :  54.0  Loss :  2.0135912895202637\n",
            "Accuracy :  48.49751243781095  Loss :  2.008861877431917\n",
            "Accuracy :  48.35910224438903  Loss :  2.0211846846297497\n",
            "Accuracy :  43.0  Loss :  2.593205213546753\n",
            "Accuracy :  47.04761904761905  Loss :  2.1744126081466675\n",
            "Accuracy :  47.41463414634146  Loss :  2.152175691069626\n",
            "Accuracy :  47.16393442622951  Loss :  2.1481564592142575\n",
            "Accuracy :  47.407407407407405  Loss :  2.139934589833389\n",
            "\n",
            "Epoch: 21\n",
            "Accuracy :  42.0  Loss :  2.198077917098999\n",
            "Accuracy :  48.0547263681592  Loss :  2.0203944225216386\n",
            "Accuracy :  47.70324189526185  Loss :  2.0435492614855493\n",
            "Accuracy :  43.0  Loss :  2.581998586654663\n",
            "Accuracy :  47.0  Loss :  2.160341909953526\n",
            "Accuracy :  47.390243902439025  Loss :  2.1402677442969344\n",
            "Accuracy :  47.131147540983605  Loss :  2.1365178178568356\n",
            "Accuracy :  47.34567901234568  Loss :  2.1292806760764416\n",
            "\n",
            "Epoch: 22\n",
            "Accuracy :  48.0  Loss :  1.9347845315933228\n",
            "Accuracy :  48.14427860696517  Loss :  2.03316014026528\n",
            "Accuracy :  47.895261845386536  Loss :  2.0389548248780933\n",
            "Accuracy :  42.0  Loss :  2.569164991378784\n",
            "Accuracy :  47.523809523809526  Loss :  2.146780190013704\n",
            "Accuracy :  47.80487804878049  Loss :  2.126901585881303\n",
            "Accuracy :  47.50819672131148  Loss :  2.121372928384875\n",
            "Accuracy :  47.839506172839506  Loss :  2.1133725731461137\n",
            "\n",
            "Epoch: 23\n",
            "Accuracy :  45.0  Loss :  2.3286004066467285\n",
            "Accuracy :  48.35820895522388  Loss :  2.02398261798555\n",
            "Accuracy :  47.80299251870324  Loss :  2.041147768051547\n",
            "Accuracy :  43.0  Loss :  2.5604326725006104\n",
            "Accuracy :  47.23809523809524  Loss :  2.1451677197501775\n",
            "Accuracy :  47.63414634146341  Loss :  2.124137555680624\n",
            "Accuracy :  47.442622950819676  Loss :  2.119414728195941\n",
            "Accuracy :  47.69135802469136  Loss :  2.1099361681643827\n",
            "\n",
            "Epoch: 24\n",
            "Accuracy :  46.0  Loss :  1.9859533309936523\n",
            "Accuracy :  47.91542288557214  Loss :  2.0355129348697947\n",
            "Accuracy :  48.12967581047381  Loss :  2.02859328304443\n",
            "Accuracy :  43.0  Loss :  2.5603673458099365\n",
            "Accuracy :  47.476190476190474  Loss :  2.1397744644255865\n",
            "Accuracy :  47.829268292682926  Loss :  2.1168309653677593\n",
            "Accuracy :  47.52459016393443  Loss :  2.110986979281316\n",
            "Accuracy :  47.77777777777778  Loss :  2.103174055064166\n",
            "\n",
            "Epoch: 25\n",
            "Accuracy :  45.0  Loss :  2.0289833545684814\n",
            "Accuracy :  47.69651741293532  Loss :  2.042187710306538\n",
            "Accuracy :  47.937655860349125  Loss :  2.030102887653056\n",
            "Accuracy :  42.0  Loss :  2.6010186672210693\n",
            "Accuracy :  46.476190476190474  Loss :  2.174736193248204\n",
            "Accuracy :  47.19512195121951  Loss :  2.152073299012533\n",
            "Accuracy :  46.967213114754095  Loss :  2.147526502609253\n",
            "Accuracy :  47.25925925925926  Loss :  2.1393129560682507\n",
            "\n",
            "Epoch: 26\n",
            "Accuracy :  50.0  Loss :  1.841219425201416\n",
            "Accuracy :  48.243781094527364  Loss :  2.026322087245201\n",
            "Accuracy :  47.86533665835412  Loss :  2.033069770532356\n",
            "Accuracy :  42.0  Loss :  2.579465627670288\n",
            "Accuracy :  47.19047619047619  Loss :  2.1526474214735485\n",
            "Accuracy :  47.609756097560975  Loss :  2.1312777792535176\n",
            "Accuracy :  47.36065573770492  Loss :  2.1266776049723393\n",
            "Accuracy :  47.592592592592595  Loss :  2.118551245442143\n",
            "\n",
            "Epoch: 27\n",
            "Accuracy :  56.0  Loss :  1.7780565023422241\n",
            "Accuracy :  47.81592039800995  Loss :  2.0259200459095967\n",
            "Accuracy :  47.86284289276808  Loss :  2.0334379717596156\n",
            "Accuracy :  43.0  Loss :  2.579878568649292\n",
            "Accuracy :  47.57142857142857  Loss :  2.1596848851158503\n",
            "Accuracy :  47.65853658536585  Loss :  2.1390955186471707\n",
            "Accuracy :  47.40983606557377  Loss :  2.134265772631911\n",
            "Accuracy :  47.54320987654321  Loss :  2.126837256513996\n",
            "\n",
            "Epoch: 28\n",
            "Accuracy :  54.0  Loss :  1.6518430709838867\n",
            "Accuracy :  48.21393034825871  Loss :  2.006364037148395\n",
            "Accuracy :  48.11720698254364  Loss :  2.0226668488057773\n",
            "Accuracy :  43.0  Loss :  2.5730597972869873\n",
            "Accuracy :  47.142857142857146  Loss :  2.1500090190342496\n",
            "Accuracy :  47.48780487804878  Loss :  2.12852378589351\n",
            "Accuracy :  47.32786885245902  Loss :  2.1239277788850126\n",
            "Accuracy :  47.629629629629626  Loss :  2.115566040262764\n",
            "\n",
            "Epoch: 29\n",
            "Accuracy :  44.0  Loss :  2.342613935470581\n",
            "Accuracy :  48.21890547263681  Loss :  2.0148335612235377\n",
            "Accuracy :  48.204488778054866  Loss :  2.0212596741103175\n",
            "Accuracy :  42.0  Loss :  2.5740997791290283\n",
            "Accuracy :  47.42857142857143  Loss :  2.151978606269473\n",
            "Accuracy :  47.707317073170735  Loss :  2.1281802363511995\n",
            "Accuracy :  47.49180327868852  Loss :  2.1251671236069476\n",
            "Accuracy :  47.7037037037037  Loss :  2.116399988716031\n",
            "\n",
            "Epoch: 30\n",
            "Accuracy :  50.0  Loss :  2.0421857833862305\n",
            "Accuracy :  48.04477611940298  Loss :  2.0420257547008456\n",
            "Accuracy :  48.00748129675811  Loss :  2.0395724913128594\n",
            "Accuracy :  43.0  Loss :  2.590658187866211\n",
            "Accuracy :  47.38095238095238  Loss :  2.170369613738287\n",
            "Accuracy :  47.63414634146341  Loss :  2.1508997969511077\n",
            "Accuracy :  47.26229508196721  Loss :  2.147020420090097\n",
            "Accuracy :  47.55555555555556  Loss :  2.14027358867504\n",
            "\n",
            "Epoch: 31\n",
            "Accuracy :  46.0  Loss :  2.0757665634155273\n",
            "Accuracy :  48.25870646766169  Loss :  2.026903787062536\n",
            "Accuracy :  48.04488778054863  Loss :  2.0334770016539423\n",
            "Accuracy :  43.0  Loss :  2.588740587234497\n",
            "Accuracy :  47.666666666666664  Loss :  2.15290557770502\n",
            "Accuracy :  47.78048780487805  Loss :  2.1329969196784786\n",
            "Accuracy :  47.40983606557377  Loss :  2.1284102846364505\n",
            "Accuracy :  47.592592592592595  Loss :  2.119876511302995\n",
            "\n",
            "Epoch: 32\n",
            "Accuracy :  55.0  Loss :  1.7536526918411255\n",
            "Accuracy :  48.70646766169154  Loss :  2.0034778616321622\n",
            "Accuracy :  48.50623441396509  Loss :  2.0073221437354336\n",
            "Accuracy :  42.0  Loss :  2.5528786182403564\n",
            "Accuracy :  47.476190476190474  Loss :  2.1271418843950545\n",
            "Accuracy :  47.80487804878049  Loss :  2.105081148263885\n",
            "Accuracy :  47.47540983606557  Loss :  2.102095949845236\n",
            "Accuracy :  47.72839506172839  Loss :  2.093841284881403\n",
            "\n",
            "Epoch: 33\n",
            "Accuracy :  43.0  Loss :  2.1556830406188965\n",
            "Accuracy :  48.16417910447761  Loss :  2.0158476082246697\n",
            "Accuracy :  47.765586034912715  Loss :  2.0338076266861913\n",
            "Accuracy :  41.0  Loss :  2.5510382652282715\n",
            "Accuracy :  47.285714285714285  Loss :  2.1428011769340154\n",
            "Accuracy :  47.609756097560975  Loss :  2.1238231426332055\n",
            "Accuracy :  47.42622950819672  Loss :  2.119354937897354\n",
            "Accuracy :  47.79012345679013  Loss :  2.1118272410498724\n",
            "\n",
            "Epoch: 34\n",
            "Accuracy :  49.0  Loss :  1.9462398290634155\n",
            "Accuracy :  47.90547263681592  Loss :  2.0240379571914673\n",
            "Accuracy :  48.01745635910225  Loss :  2.0235238931422814\n",
            "Accuracy :  41.0  Loss :  2.5481810569763184\n",
            "Accuracy :  47.76190476190476  Loss :  2.129280209541321\n",
            "Accuracy :  47.8780487804878  Loss :  2.1091418905955988\n",
            "Accuracy :  47.63934426229508  Loss :  2.104943844138599\n",
            "Accuracy :  47.888888888888886  Loss :  2.0966643109733676\n",
            "\n",
            "Epoch: 35\n",
            "Accuracy :  48.0  Loss :  1.9888699054718018\n",
            "Accuracy :  48.159203980099505  Loss :  2.019298760452081\n",
            "Accuracy :  48.19950124688279  Loss :  2.0216088841978155\n",
            "Accuracy :  44.0  Loss :  2.5765726566314697\n",
            "Accuracy :  47.38095238095238  Loss :  2.1599889369237992\n",
            "Accuracy :  47.829268292682926  Loss :  2.136216675362936\n",
            "Accuracy :  47.557377049180324  Loss :  2.131943006984523\n",
            "Accuracy :  47.76543209876543  Loss :  2.1240654050568004\n",
            "\n",
            "Epoch: 36\n",
            "Accuracy :  49.0  Loss :  1.8663520812988281\n",
            "Accuracy :  48.18407960199005  Loss :  2.017605336744394\n",
            "Accuracy :  47.942643391521194  Loss :  2.033605687338812\n",
            "Accuracy :  43.0  Loss :  2.5676212310791016\n",
            "Accuracy :  47.0  Loss :  2.150038934889294\n",
            "Accuracy :  47.46341463414634  Loss :  2.1288607847399827\n",
            "Accuracy :  47.278688524590166  Loss :  2.1235689589234648\n",
            "Accuracy :  47.5679012345679  Loss :  2.11603143480089\n",
            "\n",
            "Epoch: 37\n",
            "Accuracy :  50.0  Loss :  2.0881049633026123\n",
            "Accuracy :  47.875621890547265  Loss :  2.052525267672183\n",
            "Accuracy :  47.87032418952619  Loss :  2.044719472490344\n",
            "Accuracy :  42.0  Loss :  2.546518564224243\n",
            "Accuracy :  47.61904761904762  Loss :  2.121704095885867\n",
            "Accuracy :  47.951219512195124  Loss :  2.098424798104821\n",
            "Accuracy :  47.67213114754098  Loss :  2.0927016617821868\n",
            "Accuracy :  47.96296296296296  Loss :  2.0850092231491466\n",
            "\n",
            "Epoch: 38\n",
            "Accuracy :  53.0  Loss :  1.779954433441162\n",
            "Accuracy :  47.7363184079602  Loss :  2.038505541744517\n",
            "Accuracy :  48.104738154613464  Loss :  2.0238070987406513\n",
            "Accuracy :  41.0  Loss :  2.5607385635375977\n",
            "Accuracy :  47.523809523809526  Loss :  2.1457905144918534\n",
            "Accuracy :  47.853658536585364  Loss :  2.126026232068132\n",
            "Accuracy :  47.557377049180324  Loss :  2.119482810380029\n",
            "Accuracy :  47.77777777777778  Loss :  2.1119366221957736\n",
            "\n",
            "Epoch: 39\n",
            "Accuracy :  52.0  Loss :  1.9968615770339966\n",
            "Accuracy :  47.90547263681592  Loss :  2.0396505546807058\n",
            "Accuracy :  47.885286783042396  Loss :  2.0386729778494326\n",
            "Accuracy :  41.0  Loss :  2.5835790634155273\n",
            "Accuracy :  47.42857142857143  Loss :  2.1597811097190496\n",
            "Accuracy :  47.58536585365854  Loss :  2.142984433871944\n",
            "Accuracy :  47.40983606557377  Loss :  2.138120834944678\n",
            "Accuracy :  47.72839506172839  Loss :  2.1302376396862077\n",
            "\n",
            "Epoch: 40\n",
            "Accuracy :  51.0  Loss :  1.8229973316192627\n",
            "Accuracy :  47.223880597014926  Loss :  2.0583463367538073\n",
            "Accuracy :  47.80548628428928  Loss :  2.0297761263692764\n",
            "Accuracy :  43.0  Loss :  2.574350595474243\n",
            "Accuracy :  47.61904761904762  Loss :  2.1473101377487183\n",
            "Accuracy :  47.90243902439025  Loss :  2.1234362009094983\n",
            "Accuracy :  47.50819672131148  Loss :  2.118533570258344\n",
            "Accuracy :  47.69135802469136  Loss :  2.1098181274202137\n",
            "\n",
            "Epoch: 41\n",
            "Accuracy :  41.0  Loss :  2.5236053466796875\n",
            "Accuracy :  47.87064676616915  Loss :  2.032607663330154\n",
            "Accuracy :  47.81296758104738  Loss :  2.033182072817833\n",
            "Accuracy :  43.0  Loss :  2.5766851902008057\n",
            "Accuracy :  47.285714285714285  Loss :  2.1525284619558427\n",
            "Accuracy :  47.63414634146341  Loss :  2.133000516309971\n",
            "Accuracy :  47.34426229508197  Loss :  2.1275238365423483\n",
            "Accuracy :  47.629629629629626  Loss :  2.119639183268135\n",
            "\n",
            "Epoch: 42\n",
            "Accuracy :  51.0  Loss :  2.141458034515381\n",
            "Accuracy :  47.95522388059702  Loss :  2.0495563964938643\n",
            "Accuracy :  48.042394014962596  Loss :  2.0320299194935254\n",
            "Accuracy :  42.0  Loss :  2.5742688179016113\n",
            "Accuracy :  47.61904761904762  Loss :  2.150859458105905\n",
            "Accuracy :  47.78048780487805  Loss :  2.133222199067837\n",
            "Accuracy :  47.459016393442624  Loss :  2.1299420438829015\n",
            "Accuracy :  47.7037037037037  Loss :  2.121941751903958\n",
            "\n",
            "Epoch: 43\n",
            "Accuracy :  50.0  Loss :  2.0332868099212646\n",
            "Accuracy :  48.16417910447761  Loss :  2.0233128829975033\n",
            "Accuracy :  48.1072319201995  Loss :  2.029263507101007\n",
            "Accuracy :  42.0  Loss :  2.595184087753296\n",
            "Accuracy :  46.76190476190476  Loss :  2.1721044211160567\n",
            "Accuracy :  47.146341463414636  Loss :  2.1518885246137294\n",
            "Accuracy :  47.0  Loss :  2.1482619457557552\n",
            "Accuracy :  47.2962962962963  Loss :  2.14042964099366\n",
            "\n",
            "Epoch: 44\n",
            "Accuracy :  52.0  Loss :  1.8595726490020752\n",
            "Accuracy :  48.10945273631841  Loss :  2.023835812042009\n",
            "Accuracy :  47.64588528678304  Loss :  2.0390341852668517\n",
            "Accuracy :  42.0  Loss :  2.540100336074829\n",
            "Accuracy :  47.57142857142857  Loss :  2.124106549081348\n",
            "Accuracy :  48.0  Loss :  2.104052113323677\n",
            "Accuracy :  47.60655737704918  Loss :  2.099269986152649\n",
            "Accuracy :  47.82716049382716  Loss :  2.0920948526005687\n",
            "\n",
            "Epoch: 45\n",
            "Accuracy :  51.0  Loss :  1.933820366859436\n",
            "Accuracy :  48.039800995024876  Loss :  2.0367594838735474\n",
            "Accuracy :  47.87531172069826  Loss :  2.03697145133839\n",
            "Accuracy :  42.0  Loss :  2.5792722702026367\n",
            "Accuracy :  47.476190476190474  Loss :  2.1539551473799206\n",
            "Accuracy :  47.707317073170735  Loss :  2.134146667108303\n",
            "Accuracy :  47.278688524590166  Loss :  2.1298642803411014\n",
            "Accuracy :  47.51851851851852  Loss :  2.1216117511560886\n",
            "\n",
            "Epoch: 46\n",
            "Accuracy :  52.0  Loss :  1.935097336769104\n",
            "Accuracy :  47.84577114427861  Loss :  2.04384264661305\n",
            "Accuracy :  47.8927680798005  Loss :  2.0319375361587637\n",
            "Accuracy :  42.0  Loss :  2.5424320697784424\n",
            "Accuracy :  47.76190476190476  Loss :  2.1215640760603405\n",
            "Accuracy :  48.0  Loss :  2.09951885444362\n",
            "Accuracy :  47.60655737704918  Loss :  2.0944087251287993\n",
            "Accuracy :  47.98765432098765  Loss :  2.0860200458102756\n",
            "\n",
            "Epoch: 47\n",
            "Accuracy :  49.0  Loss :  1.9903868436813354\n",
            "Accuracy :  48.36318407960199  Loss :  2.025608401393416\n",
            "Accuracy :  48.00498753117207  Loss :  2.0319487867212653\n",
            "Accuracy :  43.0  Loss :  2.5954346656799316\n",
            "Accuracy :  47.23809523809524  Loss :  2.162683952422369\n",
            "Accuracy :  47.65853658536585  Loss :  2.1397272377479366\n",
            "Accuracy :  47.278688524590166  Loss :  2.1347411519191306\n",
            "Accuracy :  47.382716049382715  Loss :  2.126359122770804\n",
            "\n",
            "Epoch: 48\n",
            "Accuracy :  49.0  Loss :  2.002445697784424\n",
            "Accuracy :  48.00995024875622  Loss :  2.0268266699207365\n",
            "Accuracy :  47.765586034912715  Loss :  2.0412311634220686\n",
            "Accuracy :  42.0  Loss :  2.5594966411590576\n",
            "Accuracy :  47.38095238095238  Loss :  2.1428618885221935\n",
            "Accuracy :  47.707317073170735  Loss :  2.122179641956236\n",
            "Accuracy :  47.36065573770492  Loss :  2.116995367847505\n",
            "Accuracy :  47.617283950617285  Loss :  2.109499994619393\n",
            "\n",
            "Epoch: 49\n",
            "Accuracy :  44.0  Loss :  1.813906192779541\n",
            "Accuracy :  47.80597014925373  Loss :  2.035336242979439\n",
            "Accuracy :  47.99750623441396  Loss :  2.033124767039482\n",
            "Accuracy :  42.0  Loss :  2.5816454887390137\n",
            "Accuracy :  47.095238095238095  Loss :  2.1614097243263606\n",
            "Accuracy :  47.4390243902439  Loss :  2.1430225953823183\n",
            "Accuracy :  47.22950819672131  Loss :  2.1378519144214567\n",
            "Accuracy :  47.46913580246913  Loss :  2.129743030041824\n",
            "\n",
            "Epoch: 50\n",
            "Accuracy :  54.0  Loss :  1.8579926490783691\n",
            "Accuracy :  48.20398009950249  Loss :  2.0267579869844425\n",
            "Accuracy :  47.6284289276808  Loss :  2.0419394078100113\n",
            "Accuracy :  42.0  Loss :  2.5698494911193848\n",
            "Accuracy :  47.61904761904762  Loss :  2.139739229565575\n",
            "Accuracy :  47.8780487804878  Loss :  2.1184580355155758\n",
            "Accuracy :  47.540983606557376  Loss :  2.1124999210482738\n",
            "Accuracy :  47.80246913580247  Loss :  2.1042596263650024\n",
            "\n",
            "Epoch: 51\n",
            "Accuracy :  47.0  Loss :  2.162236452102661\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-21f014f66950>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-1d8c8cc73685>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Itrative Pruning\n",
        "2.4 In this section, you will reinitiate a new network with gates by running corresponding cells. You will do pruning of 10% of the lowest-ranked remaining  weights from each layer every 10th epoch until 60% of the network is pruned."
      ],
      "metadata": {
        "id": "yTHEiEr25-Zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "\n",
        "turn_off_ratio = 0.1\n",
        "gates_turned_off_index = {0:[],4:[],9:[],13:[],18:[],22:[],26:[],31:[],35:[],39:[],44:[],48:[],52:[] }\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch + 70):\n",
        "\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "  \n",
        "    if(epoch % 10 == 0):\n",
        "\n",
        "      # save model parameters\n",
        "      PATH = \"/content/gdrive/MyDrive/Deep Learning/A5/Part2_net_iterative_pruning\"\n",
        "      torch.save(net.state_dict(), PATH)\n",
        "\n",
        "      if(epoch < 65 and epoch > 0):\n",
        "\n",
        "          # code here\n",
        "        \n",
        "          index_conv2d_list = [0,4,9,13,18,22,26,31,35,39,44,48,52]\n",
        "          # Modify gates parameters\n",
        "      \n",
        "          for i in range(len(index_conv2d_list)):\n",
        "              \n",
        "              # take sum of filters\n",
        "              a = torch.sum(net.features[index_conv2d_list[i]].weight, dim=[1,2,3])\n",
        "              \n",
        "              # take the absolute value\n",
        "              b = torch.abs(a)\n",
        "              \n",
        "              # find index of 10% lower remaining weights weights\n",
        "              \n",
        "              index_list = list(b.cpu().detach().numpy().argsort()) # sort according to weights\n",
        "\n",
        "              # print(\"epoch\", epoch, \"index list:\", index_list)\n",
        "              # now remove lower weights\n",
        "              for j in range(len(gates_turned_off_index[index_conv2d_list[i]])):\n",
        "                  index_list.remove(gates_turned_off_index[index_conv2d_list[i]][j]) \n",
        "              # print(\"index list after removal:\", index_list)\n",
        "\n",
        "              lower_index_list = index_list[ : int(np.ceil(turn_off_ratio*len(index_list)) ) ] # make list with lowest absolute sum\n",
        "              \n",
        "              # extend\n",
        "              gates_turned_off_index[index_conv2d_list[i]].extend(lower_index_list)\n",
        "\n",
        "              # modify gate parameters\n",
        "              for j in range(len(lower_index_list)):\n",
        "                  gates_index = index_conv2d_list[i]+1\n",
        "                  net.features[gates_index].weight[lower_index_list[j]] = 0"
      ],
      "metadata": {
        "id": "aew2wPwo8QCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6153eb1-b7b0-4dfa-e6f9-32a1ee7f48bb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Accuracy :  14.0  Loss :  2.5531439781188965\n",
            "Accuracy :  41.60696517412935  Loss :  1.5656945758791112\n",
            "Accuracy :  48.63840399002494  Loss :  1.3940493632433124\n",
            "Accuracy :  67.0  Loss :  0.895799994468689\n",
            "Accuracy :  63.666666666666664  Loss :  1.0323497709773837\n",
            "Accuracy :  63.24390243902439  Loss :  1.0334583535427\n",
            "Accuracy :  63.63934426229508  Loss :  1.0256241862891151\n",
            "Accuracy :  63.53086419753087  Loss :  1.0313786415406216\n",
            "\n",
            "Epoch: 2\n",
            "Accuracy :  62.0  Loss :  0.9716184139251709\n",
            "Accuracy :  65.17910447761194  Loss :  0.9680414908560947\n",
            "Accuracy :  66.46384039900249  Loss :  0.9398047855667343\n",
            "Accuracy :  72.0  Loss :  0.7599804401397705\n",
            "Accuracy :  68.47619047619048  Loss :  0.9437965438479469\n",
            "Accuracy :  68.41463414634147  Loss :  0.9543773241159392\n",
            "Accuracy :  68.54098360655738  Loss :  0.9556283130020392\n",
            "Accuracy :  68.62962962962963  Loss :  0.9451019454885412\n",
            "\n",
            "Epoch: 3\n",
            "Accuracy :  60.0  Loss :  1.0269616842269897\n",
            "Accuracy :  71.98507462686567  Loss :  0.7967324067111039\n",
            "Accuracy :  73.28927680798004  Loss :  0.7613216735924271\n",
            "Accuracy :  79.0  Loss :  0.6525117754936218\n",
            "Accuracy :  74.95238095238095  Loss :  0.7374833822250366\n",
            "Accuracy :  75.46341463414635  Loss :  0.7398136912322626\n",
            "Accuracy :  75.62295081967213  Loss :  0.7394522760735184\n",
            "Accuracy :  75.4320987654321  Loss :  0.7379898112497212\n",
            "\n",
            "Epoch: 4\n",
            "Accuracy :  80.0  Loss :  0.5787803530693054\n",
            "Accuracy :  76.55223880597015  Loss :  0.6708640473992077\n",
            "Accuracy :  77.00997506234414  Loss :  0.6597484240210859\n",
            "Accuracy :  79.0  Loss :  0.5815981030464172\n",
            "Accuracy :  76.95238095238095  Loss :  0.6949458249977657\n",
            "Accuracy :  75.63414634146342  Loss :  0.7211351736289698\n",
            "Accuracy :  76.29508196721312  Loss :  0.7081309464134153\n",
            "Accuracy :  76.27160493827161  Loss :  0.7065147547810166\n",
            "\n",
            "Epoch: 5\n",
            "Accuracy :  73.0  Loss :  0.7485525012016296\n",
            "Accuracy :  79.65174129353234  Loss :  0.5864746177670968\n",
            "Accuracy :  79.90523690773067  Loss :  0.5811773459810271\n",
            "Accuracy :  86.0  Loss :  0.4551369547843933\n",
            "Accuracy :  80.33333333333333  Loss :  0.5851622081938244\n",
            "Accuracy :  80.48780487804878  Loss :  0.594785197478969\n",
            "Accuracy :  80.52459016393442  Loss :  0.5844309755036088\n",
            "Accuracy :  80.5925925925926  Loss :  0.578507885152911\n",
            "\n",
            "Epoch: 6\n",
            "Accuracy :  74.0  Loss :  0.8298596739768982\n",
            "Accuracy :  81.94029850746269  Loss :  0.5239454122325081\n",
            "Accuracy :  81.90274314214464  Loss :  0.5249346501363483\n",
            "Accuracy :  83.0  Loss :  0.519902765750885\n",
            "Accuracy :  80.33333333333333  Loss :  0.5906547790481931\n",
            "Accuracy :  79.65853658536585  Loss :  0.6004603665049483\n",
            "Accuracy :  79.50819672131148  Loss :  0.601457876748726\n",
            "Accuracy :  79.4074074074074  Loss :  0.601637335839095\n",
            "\n",
            "Epoch: 7\n",
            "Accuracy :  77.0  Loss :  0.608450174331665\n",
            "Accuracy :  83.07462686567165  Loss :  0.48928603144427435\n",
            "Accuracy :  83.44389027431421  Loss :  0.48173992064528337\n",
            "Accuracy :  86.0  Loss :  0.4060930907726288\n",
            "Accuracy :  82.0952380952381  Loss :  0.5054858426253\n",
            "Accuracy :  82.21951219512195  Loss :  0.5089004061570982\n",
            "Accuracy :  82.91803278688525  Loss :  0.5027262631986962\n",
            "Accuracy :  82.51851851851852  Loss :  0.5092641615573271\n",
            "\n",
            "Epoch: 8\n",
            "Accuracy :  83.0  Loss :  0.5044784545898438\n",
            "Accuracy :  84.8955223880597  Loss :  0.432419642138837\n",
            "Accuracy :  84.56857855361596  Loss :  0.44340775851299635\n",
            "Accuracy :  79.0  Loss :  0.5441761016845703\n",
            "Accuracy :  79.52380952380952  Loss :  0.6101185367220924\n",
            "Accuracy :  79.5609756097561  Loss :  0.6291228721781474\n",
            "Accuracy :  79.68852459016394  Loss :  0.6230551737253783\n",
            "Accuracy :  79.5925925925926  Loss :  0.6211622820960151\n",
            "\n",
            "Epoch: 9\n",
            "Accuracy :  80.0  Loss :  0.48005515336990356\n",
            "Accuracy :  86.03482587064677  Loss :  0.40693860630787426\n",
            "Accuracy :  85.75810473815461  Loss :  0.4125152688073993\n",
            "Accuracy :  84.0  Loss :  0.34098300337791443\n",
            "Accuracy :  82.04761904761905  Loss :  0.5250829174405053\n",
            "Accuracy :  82.7560975609756  Loss :  0.5244734498058877\n",
            "Accuracy :  83.04918032786885  Loss :  0.5198012214215075\n",
            "Accuracy :  83.14814814814815  Loss :  0.5187507720641148\n",
            "\n",
            "Epoch: 10\n",
            "Accuracy :  92.0  Loss :  0.2679179906845093\n",
            "Accuracy :  86.94029850746269  Loss :  0.37983043469599825\n",
            "Accuracy :  86.83291770573567  Loss :  0.3794322611387828\n",
            "Accuracy :  95.0  Loss :  0.2541678845882416\n",
            "Accuracy :  84.71428571428571  Loss :  0.48055432240168255\n",
            "Accuracy :  83.95121951219512  Loss :  0.4886167361968901\n",
            "Accuracy :  84.06557377049181  Loss :  0.48011585874635665\n",
            "Accuracy :  84.03703703703704  Loss :  0.4769865925665255\n",
            "\n",
            "Epoch: 11\n",
            "Accuracy :  85.0  Loss :  0.36911526322364807\n",
            "Accuracy :  87.38805970149254  Loss :  0.35870239828059924\n",
            "Accuracy :  87.53366583541147  Loss :  0.35788722432908276\n",
            "Accuracy :  86.0  Loss :  0.36813390254974365\n",
            "Accuracy :  84.57142857142857  Loss :  0.48391891377312796\n",
            "Accuracy :  84.60975609756098  Loss :  0.48607565135490605\n",
            "Accuracy :  84.40983606557377  Loss :  0.4818706404967386\n",
            "Accuracy :  84.4320987654321  Loss :  0.47910028475302235\n",
            "\n",
            "Epoch: 12\n",
            "Accuracy :  83.0  Loss :  0.44396209716796875\n",
            "Accuracy :  84.8955223880597  Loss :  0.43915936654776483\n",
            "Accuracy :  85.41895261845387  Loss :  0.42178369887600514\n",
            "Accuracy :  84.0  Loss :  0.4832252860069275\n",
            "Accuracy :  82.85714285714286  Loss :  0.49653229117393494\n",
            "Accuracy :  83.0  Loss :  0.498034046190541\n",
            "Accuracy :  83.14754098360656  Loss :  0.4921448147687756\n",
            "Accuracy :  83.19753086419753  Loss :  0.4931802105756454\n",
            "\n",
            "Epoch: 13\n",
            "Accuracy :  85.0  Loss :  0.3768989145755768\n",
            "Accuracy :  87.76119402985074  Loss :  0.3525318140117683\n",
            "Accuracy :  87.63840399002494  Loss :  0.35379826186452423\n",
            "Accuracy :  86.0  Loss :  0.414321631193161\n",
            "Accuracy :  83.52380952380952  Loss :  0.49510703909964787\n",
            "Accuracy :  83.63414634146342  Loss :  0.5054230442861232\n",
            "Accuracy :  83.65573770491804  Loss :  0.5033962052376544\n",
            "Accuracy :  83.50617283950618  Loss :  0.5054276865205647\n",
            "\n",
            "Epoch: 14\n",
            "Accuracy :  93.0  Loss :  0.19760051369667053\n",
            "Accuracy :  88.85074626865672  Loss :  0.3200981716314952\n",
            "Accuracy :  88.5935162094763  Loss :  0.3272132187189901\n",
            "Accuracy :  85.0  Loss :  0.45005321502685547\n",
            "Accuracy :  83.28571428571429  Loss :  0.5006912350654602\n",
            "Accuracy :  83.41463414634147  Loss :  0.5074961825114924\n",
            "Accuracy :  83.72131147540983  Loss :  0.5013534123780298\n",
            "Accuracy :  83.91358024691358  Loss :  0.4939323402481315\n",
            "\n",
            "Epoch: 15\n",
            "Accuracy :  96.0  Loss :  0.13649173080921173\n",
            "Accuracy :  90.06467661691542  Loss :  0.2870544705224867\n",
            "Accuracy :  89.93017456359102  Loss :  0.2935715663314163\n",
            "Accuracy :  90.0  Loss :  0.2823668420314789\n",
            "Accuracy :  84.85714285714286  Loss :  0.4576645280633654\n",
            "Accuracy :  84.48780487804878  Loss :  0.48198922378260917\n",
            "Accuracy :  84.14754098360656  Loss :  0.482064637004352\n",
            "Accuracy :  84.23456790123457  Loss :  0.4847131978582453\n",
            "\n",
            "Epoch: 16\n",
            "Accuracy :  84.0  Loss :  0.3084965944290161\n",
            "Accuracy :  90.363184079602  Loss :  0.2823574093740378\n",
            "Accuracy :  90.27680798004988  Loss :  0.28118970570272933\n",
            "Accuracy :  88.0  Loss :  0.36788666248321533\n",
            "Accuracy :  85.14285714285714  Loss :  0.41864885957468123\n",
            "Accuracy :  85.41463414634147  Loss :  0.43769143521785736\n",
            "Accuracy :  85.59016393442623  Loss :  0.4331214542760224\n",
            "Accuracy :  85.71604938271605  Loss :  0.4314744923217797\n",
            "\n",
            "Epoch: 17\n",
            "Accuracy :  90.0  Loss :  0.31718966364860535\n",
            "Accuracy :  90.94527363184079  Loss :  0.2596321837300092\n",
            "Accuracy :  90.9925187032419  Loss :  0.25940628085945017\n",
            "Accuracy :  90.0  Loss :  0.32645660638809204\n",
            "Accuracy :  85.66666666666667  Loss :  0.42966403776691076\n",
            "Accuracy :  85.07317073170732  Loss :  0.45564198530301814\n",
            "Accuracy :  85.52459016393442  Loss :  0.44495104351004616\n",
            "Accuracy :  85.61728395061728  Loss :  0.44274482370158774\n",
            "\n",
            "Epoch: 18\n",
            "Accuracy :  91.0  Loss :  0.2658357620239258\n",
            "Accuracy :  91.49253731343283  Loss :  0.2440508960268984\n",
            "Accuracy :  91.63840399002494  Loss :  0.24642032485204446\n",
            "Accuracy :  89.0  Loss :  0.3119795322418213\n",
            "Accuracy :  87.23809523809524  Loss :  0.3814712628012612\n",
            "Accuracy :  86.8048780487805  Loss :  0.40128000952848575\n",
            "Accuracy :  87.0  Loss :  0.3966226450732497\n",
            "Accuracy :  87.03703703703704  Loss :  0.4023976340705966\n",
            "\n",
            "Epoch: 19\n",
            "Accuracy :  93.0  Loss :  0.23100000619888306\n",
            "Accuracy :  91.97512437810946  Loss :  0.22893855843081404\n",
            "Accuracy :  91.85286783042395  Loss :  0.232312136661828\n",
            "Accuracy :  87.0  Loss :  0.42670831084251404\n",
            "Accuracy :  85.57142857142857  Loss :  0.4306088572456723\n",
            "Accuracy :  85.48780487804878  Loss :  0.4501110315322876\n",
            "Accuracy :  85.77049180327869  Loss :  0.4434260506121839\n",
            "Accuracy :  85.98765432098766  Loss :  0.44531743394003975\n",
            "\n",
            "Epoch: 20\n",
            "Accuracy :  90.0  Loss :  0.26744866371154785\n",
            "Accuracy :  92.92039800995025  Loss :  0.20856845864460835\n",
            "Accuracy :  92.36907730673316  Loss :  0.22002702739619256\n",
            "Accuracy :  92.0  Loss :  0.33680805563926697\n",
            "Accuracy :  85.33333333333333  Loss :  0.44231013527938295\n",
            "Accuracy :  85.29268292682927  Loss :  0.456509893260351\n",
            "Accuracy :  85.8360655737705  Loss :  0.44371306016796924\n",
            "Accuracy :  85.9753086419753  Loss :  0.4443496612854946\n",
            "\n",
            "Epoch: 21\n",
            "Accuracy :  91.0  Loss :  0.26572054624557495\n",
            "Accuracy :  93.05472636815921  Loss :  0.19896229230497606\n",
            "Accuracy :  92.68329177057356  Loss :  0.20928651720582694\n",
            "Accuracy :  93.0  Loss :  0.256740927696228\n",
            "Accuracy :  87.28571428571429  Loss :  0.40538225713230314\n",
            "Accuracy :  87.53658536585365  Loss :  0.4094054738922817\n",
            "Accuracy :  87.68852459016394  Loss :  0.4026813875944888\n",
            "Accuracy :  87.82716049382717  Loss :  0.402988123121085\n",
            "\n",
            "Epoch: 22\n",
            "Accuracy :  78.0  Loss :  0.5847802758216858\n",
            "Accuracy :  89.28358208955224  Loss :  0.3027705453076766\n",
            "Accuracy :  89.9501246882793  Loss :  0.2851877403155229\n",
            "Accuracy :  91.0  Loss :  0.2860525846481323\n",
            "Accuracy :  85.52380952380952  Loss :  0.4539285557610648\n",
            "Accuracy :  85.85365853658537  Loss :  0.45359253665296045\n",
            "Accuracy :  86.11475409836065  Loss :  0.4448341492746697\n",
            "Accuracy :  86.07407407407408  Loss :  0.4445824041778659\n",
            "\n",
            "Epoch: 23\n",
            "Accuracy :  91.0  Loss :  0.2339792251586914\n",
            "Accuracy :  92.04975124378109  Loss :  0.22765936294746636\n",
            "Accuracy :  91.85785536159601  Loss :  0.23249128550068102\n",
            "Accuracy :  88.0  Loss :  0.34520792961120605\n",
            "Accuracy :  86.04761904761905  Loss :  0.43986006010146367\n",
            "Accuracy :  86.2439024390244  Loss :  0.4365671131669021\n",
            "Accuracy :  86.31147540983606  Loss :  0.42854426972201615\n",
            "Accuracy :  86.14814814814815  Loss :  0.4340379054163709\n",
            "\n",
            "Epoch: 24\n",
            "Accuracy :  96.0  Loss :  0.15448392927646637\n",
            "Accuracy :  92.85074626865672  Loss :  0.20663572176920242\n",
            "Accuracy :  92.66583541147132  Loss :  0.20991193878457434\n",
            "Accuracy :  90.0  Loss :  0.3325699269771576\n",
            "Accuracy :  86.19047619047619  Loss :  0.42697664598623913\n",
            "Accuracy :  85.60975609756098  Loss :  0.45659171553646644\n",
            "Accuracy :  85.8360655737705  Loss :  0.44363715707278645\n",
            "Accuracy :  86.08641975308642  Loss :  0.4382861206928889\n",
            "\n",
            "Epoch: 25\n",
            "Accuracy :  92.0  Loss :  0.14873431622982025\n",
            "Accuracy :  93.363184079602  Loss :  0.18991533648315354\n",
            "Accuracy :  93.05985037406484  Loss :  0.1965262182335604\n",
            "Accuracy :  90.0  Loss :  0.2772787809371948\n",
            "Accuracy :  86.95238095238095  Loss :  0.4156331888266972\n",
            "Accuracy :  86.60975609756098  Loss :  0.43111998496986015\n",
            "Accuracy :  86.44262295081967  Loss :  0.4286412613802269\n",
            "Accuracy :  86.4320987654321  Loss :  0.42873651322759226\n",
            "\n",
            "Epoch: 26\n",
            "Accuracy :  92.0  Loss :  0.23105907440185547\n",
            "Accuracy :  94.01492537313433  Loss :  0.17667803868873797\n",
            "Accuracy :  93.80798004987531  Loss :  0.180706450218646\n",
            "Accuracy :  91.0  Loss :  0.3169770836830139\n",
            "Accuracy :  87.61904761904762  Loss :  0.3868660174664997\n",
            "Accuracy :  87.5609756097561  Loss :  0.39671986095789\n",
            "Accuracy :  87.52459016393442  Loss :  0.39656229268332\n",
            "Accuracy :  87.49382716049382  Loss :  0.3988831155462029\n",
            "\n",
            "Epoch: 27\n",
            "Accuracy :  97.0  Loss :  0.1208040714263916\n",
            "Accuracy :  94.09950248756219  Loss :  0.16641820629882575\n",
            "Accuracy :  94.0  Loss :  0.1691553207518454\n",
            "Accuracy :  91.0  Loss :  0.2647485136985779\n",
            "Accuracy :  88.0  Loss :  0.37313683543886456\n",
            "Accuracy :  87.95121951219512  Loss :  0.39243259626190835\n",
            "Accuracy :  87.93442622950819  Loss :  0.38549194228453715\n",
            "Accuracy :  87.95061728395062  Loss :  0.3851119209954768\n",
            "\n",
            "Epoch: 28\n",
            "Accuracy :  98.0  Loss :  0.1171993538737297\n",
            "Accuracy :  94.55223880597015  Loss :  0.15580928890933446\n",
            "Accuracy :  94.25935162094763  Loss :  0.16324556942220936\n",
            "Accuracy :  89.0  Loss :  0.29289010167121887\n",
            "Accuracy :  88.0  Loss :  0.3660896292754582\n",
            "Accuracy :  87.85365853658537  Loss :  0.39766621008151914\n",
            "Accuracy :  88.04918032786885  Loss :  0.38396826093314124\n",
            "Accuracy :  88.14814814814815  Loss :  0.3859479817343347\n",
            "\n",
            "Epoch: 29\n",
            "Accuracy :  93.0  Loss :  0.17135560512542725\n",
            "Accuracy :  94.70646766169155  Loss :  0.15185584706157002\n",
            "Accuracy :  94.56857855361596  Loss :  0.154567364092331\n",
            "Accuracy :  92.0  Loss :  0.2815611958503723\n",
            "Accuracy :  87.9047619047619  Loss :  0.4022167133433478\n",
            "Accuracy :  87.78048780487805  Loss :  0.41450304338117927\n",
            "Accuracy :  87.85245901639344  Loss :  0.3984306970092117\n",
            "Accuracy :  87.85185185185185  Loss :  0.39998307548187395\n",
            "\n",
            "Epoch: 30\n",
            "Accuracy :  96.0  Loss :  0.14219194650650024\n",
            "Accuracy :  94.9950248756219  Loss :  0.14178460161780837\n",
            "Accuracy :  94.73316708229426  Loss :  0.14757565168334064\n",
            "Accuracy :  89.0  Loss :  0.3479962944984436\n",
            "Accuracy :  87.38095238095238  Loss :  0.41624226030849276\n",
            "Accuracy :  87.60975609756098  Loss :  0.43011332176080563\n",
            "Accuracy :  87.73770491803279  Loss :  0.40928210073807203\n",
            "Accuracy :  87.61728395061728  Loss :  0.41080128337130134\n",
            "\n",
            "Epoch: 31\n",
            "Accuracy :  97.0  Loss :  0.09690815955400467\n",
            "Accuracy :  95.34825870646766  Loss :  0.13248735337300382\n",
            "Accuracy :  95.0997506234414  Loss :  0.13926822058113286\n",
            "Accuracy :  94.0  Loss :  0.2389887273311615\n",
            "Accuracy :  88.0952380952381  Loss :  0.3955877252987453\n",
            "Accuracy :  87.7560975609756  Loss :  0.4116419299346645\n",
            "Accuracy :  87.98360655737704  Loss :  0.3993523091566367\n",
            "Accuracy :  88.07407407407408  Loss :  0.4029352495699753\n",
            "\n",
            "Epoch: 32\n",
            "Accuracy :  88.0  Loss :  0.38900086283683777\n",
            "Accuracy :  91.02985074626865  Loss :  0.2509126304764653\n",
            "Accuracy :  91.51620947630923  Loss :  0.23712935722305292\n",
            "Accuracy :  86.0  Loss :  0.4521687626838684\n",
            "Accuracy :  87.66666666666667  Loss :  0.4034196862152645\n",
            "Accuracy :  87.3170731707317  Loss :  0.4194922011073043\n",
            "Accuracy :  87.54098360655738  Loss :  0.4064517592797514\n",
            "Accuracy :  87.45679012345678  Loss :  0.40836119559811956\n",
            "\n",
            "Epoch: 33\n",
            "Accuracy :  90.0  Loss :  0.2236335426568985\n",
            "Accuracy :  93.65671641791045  Loss :  0.18436599585843916\n",
            "Accuracy :  93.64588528678304  Loss :  0.18401090923390187\n",
            "Accuracy :  88.0  Loss :  0.35943737626075745\n",
            "Accuracy :  87.19047619047619  Loss :  0.3820598537013644\n",
            "Accuracy :  87.78048780487805  Loss :  0.3863397818513033\n",
            "Accuracy :  88.0327868852459  Loss :  0.38093815963776384\n",
            "Accuracy :  87.98765432098766  Loss :  0.38725331720010736\n",
            "\n",
            "Epoch: 34\n",
            "Accuracy :  96.0  Loss :  0.1216595396399498\n",
            "Accuracy :  94.56716417910448  Loss :  0.15302488580346107\n",
            "Accuracy :  94.26932668329177  Loss :  0.16085936441237195\n",
            "Accuracy :  87.0  Loss :  0.41360703110694885\n",
            "Accuracy :  86.66666666666667  Loss :  0.41508368580114274\n",
            "Accuracy :  86.34146341463415  Loss :  0.44353385051576105\n",
            "Accuracy :  86.45901639344262  Loss :  0.4352524014281445\n",
            "Accuracy :  86.5679012345679  Loss :  0.44106862629637306\n",
            "\n",
            "Epoch: 35\n",
            "Accuracy :  96.0  Loss :  0.11788079887628555\n",
            "Accuracy :  95.31840796019901  Loss :  0.13634910390003405\n",
            "Accuracy :  94.94763092269326  Loss :  0.14425549056018677\n",
            "Accuracy :  90.0  Loss :  0.3197896182537079\n",
            "Accuracy :  88.14285714285714  Loss :  0.38699069761094596\n",
            "Accuracy :  87.53658536585365  Loss :  0.4153948360826911\n",
            "Accuracy :  87.91803278688525  Loss :  0.40530629480471375\n",
            "Accuracy :  88.09876543209876  Loss :  0.39901551935407853\n",
            "\n",
            "Epoch: 36\n",
            "Accuracy :  97.0  Loss :  0.0885433554649353\n",
            "Accuracy :  95.34328358208955  Loss :  0.13394788567403063\n",
            "Accuracy :  95.17456359102245  Loss :  0.13731337300596985\n",
            "Accuracy :  90.0  Loss :  0.35288098454475403\n",
            "Accuracy :  87.47619047619048  Loss :  0.4091209687647365\n",
            "Accuracy :  87.41463414634147  Loss :  0.42164509888829255\n",
            "Accuracy :  87.73770491803279  Loss :  0.4194528591193137\n",
            "Accuracy :  87.8395061728395  Loss :  0.42296785714081775\n",
            "\n",
            "Epoch: 37\n",
            "Accuracy :  95.0  Loss :  0.10842929780483246\n",
            "Accuracy :  95.35323383084577  Loss :  0.13066765560365434\n",
            "Accuracy :  95.2793017456359  Loss :  0.1344149186742722\n",
            "Accuracy :  90.0  Loss :  0.3920166492462158\n",
            "Accuracy :  88.33333333333333  Loss :  0.39943817328839076\n",
            "Accuracy :  87.90243902439025  Loss :  0.4151981574733083\n",
            "Accuracy :  88.11475409836065  Loss :  0.40359186613168874\n",
            "Accuracy :  88.09876543209876  Loss :  0.4069753072144073\n",
            "\n",
            "Epoch: 38\n",
            "Accuracy :  97.0  Loss :  0.13720424473285675\n",
            "Accuracy :  95.31840796019901  Loss :  0.1285406028416323\n",
            "Accuracy :  95.32418952618454  Loss :  0.12932654858527337\n",
            "Accuracy :  90.0  Loss :  0.3668740391731262\n",
            "Accuracy :  88.14285714285714  Loss :  0.41079602922712055\n",
            "Accuracy :  87.70731707317073  Loss :  0.423614411092386\n",
            "Accuracy :  87.78688524590164  Loss :  0.4126345571435866\n",
            "Accuracy :  87.72839506172839  Loss :  0.4133455714693776\n",
            "\n",
            "Epoch: 39\n",
            "Accuracy :  98.0  Loss :  0.056853074580430984\n",
            "Accuracy :  96.02487562189054  Loss :  0.11074565343595856\n",
            "Accuracy :  96.0349127182045  Loss :  0.11260356067533504\n",
            "Accuracy :  91.0  Loss :  0.37080565094947815\n",
            "Accuracy :  87.76190476190476  Loss :  0.41482682526111603\n",
            "Accuracy :  88.02439024390245  Loss :  0.4207970761671299\n",
            "Accuracy :  88.59016393442623  Loss :  0.40581285122965205\n",
            "Accuracy :  88.67901234567901  Loss :  0.40458517604404026\n",
            "\n",
            "Epoch: 40\n",
            "Accuracy :  95.0  Loss :  0.10583171248435974\n",
            "Accuracy :  95.78109452736318  Loss :  0.11728408766345154\n",
            "Accuracy :  95.87780548628429  Loss :  0.11536967248291223\n",
            "Accuracy :  90.0  Loss :  0.3798382878303528\n",
            "Accuracy :  87.52380952380952  Loss :  0.41825724499566214\n",
            "Accuracy :  87.8780487804878  Loss :  0.4299884856474109\n",
            "Accuracy :  88.36065573770492  Loss :  0.40570065867705424\n",
            "Accuracy :  88.33333333333333  Loss :  0.4102149079611272\n",
            "\n",
            "Epoch: 41\n",
            "Accuracy :  97.0  Loss :  0.08748810738325119\n",
            "Accuracy :  95.87064676616916  Loss :  0.1135588644711829\n",
            "Accuracy :  96.13466334164589  Loss :  0.11071911940067783\n",
            "Accuracy :  91.0  Loss :  0.2606988251209259\n",
            "Accuracy :  87.66666666666667  Loss :  0.4317573528914225\n",
            "Accuracy :  87.41463414634147  Loss :  0.4543219524912718\n",
            "Accuracy :  87.59016393442623  Loss :  0.44878785165606955\n",
            "Accuracy :  87.62962962962963  Loss :  0.4431755901486785\n",
            "\n",
            "Epoch: 42\n",
            "Accuracy :  84.0  Loss :  0.5026500225067139\n",
            "Accuracy :  91.83084577114428  Loss :  0.23822857739765252\n",
            "Accuracy :  92.54613466334165  Loss :  0.21389807970669503\n",
            "Accuracy :  86.0  Loss :  0.3912169635295868\n",
            "Accuracy :  86.23809523809524  Loss :  0.4589643740937823\n",
            "Accuracy :  86.34146341463415  Loss :  0.45570380186162346\n",
            "Accuracy :  86.50819672131148  Loss :  0.4484127454581808\n",
            "Accuracy :  86.51851851851852  Loss :  0.4450886304731722\n",
            "\n",
            "Epoch: 43\n",
            "Accuracy :  92.0  Loss :  0.17086189985275269\n",
            "Accuracy :  94.65174129353234  Loss :  0.15168561020967972\n",
            "Accuracy :  94.35162094763092  Loss :  0.15843377358969904\n",
            "Accuracy :  92.0  Loss :  0.24972233176231384\n",
            "Accuracy :  87.9047619047619  Loss :  0.40860671514556524\n",
            "Accuracy :  87.46341463414635  Loss :  0.43419791830749044\n",
            "Accuracy :  87.68852459016394  Loss :  0.425042132373716\n",
            "Accuracy :  87.60493827160494  Loss :  0.4224480659137537\n",
            "\n",
            "Epoch: 44\n",
            "Accuracy :  93.0  Loss :  0.15200184285640717\n",
            "Accuracy :  95.16417910447761  Loss :  0.13429088558797814\n",
            "Accuracy :  95.12468827930175  Loss :  0.1357681229803479\n",
            "Accuracy :  87.0  Loss :  0.4056621491909027\n",
            "Accuracy :  86.95238095238095  Loss :  0.450935260170982\n",
            "Accuracy :  87.09756097560975  Loss :  0.45056141222395546\n",
            "Accuracy :  87.59016393442623  Loss :  0.4326629685085328\n",
            "Accuracy :  87.71604938271605  Loss :  0.4234537281739859\n",
            "\n",
            "Epoch: 45\n",
            "Accuracy :  95.0  Loss :  0.1271778643131256\n",
            "Accuracy :  95.7363184079602  Loss :  0.11865015634664552\n",
            "Accuracy :  95.49625935162095  Loss :  0.1279076288345077\n",
            "Accuracy :  89.0  Loss :  0.3530716598033905\n",
            "Accuracy :  89.0  Loss :  0.38017346532571883\n",
            "Accuracy :  88.48780487804878  Loss :  0.3989685563052573\n",
            "Accuracy :  88.77049180327869  Loss :  0.38457026613540335\n",
            "Accuracy :  88.72839506172839  Loss :  0.38183273411827323\n",
            "\n",
            "Epoch: 46\n",
            "Accuracy :  97.0  Loss :  0.09539178758859634\n",
            "Accuracy :  96.2636815920398  Loss :  0.10751463337546557\n",
            "Accuracy :  96.07231920199501  Loss :  0.11380050791673678\n",
            "Accuracy :  89.0  Loss :  0.4368598163127899\n",
            "Accuracy :  87.19047619047619  Loss :  0.4491767301445916\n",
            "Accuracy :  87.1951219512195  Loss :  0.4559649530707336\n",
            "Accuracy :  87.36065573770492  Loss :  0.441019955472868\n",
            "Accuracy :  87.49382716049382  Loss :  0.436046486283526\n",
            "\n",
            "Epoch: 47\n",
            "Accuracy :  95.0  Loss :  0.14768357574939728\n",
            "Accuracy :  96.23880597014926  Loss :  0.10619243963700326\n",
            "Accuracy :  95.93765586034912  Loss :  0.11438303968945793\n",
            "Accuracy :  88.0  Loss :  0.3789902627468109\n",
            "Accuracy :  87.66666666666667  Loss :  0.4227981858310245\n",
            "Accuracy :  88.2439024390244  Loss :  0.422500729924295\n",
            "Accuracy :  88.40983606557377  Loss :  0.415729325203622\n",
            "Accuracy :  88.75308641975309  Loss :  0.4043617786632644\n",
            "\n",
            "Epoch: 48\n",
            "Accuracy :  98.0  Loss :  0.08272545039653778\n",
            "Accuracy :  96.34328358208955  Loss :  0.10065390574583663\n",
            "Accuracy :  96.13965087281795  Loss :  0.10786885972555141\n",
            "Accuracy :  89.0  Loss :  0.3822844326496124\n",
            "Accuracy :  87.95238095238095  Loss :  0.40937673052151996\n",
            "Accuracy :  88.1219512195122  Loss :  0.42135691860827\n",
            "Accuracy :  88.44262295081967  Loss :  0.40402729687143546\n",
            "Accuracy :  88.66666666666667  Loss :  0.400530696651082\n",
            "\n",
            "Epoch: 49\n",
            "Accuracy :  95.0  Loss :  0.12103372812271118\n",
            "Accuracy :  96.85074626865672  Loss :  0.09421174810152148\n",
            "Accuracy :  96.59102244389027  Loss :  0.09772412576004305\n",
            "Accuracy :  92.0  Loss :  0.4190385341644287\n",
            "Accuracy :  87.85714285714286  Loss :  0.4485150022166116\n",
            "Accuracy :  87.70731707317073  Loss :  0.4563559119294329\n",
            "Accuracy :  87.62295081967213  Loss :  0.44860038649840434\n",
            "Accuracy :  87.58024691358025  Loss :  0.44467849017661293\n",
            "\n",
            "Epoch: 50\n",
            "Accuracy :  97.0  Loss :  0.06219564378261566\n",
            "Accuracy :  96.66169154228855  Loss :  0.09345217911869436\n",
            "Accuracy :  96.58104738154613  Loss :  0.09678147954081286\n",
            "Accuracy :  87.0  Loss :  0.46742430329322815\n",
            "Accuracy :  87.14285714285714  Loss :  0.48691120530877796\n",
            "Accuracy :  87.4390243902439  Loss :  0.4847792179846182\n",
            "Accuracy :  87.8360655737705  Loss :  0.4618189830760487\n",
            "Accuracy :  87.81481481481481  Loss :  0.4634134688495118\n",
            "\n",
            "Epoch: 51\n",
            "Accuracy :  98.0  Loss :  0.06953425705432892\n",
            "Accuracy :  96.55721393034825  Loss :  0.09474796081304698\n",
            "Accuracy :  96.60099750623442  Loss :  0.09470763674094121\n",
            "Accuracy :  91.0  Loss :  0.3367539644241333\n",
            "Accuracy :  88.0952380952381  Loss :  0.43363292586235774\n",
            "Accuracy :  88.14634146341463  Loss :  0.4475581267984902\n",
            "Accuracy :  88.19672131147541  Loss :  0.43907124488080135\n",
            "Accuracy :  88.0246913580247  Loss :  0.44268405271901023\n",
            "\n",
            "Epoch: 52\n",
            "Accuracy :  84.0  Loss :  0.5261339545249939\n",
            "Accuracy :  91.69651741293532  Loss :  0.23687984712829638\n",
            "Accuracy :  92.64089775561098  Loss :  0.21110074562101888\n",
            "Accuracy :  88.0  Loss :  0.3839185833930969\n",
            "Accuracy :  86.76190476190476  Loss :  0.4385848981993539\n",
            "Accuracy :  86.65853658536585  Loss :  0.447762370836444\n",
            "Accuracy :  86.81967213114754  Loss :  0.441829010355668\n",
            "Accuracy :  87.07407407407408  Loss :  0.4292100607245057\n",
            "\n",
            "Epoch: 53\n",
            "Accuracy :  98.0  Loss :  0.07923270016908646\n",
            "Accuracy :  95.01990049751244  Loss :  0.1411421776410952\n",
            "Accuracy :  94.9650872817955  Loss :  0.14066880588370964\n",
            "Accuracy :  91.0  Loss :  0.3617110848426819\n",
            "Accuracy :  87.57142857142857  Loss :  0.4355499418008895\n",
            "Accuracy :  87.36585365853658  Loss :  0.4485455613310744\n",
            "Accuracy :  87.47540983606558  Loss :  0.4456619341842464\n",
            "Accuracy :  87.62962962962963  Loss :  0.4340382832803844\n",
            "\n",
            "Epoch: 54\n",
            "Accuracy :  93.0  Loss :  0.11447915434837341\n",
            "Accuracy :  95.56218905472637  Loss :  0.12420852867821555\n",
            "Accuracy :  95.4139650872818  Loss :  0.12778139283457896\n",
            "Accuracy :  89.0  Loss :  0.44442886114120483\n",
            "Accuracy :  86.57142857142857  Loss :  0.4719215496664956\n",
            "Accuracy :  87.1951219512195  Loss :  0.46782428571363777\n",
            "Accuracy :  87.55737704918033  Loss :  0.45306663044163437\n",
            "Accuracy :  87.69135802469135  Loss :  0.44736376459951754\n",
            "\n",
            "Epoch: 55\n",
            "Accuracy :  95.0  Loss :  0.12577299773693085\n",
            "Accuracy :  96.02487562189054  Loss :  0.11275875062417628\n",
            "Accuracy :  95.80299251870325  Loss :  0.11759661372630227\n",
            "Accuracy :  89.0  Loss :  0.3892059624195099\n",
            "Accuracy :  87.9047619047619  Loss :  0.4091841323035104\n",
            "Accuracy :  88.0  Loss :  0.4213338468859835\n",
            "Accuracy :  88.31147540983606  Loss :  0.41169117708675196\n",
            "Accuracy :  88.32098765432099  Loss :  0.4136203124567314\n",
            "\n",
            "Epoch: 56\n",
            "Accuracy :  98.0  Loss :  0.0659373477101326\n",
            "Accuracy :  96.0  Loss :  0.11144894036228087\n",
            "Accuracy :  95.89775561097257  Loss :  0.1158021492860338\n",
            "Accuracy :  89.0  Loss :  0.3585025370121002\n",
            "Accuracy :  88.0952380952381  Loss :  0.3986667458500181\n",
            "Accuracy :  88.07317073170732  Loss :  0.417525275451381\n",
            "Accuracy :  88.19672131147541  Loss :  0.41419996247916924\n",
            "Accuracy :  88.39506172839506  Loss :  0.4125805826466761\n",
            "\n",
            "Epoch: 57\n",
            "Accuracy :  98.0  Loss :  0.10309576988220215\n",
            "Accuracy :  96.50248756218906  Loss :  0.09844327506734364\n",
            "Accuracy :  96.39650872817955  Loss :  0.1021212924810642\n",
            "Accuracy :  89.0  Loss :  0.3482910096645355\n",
            "Accuracy :  88.14285714285714  Loss :  0.4182284701438177\n",
            "Accuracy :  88.04878048780488  Loss :  0.43233840712686866\n",
            "Accuracy :  88.37704918032787  Loss :  0.42371367553218464\n",
            "Accuracy :  88.65432098765432  Loss :  0.4179170023144027\n",
            "\n",
            "Epoch: 58\n",
            "Accuracy :  93.0  Loss :  0.17678287625312805\n",
            "Accuracy :  96.55223880597015  Loss :  0.09620198219162018\n",
            "Accuracy :  96.62344139650872  Loss :  0.0965857196394865\n",
            "Accuracy :  91.0  Loss :  0.397118479013443\n",
            "Accuracy :  88.80952380952381  Loss :  0.4137521904139292\n",
            "Accuracy :  88.60975609756098  Loss :  0.41867290037434274\n",
            "Accuracy :  88.73770491803279  Loss :  0.4091230560033048\n",
            "Accuracy :  88.5925925925926  Loss :  0.40826639753800853\n",
            "\n",
            "Epoch: 59\n",
            "Accuracy :  97.0  Loss :  0.08065791428089142\n",
            "Accuracy :  96.97014925373135  Loss :  0.08776729512236901\n",
            "Accuracy :  96.74812967581047  Loss :  0.09351003975456493\n",
            "Accuracy :  90.0  Loss :  0.32446223497390747\n",
            "Accuracy :  88.71428571428571  Loss :  0.4172298588923046\n",
            "Accuracy :  88.26829268292683  Loss :  0.4331816990927952\n",
            "Accuracy :  88.21311475409836  Loss :  0.4260038669969215\n",
            "Accuracy :  88.37037037037037  Loss :  0.42424734911800904\n",
            "\n",
            "Epoch: 60\n",
            "Accuracy :  97.0  Loss :  0.07500558346509933\n",
            "Accuracy :  96.38308457711443  Loss :  0.09724594328311545\n",
            "Accuracy :  96.4713216957606  Loss :  0.09646872975898353\n",
            "Accuracy :  87.0  Loss :  0.4219808578491211\n",
            "Accuracy :  87.71428571428571  Loss :  0.4633001536130905\n",
            "Accuracy :  88.2439024390244  Loss :  0.45710504454810447\n",
            "Accuracy :  88.08196721311475  Loss :  0.4662787259602156\n",
            "Accuracy :  88.06172839506173  Loss :  0.4613184965687034\n",
            "\n",
            "Epoch: 61\n",
            "Accuracy :  95.0  Loss :  0.13189934194087982\n",
            "Accuracy :  97.1044776119403  Loss :  0.0829136000612556\n",
            "Accuracy :  96.81795511221945  Loss :  0.0904571719890335\n",
            "Accuracy :  92.0  Loss :  0.34696057438850403\n",
            "Accuracy :  87.9047619047619  Loss :  0.4191652742170152\n",
            "Accuracy :  87.8780487804878  Loss :  0.43770743688432184\n",
            "Accuracy :  88.18032786885246  Loss :  0.43316683124323363\n",
            "Accuracy :  88.4074074074074  Loss :  0.4263160197455206\n",
            "\n",
            "Epoch: 62\n",
            "Accuracy :  86.0  Loss :  0.47590646147727966\n",
            "Accuracy :  91.58208955223881  Loss :  0.24631955745208323\n",
            "Accuracy :  92.36907730673316  Loss :  0.21656800407052337\n",
            "Accuracy :  91.0  Loss :  0.3407922089099884\n",
            "Accuracy :  87.66666666666667  Loss :  0.4161501881622133\n",
            "Accuracy :  87.90243902439025  Loss :  0.41705832176092195\n",
            "Accuracy :  87.80327868852459  Loss :  0.41970558244673933\n",
            "Accuracy :  87.77777777777777  Loss :  0.4162549087662756\n",
            "\n",
            "Epoch: 63\n",
            "Accuracy :  93.0  Loss :  0.15187416970729828\n",
            "Accuracy :  94.87064676616916  Loss :  0.14149151379196204\n",
            "Accuracy :  94.89027431421447  Loss :  0.14222585014125652\n",
            "Accuracy :  92.0  Loss :  0.26566025614738464\n",
            "Accuracy :  88.14285714285714  Loss :  0.3917696724335353\n",
            "Accuracy :  87.73170731707317  Loss :  0.41392354739875326\n",
            "Accuracy :  87.59016393442623  Loss :  0.4151669972744144\n",
            "Accuracy :  87.76543209876543  Loss :  0.4123937607179453\n",
            "\n",
            "Epoch: 64\n",
            "Accuracy :  94.0  Loss :  0.21705934405326843\n",
            "Accuracy :  95.70149253731343  Loss :  0.12127673711201445\n",
            "Accuracy :  95.40897755610973  Loss :  0.1277887599445192\n",
            "Accuracy :  92.0  Loss :  0.2677154839038849\n",
            "Accuracy :  88.0  Loss :  0.4265061787196568\n",
            "Accuracy :  88.0  Loss :  0.42973289547896965\n",
            "Accuracy :  88.01639344262296  Loss :  0.42661021723121895\n",
            "Accuracy :  88.04938271604938  Loss :  0.4273478350153676\n",
            "\n",
            "Epoch: 65\n",
            "Accuracy :  99.0  Loss :  0.040642738342285156\n",
            "Accuracy :  96.05970149253731  Loss :  0.11011392790916844\n",
            "Accuracy :  95.99501246882792  Loss :  0.11193704118771\n",
            "Accuracy :  91.0  Loss :  0.3224504888057709\n",
            "Accuracy :  88.47619047619048  Loss :  0.42286014769758495\n",
            "Accuracy :  88.58536585365853  Loss :  0.4246900892112313\n",
            "Accuracy :  88.26229508196721  Loss :  0.4225499066905897\n",
            "Accuracy :  88.23456790123457  Loss :  0.4183474432355092\n",
            "\n",
            "Epoch: 66\n",
            "Accuracy :  95.0  Loss :  0.23842233419418335\n",
            "Accuracy :  96.23383084577114  Loss :  0.10720915996946802\n",
            "Accuracy :  96.16209476309227  Loss :  0.10957063443754379\n",
            "Accuracy :  88.0  Loss :  0.4673670828342438\n",
            "Accuracy :  88.71428571428571  Loss :  0.3953616732642764\n",
            "Accuracy :  88.70731707317073  Loss :  0.4141431516263543\n",
            "Accuracy :  88.8360655737705  Loss :  0.4111401641466578\n",
            "Accuracy :  88.72839506172839  Loss :  0.41280698169160773\n",
            "\n",
            "Epoch: 67\n",
            "Accuracy :  98.0  Loss :  0.03908435255289078\n",
            "Accuracy :  96.38805970149254  Loss :  0.10116717394496967\n",
            "Accuracy :  96.3216957605985  Loss :  0.1030080285610775\n",
            "Accuracy :  91.0  Loss :  0.34970566630363464\n",
            "Accuracy :  89.04761904761905  Loss :  0.4114130636056264\n",
            "Accuracy :  88.78048780487805  Loss :  0.4281152334155106\n",
            "Accuracy :  88.70491803278688  Loss :  0.4258655119137686\n",
            "Accuracy :  88.67901234567901  Loss :  0.4222235030230181\n",
            "\n",
            "Epoch: 68\n",
            "Accuracy :  99.0  Loss :  0.05219624936580658\n",
            "Accuracy :  96.42288557213931  Loss :  0.10123219163115345\n",
            "Accuracy :  96.4139650872818  Loss :  0.10140696476373887\n",
            "Accuracy :  90.0  Loss :  0.3908713161945343\n",
            "Accuracy :  88.04761904761905  Loss :  0.4095053360575721\n",
            "Accuracy :  87.85365853658537  Loss :  0.4316921837446166\n",
            "Accuracy :  88.1311475409836  Loss :  0.4281920419364679\n",
            "Accuracy :  88.29629629629629  Loss :  0.4219045517621217\n",
            "\n",
            "Epoch: 69\n",
            "Accuracy :  95.0  Loss :  0.09871762990951538\n",
            "Accuracy :  96.7860696517413  Loss :  0.08945375298200851\n",
            "Accuracy :  96.67331670822942  Loss :  0.09364738315808357\n",
            "Accuracy :  92.0  Loss :  0.4039366543292999\n",
            "Accuracy :  88.71428571428571  Loss :  0.4158552438020706\n",
            "Accuracy :  88.39024390243902  Loss :  0.43657948876299507\n",
            "Accuracy :  88.50819672131148  Loss :  0.4306891995375274\n",
            "Accuracy :  88.61728395061728  Loss :  0.42633320372781636\n",
            "\n",
            "Epoch: 70\n",
            "Accuracy :  97.0  Loss :  0.06504780054092407\n",
            "Accuracy :  96.99004975124379  Loss :  0.08583169454588226\n",
            "Accuracy :  96.95511221945137  Loss :  0.0870806522565218\n",
            "Accuracy :  91.0  Loss :  0.34481561183929443\n",
            "Accuracy :  89.14285714285714  Loss :  0.4172397887422925\n",
            "Accuracy :  88.85365853658537  Loss :  0.43562623422320296\n",
            "Accuracy :  88.78688524590164  Loss :  0.4342645054957906\n",
            "Accuracy :  88.71604938271605  Loss :  0.4376421832008126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Print the final weights to show pruning occured"
      ],
      "metadata": {
        "id": "1WE09r4FWAq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_conv2d_list = [0,4,9,13,18,22,26,31,35,39,44,48,52]\n",
        "# Modify gates parameters\n",
        "for i in range(len(index_conv2d_list)):\n",
        "  print(net.features[index_conv2d_list[i]+1].weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc081McNV_1t",
        "outputId": "da9b2c25-bb34-47c9-f8e8-c199775542c7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 0., 1., 0.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 0., 1., 1., 0.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 0.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
            "        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
            "        1., 0.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
            "        1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 0.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "        1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 1.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 1.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
            "        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
            "        1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 0.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
            "        1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 1.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
            "        0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
            "        0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
            "        0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
            "        0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
            "        1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 1.], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Activations for ranking filters\n",
        "2.5 So you implemented one matric for ranking of the filters that is megnitude of there weights. In this step you need to do itrative pruning but this time you will rank filters from the layer based on average of there activations. You will do pruning of 10% of the lowest-ranked remaining  weights from each layer every 10th epoch until 60% of the network is pruned. To get the activations you this link is usefull \n",
        "[link](https://web.stanford.edu/~nanbhas/blog/forward-hooks-pytorch/), use the forward hooks method to do this. "
      ],
      "metadata": {
        "id": "0_T8LcLWao17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getActivation(name):\n",
        "    # the hook signature\n",
        "    activation = {}\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook"
      ],
      "metadata": {
        "id": "2vx5mk54-Q4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use forward hook's method\n",
        "\n",
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "\n",
        "turn_off_ratio = 0.1\n",
        "gates_turned_off_index = {0:[],4:[],9:[],13:[],18:[],22:[],26:[],31:[],35:[],39:[],44:[],48:[],52:[] }\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch + 70):\n",
        "\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "  \n",
        "    if(epoch % 10 == 0):\n",
        "\n",
        "      # save model parameters\n",
        "      PATH = \"/content/gdrive/MyDrive/Deep Learning/A5/Part2_net_iterative_pruning\"\n",
        "      torch.save(net.state_dict(), PATH)\n",
        "\n",
        "      if(epoch < 65 and epoch > 0):\n",
        "\n",
        "          # code here\n",
        "        \n",
        "          index_conv2d_list = [0,4,9,13,18,22,26,31,35,39,44,48,52]\n",
        "          # Modify gates parameters\n",
        "      \n",
        "          for i in range(len(index_conv2d_list)):\n",
        "              \n",
        "              # take sum of filters\n",
        "              a = torch.sum(net.features[index_conv2d_list[i]].weight, dim=[1,2,3])\n",
        "              \n",
        "              # take the absolute value\n",
        "              b = torch.abs(a)\n",
        "              \n",
        "              # find index of 10% lower remaining weights weights\n",
        "              \n",
        "              index_list = list(b.cpu().detach().numpy().argsort()) # sort according to weights\n",
        "\n",
        "              # print(\"epoch\", epoch, \"index list:\", index_list)\n",
        "              # now remove lower weights\n",
        "              for j in range(len(gates_turned_off_index[index_conv2d_list[i]])):\n",
        "                  index_list.remove(gates_turned_off_index[index_conv2d_list[i]][j]) \n",
        "              # print(\"index list after removal:\", index_list)\n",
        "\n",
        "              lower_index_list = index_list[ : int(np.ceil(turn_off_ratio*len(index_list)) ) ] # make list with lowest absolute sum\n",
        "              \n",
        "              # extend\n",
        "              gates_turned_off_index[index_conv2d_list[i]].extend(lower_index_list)\n",
        "\n",
        "              # modify gate parameters\n",
        "              for j in range(len(lower_index_list)):\n",
        "                  gates_index = index_conv2d_list[i]+1\n",
        "                  net.features[gates_index].weight[lower_index_list[j]] = 0"
      ],
      "metadata": {
        "id": "1Ku6iHffdaLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Online filter pruning (Bonus)\n",
        "\n",
        "Till now we did offline pruning but that is an iterative process where we iteratively try to put some mathematical boundaries around the network and then the network tries to fit into these boundaries. Online pruning on the other hand makes to the process of pruning a part of network convergence itself. \n",
        "\n",
        "In this section, we will make the gates learnable and apply L1 norm to them. After training threashold on gates value and remove part of network globally in one go. fine tune once. Example Threashould 0.0005. "
      ],
      "metadata": {
        "id": "ZUd84QNn9O-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hello"
      ],
      "metadata": {
        "id": "8FvNKs4mAevZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}