{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "5EMJsuURtnn1"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ya1dJNI03vUG"
      },
      "cell_type": "markdown",
      "source": [
        "Since there is basically no external data file, this notebook is extremly simple to open in collab and to train your models there. This is highly recommended since LSTMS take longer to train. "
      ]
    },
    {
      "metadata": {
        "id": "BPKdISeNb6Ny"
      },
      "cell_type": "markdown",
      "source": [
        "# **Task 3: **\n",
        "\n",
        "**We're going to build a network that takes and converts dates from one format into another. **\n",
        "\n",
        "For example, given a date string such as \"14-03-2020\", we want out network to, character by character read this string and output to us \"The 14th of March 2020\".\n",
        "\n",
        "Since our data is a sequence of information, each part derives it's meaning from a prior part.\n",
        "\"2\" as the second month character could either encode for Feb or for december depending on what number preceded it. This is a problem that is well handled by recurrent neural networks. \n",
        "\n",
        "We're going to be using LSTM's to build this network, which are recurrent learning cells. \n"
      ]
    },
    {
      "metadata": {
        "id": "i2Dn8er7fEzJ"
      },
      "cell_type": "markdown",
      "source": [
        "Below is a model that allows us to do sequence to sequence conversion where the input and output are of different lengths, the example provided is one of english to french translation. This is similar to the encoder, decoder style of machine translation we have learnt about in class.\n"
      ]
    },
    {
      "metadata": {
        "id": "SFPZi1gi1lUy"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://blog.keras.io/img/seq2seq/seq2seq-teacher-forcing.png)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "-NaVnqtr1xLO"
      },
      "cell_type": "markdown",
      "source": [
        "Below is a function that generates the dataset, giving you date entries in different formats in for as many days (2019 April 15th onwards) as you'd like.\n",
        "Go ahead, test it, see how it returns values and what they are."
      ]
    },
    {
      "metadata": {
        "id": "pvDoWosatnoC"
      },
      "cell_type": "markdown",
      "source": [
        "### Make Dataset"
      ]
    },
    {
      "metadata": {
        "id": "-GIMoCVttnoF"
      },
      "cell_type": "code",
      "source": [
        "def make_short_date(dt):\n",
        "    return dt.strftime('%d-%m-%Y')\n",
        "\n",
        "def make_long_date(dt):\n",
        "    date = dt.strftime('%d')\n",
        "    if date[-1] == '1':\n",
        "        suffix = 'st'\n",
        "    elif date[-1] == '2':\n",
        "        suffix = 'nd'\n",
        "    elif date[-1] == '3':\n",
        "        suffix = 'rd'\n",
        "    else:\n",
        "        suffix = 'th'\n",
        "    month = dt.strftime('%B')\n",
        "    year = dt.strftime('%Y')\n",
        "    \n",
        "    return date + suffix + ' of ' + month + ' ' + year\n",
        "\n",
        "def make_dataset(n):\n",
        "    dates = pd.date_range(datetime(1900, 4, 14), periods=n, normalize=True)\n",
        "    \n",
        "    x = dates.map(make_short_date).values\n",
        "    y = dates.map(make_long_date).values\n",
        "    \n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "euanX1Mctnog",
        "outputId": "55620f02-b697-4be0-b94e-9035f15c4897",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "x, y = make_dataset(50)\n",
        "x[:5], y[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['14-04-1990', '15-04-1990', '16-04-1990', '17-04-1990',\n",
              "        '18-04-1990'], dtype=object),\n",
              " array(['14th of April 1990', '15th of April 1990', '16th of April 1990',\n",
              "        '17th of April 1990', '18th of April 1990'], dtype=object))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "J_H6BdzZfo-l"
      },
      "cell_type": "markdown",
      "source": [
        "We've got some hyper-paramters set for you here, we're going to start working with 10,000 training examples and see how well our models trains with that."
      ]
    },
    {
      "metadata": {
        "id": "R1h84iik-FXh"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MbTUAyfmtno2"
      },
      "cell_type": "code",
      "source": [
        "dataset = make_dataset(num_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dV4JZXexwlim",
        "outputId": "e20ca000-5cdd-424f-84e6-dc62d6af2cde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'15-04-1990'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[1][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vkPvlrWBRN7k",
        "outputId": "62d65783-34a4-4008-c59d-466cd4253bd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'15th of April 1990'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "hKruNCkNfzjw"
      },
      "cell_type": "markdown",
      "source": [
        "### Part 1 - Generation and preperation of dataset\n",
        "\n",
        "Prepare the dataset for training. The following steps will have to be taken.\n",
        "\n",
        "We need a total of 3 datasets: \n",
        "1. encoder_input (our original data)\n",
        "2. decoder_input (the target data with start and end tokens added) -> our start token is a \"\\t\" character, and the stop character \"\\n\".\n",
        "3. decoder_target (target data without a start token, but with an end token) \n",
        "\n",
        "decoder_input and decoder_target data are different since once the model is trained, we will pass the decoder a sequence containing only a \"\\t\" and it will generate the rest of the sentence for us after, ending with the \"\\n\" token.\n",
        "\n",
        "Here is an example of this format of data for a single sample.\n",
        "\n",
        "encoder_input: \"14-03-2019\"\n",
        "decoder_input: \"\\tThe 14th of March 2019\\n\"\n",
        "decoder_target: \"The 14th of March 2019\\n\"\n",
        "\n",
        "Now that we know what the target is for the dataset, it's time to start converting it into a form the network can understand and work with.\n",
        "We need each sample to be an n*m numpy array of 0's. Where n is the maximum length of the sequence and m is the vocubulary size.\n",
        "\n",
        "An input would go from \"14-05-19\" to a array of size (1*8*10), where 1 is our batch size, 8 is sequence length and our vocab is 10 (including the '-').\n",
        "\n",
        "To do this, complete the following:\n",
        "\n",
        "1. Create a list of all possible vocab for the input and output target data (use a set)\n",
        "2. Use this set to create a dictionary that can convert characters into ints\n",
        "\n",
        "    *For instance you'll have a 'char_2_index' array that will function as \"char_2_index['-'] = 13\"*\n",
        "3. Convert these lists of ints into a 2d numpy array (3d when considering batches)"
      ]
    },
    {
      "metadata": {
        "id": "j_Vr_NaRy1tR"
      },
      "cell_type": "markdown",
      "source": [
        "**Example: **\n",
        "\n",
        "Input sentence: 14-04-2019 into a 3d tensor would result in the following:\n",
        "\n",
        "\n",
        "[[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
        "\n",
        "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
        "  \n",
        "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
        "  \n",
        "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
        "  \n",
        "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
        "  \n",
        "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
        "  \n",
        "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
        "  \n",
        "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
        "  \n",
        "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
        "  \n",
        "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]]\n",
        "-\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# code help: https://keras.io/examples/nlp/lstm_seq2seq/\n",
        "# create list to contain input dates and target format of dates\n",
        "input_dates = []\n",
        "target_dates = []\n",
        "# create set (as mentioned in instructions) of individual characters\n",
        "input_chars = set()\n",
        "target_chars = set()\n",
        "\n",
        "for i in range(len(dataset[0])):\n",
        "\n",
        "    # populate input_dates list\n",
        "    input_date = dataset[0][i]\n",
        "    input_dates.append(input_date)\n",
        "\n",
        "    # populate target_dates list\n",
        "    target_date = \"\\t\" + dataset[1][i] + \"\\n\"\n",
        "    target_dates.append(target_date)\n",
        "    \n",
        "    # populate input_chars set\n",
        "    for char in input_date:\n",
        "        if char not in input_chars:\n",
        "            input_chars.add(char)\n",
        "\n",
        "    # populate target_chars set\n",
        "    for char in target_date:\n",
        "        if char not in target_chars:\n",
        "            target_chars.add(char)\n",
        "\n",
        "# make set to list and sort\n",
        "input_chars = sorted(list(input_chars))\n",
        "target_chars = sorted(list(target_chars))\n",
        "\n",
        "max_len_encoder_seq = max([len(txt) for txt in input_dates])\n",
        "max_len_decoder_seq = max([len(txt) for txt in target_dates])\n",
        "\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_chars)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_chars)])\n",
        "\n",
        "encoder_input = np.zeros( (len(input_dates), max_len_encoder_seq, len(input_chars)))\n",
        "decoder_input = np.zeros( (len(input_dates), max_len_decoder_seq, len(target_chars)))\n",
        "decoder_target = np.zeros( (len(input_dates), max_len_decoder_seq, len(target_chars)))\n",
        "\n",
        "for batch in len(input_dates):\n",
        "    for t, char in enumerate(input_dates[batch]):\n",
        "        encoder_input[batch, t, input_token_index[char]] = 1.0\n",
        "    \n",
        "    for t, char in enumerate(target_dates[batch]):\n",
        "        # decoder_target is ahead of decoder_input by one timestep\n",
        "        decoder_input[batch, t, target_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            decoder_target[batch, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_input[batch, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "    decoder_target[batch, t:, target_token_index[\" \"]] = 1.0"
      ],
      "metadata": {
        "id": "MuwtQ4cbRGzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H4oTdGIqhCs1"
      },
      "cell_type": "markdown",
      "source": [
        "**Part 2 - Setting up the network**\n",
        "\n",
        "Before we begin, uncomment the following lines of code and fill in appropriate variables to have an overview of what your network will be training with."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of samples:', num_samples)\n",
        "print('Number of unique input tokens:', len(input_chars))\n",
        "print('Number of unique output tokens:', len(target_chars))\n",
        "print('Max sequence length for inputs:', max_len_encoder_seq)\n",
        "print('Max sequence length for outputs:', max_len_decoder_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iK17D7x_0SIG",
        "outputId": "440a97aa-233a-4bd5-9200-8a0141eb8ce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 10000\n",
            "Number of unique input tokens: 11\n",
            "Number of unique output tokens: 40\n",
            "Max sequence length for inputs: 10\n",
            "Max sequence length for outputs: 24\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "QtoKiL3mjdOt"
      },
      "cell_type": "markdown",
      "source": [
        "Great, now you have to set up an encoder decoder network. \n",
        "\n",
        "This will require 2 LSTMS\n",
        "\n",
        "1. An encoder LSTM (size - latent dimension as we defined above):\n",
        "  - We'll pass our encoder_input data to this\n",
        "  - We will let it run through the LSTM and get the states back from it (discard the network output, we only need the c and h states), save these\n",
        " \n",
        "2. A decoder LSTM (size - latent dimension):\n",
        "  - We'll be passing decoder_input data to this (with the '\\t' and ''\\n' added and encoded)\n",
        "  - We will also be passing a specific initial state to this (states c and h, taken from the encoder network)\n",
        "  \n",
        "Following this LSTM, you will need a dense layer of output_tokens (output vocab) size to convert the result into a one hot encoded target. Figure out what activation this should require"
      ]
    },
    {
      "metadata": {
        "id": "824bTKHE4BdG"
      },
      "cell_type": "code",
      "source": [
        "encoder_inputs = Input(shape=(None, len(input_chars)))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "encoder_states = [state_h, state_c]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YQWudG_D4BbO"
      },
      "cell_type": "code",
      "source": [
        "# decoder\n",
        "decoder_inputs = Input(shape=(None, len(target_chars)))\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(target_chars), activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_IHxijrP4BZY"
      },
      "cell_type": "code",
      "source": [
        "# Model definition\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2eFKrSlO0_VM"
      },
      "cell_type": "markdown",
      "source": [
        "** Model structure ** \n",
        "\n",
        "So you have \n",
        "\n",
        "  1. (encoder_input) -> encoder LSTM -> (output, states)\n",
        "  2. (decoder_input, states) -> decoder LSTM -> Dense (decoder_output)\n",
        "  \n",
        "For the overall model: \n",
        "1. Inputs - [encoder_input, decoder_input]\n",
        "2. Outputs - [decoder_target]\n",
        "\n",
        "Model Optimizer - RMSProp\n",
        "Model Loss - categorical_crossentropy\n"
      ]
    },
    {
      "metadata": {
        "id": "J4KCRh_Z4ClD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95ccd72b-f2ce-4703-a2e8-6f9520c04c94"
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit([encoder_input, decoder_input], decoder_target, batch_size=batch_size, epochs=epochs, validation_split=0.2,)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "125/125 [==============================] - 27s 189ms/step - loss: 1.8721 - accuracy: 0.4826 - val_loss: 1.2375 - val_accuracy: 0.6105\n",
            "Epoch 2/100\n",
            "125/125 [==============================] - 26s 211ms/step - loss: 0.9051 - accuracy: 0.7305 - val_loss: 0.6767 - val_accuracy: 0.7596\n",
            "Epoch 3/100\n",
            "125/125 [==============================] - 23s 182ms/step - loss: 0.5057 - accuracy: 0.8226 - val_loss: 0.4950 - val_accuracy: 0.7925\n",
            "Epoch 4/100\n",
            "125/125 [==============================] - 26s 210ms/step - loss: 0.3731 - accuracy: 0.8566 - val_loss: 0.4047 - val_accuracy: 0.8431\n",
            "Epoch 5/100\n",
            "125/125 [==============================] - 31s 247ms/step - loss: 0.3190 - accuracy: 0.8741 - val_loss: 0.3371 - val_accuracy: 0.8675\n",
            "Epoch 6/100\n",
            "125/125 [==============================] - 26s 204ms/step - loss: 0.2709 - accuracy: 0.8888 - val_loss: 0.3722 - val_accuracy: 0.8326\n",
            "Epoch 7/100\n",
            "125/125 [==============================] - 22s 178ms/step - loss: 0.2502 - accuracy: 0.8930 - val_loss: 0.3738 - val_accuracy: 0.8455\n",
            "Epoch 8/100\n",
            "125/125 [==============================] - 22s 179ms/step - loss: 0.2399 - accuracy: 0.8951 - val_loss: 0.3624 - val_accuracy: 0.8751\n",
            "Epoch 9/100\n",
            "125/125 [==============================] - 22s 178ms/step - loss: 0.2239 - accuracy: 0.9027 - val_loss: 0.3104 - val_accuracy: 0.8808\n",
            "Epoch 10/100\n",
            "125/125 [==============================] - 24s 195ms/step - loss: 0.2007 - accuracy: 0.9157 - val_loss: 0.4078 - val_accuracy: 0.8463\n",
            "Epoch 11/100\n",
            "125/125 [==============================] - 25s 204ms/step - loss: 0.1677 - accuracy: 0.9345 - val_loss: 0.2970 - val_accuracy: 0.9101\n",
            "Epoch 12/100\n",
            "125/125 [==============================] - 27s 220ms/step - loss: 0.1353 - accuracy: 0.9488 - val_loss: 0.2842 - val_accuracy: 0.9118\n",
            "Epoch 13/100\n",
            "125/125 [==============================] - 26s 204ms/step - loss: 0.1181 - accuracy: 0.9559 - val_loss: 0.2655 - val_accuracy: 0.9420\n",
            "Epoch 14/100\n",
            "125/125 [==============================] - 22s 178ms/step - loss: 0.1042 - accuracy: 0.9611 - val_loss: 0.2864 - val_accuracy: 0.9489\n",
            "Epoch 15/100\n",
            "125/125 [==============================] - 24s 195ms/step - loss: 0.0791 - accuracy: 0.9721 - val_loss: 0.3523 - val_accuracy: 0.9132\n",
            "Epoch 16/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0476 - accuracy: 0.9837 - val_loss: 0.3379 - val_accuracy: 0.9204\n",
            "Epoch 17/100\n",
            "125/125 [==============================] - 24s 190ms/step - loss: 0.0280 - accuracy: 0.9908 - val_loss: 0.3309 - val_accuracy: 0.9256\n",
            "Epoch 18/100\n",
            "125/125 [==============================] - 24s 188ms/step - loss: 0.0218 - accuracy: 0.9931 - val_loss: 0.3521 - val_accuracy: 0.9179\n",
            "Epoch 19/100\n",
            "125/125 [==============================] - 23s 187ms/step - loss: 0.0135 - accuracy: 0.9962 - val_loss: 0.3441 - val_accuracy: 0.9315\n",
            "Epoch 20/100\n",
            "125/125 [==============================] - 22s 179ms/step - loss: 0.0105 - accuracy: 0.9973 - val_loss: 0.3466 - val_accuracy: 0.9347\n",
            "Epoch 21/100\n",
            "125/125 [==============================] - 22s 179ms/step - loss: 0.0097 - accuracy: 0.9974 - val_loss: 0.3399 - val_accuracy: 0.9359\n",
            "Epoch 22/100\n",
            "125/125 [==============================] - 26s 209ms/step - loss: 0.0058 - accuracy: 0.9985 - val_loss: 0.3648 - val_accuracy: 0.9356\n",
            "Epoch 23/100\n",
            "125/125 [==============================] - 24s 192ms/step - loss: 0.0053 - accuracy: 0.9988 - val_loss: 0.3134 - val_accuracy: 0.9451\n",
            "Epoch 24/100\n",
            "125/125 [==============================] - 24s 196ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.3436 - val_accuracy: 0.9449\n",
            "Epoch 25/100\n",
            "125/125 [==============================] - 25s 204ms/step - loss: 0.0071 - accuracy: 0.9986 - val_loss: 0.3332 - val_accuracy: 0.9454\n",
            "Epoch 26/100\n",
            "125/125 [==============================] - 25s 197ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.3774 - val_accuracy: 0.9420\n",
            "Epoch 27/100\n",
            "125/125 [==============================] - 27s 213ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.3771 - val_accuracy: 0.9433\n",
            "Epoch 28/100\n",
            "125/125 [==============================] - 35s 281ms/step - loss: 0.0035 - accuracy: 0.9991 - val_loss: 0.3555 - val_accuracy: 0.9419\n",
            "Epoch 29/100\n",
            "125/125 [==============================] - 23s 185ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.3694 - val_accuracy: 0.9407\n",
            "Epoch 30/100\n",
            "125/125 [==============================] - 24s 192ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.3809 - val_accuracy: 0.9430\n",
            "Epoch 31/100\n",
            "125/125 [==============================] - 25s 204ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.4237 - val_accuracy: 0.9383\n",
            "Epoch 32/100\n",
            "125/125 [==============================] - 24s 191ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.3659 - val_accuracy: 0.9425\n",
            "Epoch 33/100\n",
            "125/125 [==============================] - 25s 203ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.4159 - val_accuracy: 0.9438\n",
            "Epoch 34/100\n",
            "125/125 [==============================] - 22s 178ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.4105 - val_accuracy: 0.9438\n",
            "Epoch 35/100\n",
            "125/125 [==============================] - 22s 179ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.3654 - val_accuracy: 0.9473\n",
            "Epoch 36/100\n",
            "125/125 [==============================] - 22s 179ms/step - loss: 9.4885e-04 - accuracy: 0.9998 - val_loss: 0.4197 - val_accuracy: 0.9471\n",
            "Epoch 37/100\n",
            "125/125 [==============================] - 22s 179ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.3962 - val_accuracy: 0.9485\n",
            "Epoch 38/100\n",
            "125/125 [==============================] - 23s 181ms/step - loss: 6.7126e-06 - accuracy: 1.0000 - val_loss: 0.3950 - val_accuracy: 0.9469\n",
            "Epoch 39/100\n",
            "125/125 [==============================] - 23s 181ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.4472 - val_accuracy: 0.9466\n",
            "Epoch 40/100\n",
            "125/125 [==============================] - 22s 179ms/step - loss: 7.5390e-06 - accuracy: 1.0000 - val_loss: 0.4536 - val_accuracy: 0.9464\n",
            "Epoch 41/100\n",
            "125/125 [==============================] - 23s 180ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.3974 - val_accuracy: 0.9464\n",
            "Epoch 42/100\n",
            "125/125 [==============================] - 23s 181ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.4119 - val_accuracy: 0.9451\n",
            "Epoch 43/100\n",
            "125/125 [==============================] - 23s 180ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.3586 - val_accuracy: 0.9497\n",
            "Epoch 44/100\n",
            "125/125 [==============================] - 23s 182ms/step - loss: 6.9225e-04 - accuracy: 0.9998 - val_loss: 0.4377 - val_accuracy: 0.9466\n",
            "Epoch 45/100\n",
            "125/125 [==============================] - 25s 201ms/step - loss: 1.2405e-06 - accuracy: 1.0000 - val_loss: 0.4086 - val_accuracy: 0.9500\n",
            "Epoch 46/100\n",
            "125/125 [==============================] - 23s 183ms/step - loss: 1.4673e-07 - accuracy: 1.0000 - val_loss: 0.4077 - val_accuracy: 0.9504\n",
            "Epoch 47/100\n",
            "125/125 [==============================] - 24s 192ms/step - loss: 8.5228e-08 - accuracy: 1.0000 - val_loss: 0.4164 - val_accuracy: 0.9498\n",
            "Epoch 48/100\n",
            "125/125 [==============================] - 23s 182ms/step - loss: 6.2854e-08 - accuracy: 1.0000 - val_loss: 0.4172 - val_accuracy: 0.9499\n",
            "Epoch 49/100\n",
            "125/125 [==============================] - 23s 185ms/step - loss: 5.0359e-08 - accuracy: 1.0000 - val_loss: 0.4216 - val_accuracy: 0.9498\n",
            "Epoch 50/100\n",
            "125/125 [==============================] - 23s 183ms/step - loss: 4.2209e-08 - accuracy: 1.0000 - val_loss: 0.4258 - val_accuracy: 0.9497\n",
            "Epoch 51/100\n",
            "125/125 [==============================] - 23s 186ms/step - loss: 3.6418e-08 - accuracy: 1.0000 - val_loss: 0.4254 - val_accuracy: 0.9499\n",
            "Epoch 52/100\n",
            "125/125 [==============================] - 23s 183ms/step - loss: 3.2028e-08 - accuracy: 1.0000 - val_loss: 0.4276 - val_accuracy: 0.9498\n",
            "Epoch 53/100\n",
            "125/125 [==============================] - 23s 184ms/step - loss: 2.8597e-08 - accuracy: 1.0000 - val_loss: 0.4294 - val_accuracy: 0.9498\n",
            "Epoch 54/100\n",
            "125/125 [==============================] - 24s 191ms/step - loss: 2.5914e-08 - accuracy: 1.0000 - val_loss: 0.4306 - val_accuracy: 0.9498\n",
            "Epoch 55/100\n",
            "125/125 [==============================] - 29s 235ms/step - loss: 2.3569e-08 - accuracy: 1.0000 - val_loss: 0.4316 - val_accuracy: 0.9498\n",
            "Epoch 56/100\n",
            "125/125 [==============================] - 26s 212ms/step - loss: 2.1733e-08 - accuracy: 1.0000 - val_loss: 0.4313 - val_accuracy: 0.9499\n",
            "Epoch 57/100\n",
            "125/125 [==============================] - 25s 198ms/step - loss: 2.0095e-08 - accuracy: 1.0000 - val_loss: 0.4311 - val_accuracy: 0.9499\n",
            "Epoch 58/100\n",
            "125/125 [==============================] - 23s 185ms/step - loss: 1.8646e-08 - accuracy: 1.0000 - val_loss: 0.4322 - val_accuracy: 0.9498\n",
            "Epoch 59/100\n",
            "125/125 [==============================] - 23s 182ms/step - loss: 1.7405e-08 - accuracy: 1.0000 - val_loss: 0.4338 - val_accuracy: 0.9500\n",
            "Epoch 60/100\n",
            "125/125 [==============================] - 23s 185ms/step - loss: 1.6317e-08 - accuracy: 1.0000 - val_loss: 0.4330 - val_accuracy: 0.9500\n",
            "Epoch 61/100\n",
            "125/125 [==============================] - 23s 186ms/step - loss: 1.5337e-08 - accuracy: 1.0000 - val_loss: 0.4341 - val_accuracy: 0.9499\n",
            "Epoch 62/100\n",
            "125/125 [==============================] - 24s 191ms/step - loss: 1.4488e-08 - accuracy: 1.0000 - val_loss: 0.4339 - val_accuracy: 0.9499\n",
            "Epoch 63/100\n",
            "125/125 [==============================] - 24s 188ms/step - loss: 1.3789e-08 - accuracy: 1.0000 - val_loss: 0.4331 - val_accuracy: 0.9499\n",
            "Epoch 64/100\n",
            "125/125 [==============================] - 24s 190ms/step - loss: 1.3116e-08 - accuracy: 1.0000 - val_loss: 0.4333 - val_accuracy: 0.9500\n",
            "Epoch 65/100\n",
            "125/125 [==============================] - 24s 190ms/step - loss: 1.2444e-08 - accuracy: 1.0000 - val_loss: 0.4335 - val_accuracy: 0.9500\n",
            "Epoch 66/100\n",
            "125/125 [==============================] - 23s 184ms/step - loss: 1.1923e-08 - accuracy: 1.0000 - val_loss: 0.4343 - val_accuracy: 0.9499\n",
            "Epoch 67/100\n",
            "125/125 [==============================] - 23s 186ms/step - loss: 1.1381e-08 - accuracy: 1.0000 - val_loss: 0.4336 - val_accuracy: 0.9500\n",
            "Epoch 68/100\n",
            "125/125 [==============================] - 26s 205ms/step - loss: 1.0986e-08 - accuracy: 1.0000 - val_loss: 0.4345 - val_accuracy: 0.9500\n",
            "Epoch 69/100\n",
            "125/125 [==============================] - 24s 193ms/step - loss: 1.0554e-08 - accuracy: 1.0000 - val_loss: 0.4343 - val_accuracy: 0.9500\n",
            "Epoch 70/100\n",
            "125/125 [==============================] - 26s 210ms/step - loss: 1.0162e-08 - accuracy: 1.0000 - val_loss: 0.4339 - val_accuracy: 0.9501\n",
            "Epoch 71/100\n",
            "125/125 [==============================] - 24s 194ms/step - loss: 9.7938e-09 - accuracy: 1.0000 - val_loss: 0.4354 - val_accuracy: 0.9499\n",
            "Epoch 72/100\n",
            "125/125 [==============================] - 23s 187ms/step - loss: 9.4734e-09 - accuracy: 1.0000 - val_loss: 0.4356 - val_accuracy: 0.9499\n",
            "Epoch 73/100\n",
            "125/125 [==============================] - 24s 189ms/step - loss: 9.1735e-09 - accuracy: 1.0000 - val_loss: 0.4355 - val_accuracy: 0.9499\n",
            "Epoch 74/100\n",
            "125/125 [==============================] - 30s 238ms/step - loss: 8.8873e-09 - accuracy: 1.0000 - val_loss: 0.4348 - val_accuracy: 0.9499\n",
            "Epoch 75/100\n",
            "125/125 [==============================] - 24s 195ms/step - loss: 8.6048e-09 - accuracy: 1.0000 - val_loss: 0.4349 - val_accuracy: 0.9499\n",
            "Epoch 76/100\n",
            "125/125 [==============================] - 28s 221ms/step - loss: 8.3651e-09 - accuracy: 1.0000 - val_loss: 0.4350 - val_accuracy: 0.9499\n",
            "Epoch 77/100\n",
            "125/125 [==============================] - 25s 201ms/step - loss: 8.1298e-09 - accuracy: 1.0000 - val_loss: 0.4343 - val_accuracy: 0.9500\n",
            "Epoch 78/100\n",
            "125/125 [==============================] - 24s 191ms/step - loss: 7.9044e-09 - accuracy: 1.0000 - val_loss: 0.4352 - val_accuracy: 0.9499\n",
            "Epoch 79/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 7.7405e-09 - accuracy: 1.0000 - val_loss: 0.4348 - val_accuracy: 0.9500\n",
            "Epoch 80/100\n",
            "125/125 [==============================] - 23s 187ms/step - loss: 7.5381e-09 - accuracy: 1.0000 - val_loss: 0.4343 - val_accuracy: 0.9499\n",
            "Epoch 81/100\n",
            "125/125 [==============================] - 23s 184ms/step - loss: 7.3668e-09 - accuracy: 1.0000 - val_loss: 0.4346 - val_accuracy: 0.9500\n",
            "Epoch 82/100\n",
            "125/125 [==============================] - 23s 183ms/step - loss: 7.2022e-09 - accuracy: 1.0000 - val_loss: 0.4353 - val_accuracy: 0.9499\n",
            "Epoch 83/100\n",
            "125/125 [==============================] - 23s 182ms/step - loss: 7.0302e-09 - accuracy: 1.0000 - val_loss: 0.4357 - val_accuracy: 0.9499\n",
            "Epoch 84/100\n",
            "125/125 [==============================] - 23s 182ms/step - loss: 6.9185e-09 - accuracy: 1.0000 - val_loss: 0.4359 - val_accuracy: 0.9499\n",
            "Epoch 85/100\n",
            "125/125 [==============================] - 23s 182ms/step - loss: 6.7577e-09 - accuracy: 1.0000 - val_loss: 0.4360 - val_accuracy: 0.9500\n",
            "Epoch 86/100\n",
            "125/125 [==============================] - 23s 183ms/step - loss: 6.6248e-09 - accuracy: 1.0000 - val_loss: 0.4363 - val_accuracy: 0.9499\n",
            "Epoch 87/100\n",
            "125/125 [==============================] - 23s 182ms/step - loss: 6.5081e-09 - accuracy: 1.0000 - val_loss: 0.4351 - val_accuracy: 0.9500\n",
            "Epoch 88/100\n",
            "125/125 [==============================] - 23s 181ms/step - loss: 6.3963e-09 - accuracy: 1.0000 - val_loss: 0.4353 - val_accuracy: 0.9500\n",
            "Epoch 89/100\n",
            "125/125 [==============================] - 23s 181ms/step - loss: 6.2908e-09 - accuracy: 1.0000 - val_loss: 0.4344 - val_accuracy: 0.9500\n",
            "Epoch 90/100\n",
            "125/125 [==============================] - 23s 180ms/step - loss: 6.1809e-09 - accuracy: 1.0000 - val_loss: 0.4362 - val_accuracy: 0.9500\n",
            "Epoch 91/100\n",
            "125/125 [==============================] - 23s 183ms/step - loss: 6.0977e-09 - accuracy: 1.0000 - val_loss: 0.4358 - val_accuracy: 0.9499\n",
            "Epoch 92/100\n",
            "125/125 [==============================] - 23s 182ms/step - loss: 5.9959e-09 - accuracy: 1.0000 - val_loss: 0.4348 - val_accuracy: 0.9500\n",
            "Epoch 93/100\n",
            "125/125 [==============================] - 23s 181ms/step - loss: 5.9089e-09 - accuracy: 1.0000 - val_loss: 0.4345 - val_accuracy: 0.9500\n",
            "Epoch 94/100\n",
            "125/125 [==============================] - 23s 182ms/step - loss: 5.8369e-09 - accuracy: 1.0000 - val_loss: 0.4346 - val_accuracy: 0.9499\n",
            "Epoch 95/100\n",
            "125/125 [==============================] - 23s 183ms/step - loss: 5.7289e-09 - accuracy: 1.0000 - val_loss: 0.4339 - val_accuracy: 0.9500\n",
            "Epoch 96/100\n",
            "125/125 [==============================] - 23s 184ms/step - loss: 5.6519e-09 - accuracy: 1.0000 - val_loss: 0.4340 - val_accuracy: 0.9500\n",
            "Epoch 97/100\n",
            "125/125 [==============================] - 24s 189ms/step - loss: 5.5904e-09 - accuracy: 1.0000 - val_loss: 0.4337 - val_accuracy: 0.9499\n",
            "Epoch 98/100\n",
            "125/125 [==============================] - 23s 183ms/step - loss: 5.5290e-09 - accuracy: 1.0000 - val_loss: 0.4344 - val_accuracy: 0.9500\n",
            "Epoch 99/100\n",
            "125/125 [==============================] - 23s 186ms/step - loss: 5.4569e-09 - accuracy: 1.0000 - val_loss: 0.4333 - val_accuracy: 0.9500\n",
            "Epoch 100/100\n",
            "125/125 [==============================] - 23s 184ms/step - loss: 5.4153e-09 - accuracy: 1.0000 - val_loss: 0.4329 - val_accuracy: 0.9499\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f78fd7f0810>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "si2Uu3YS1P5b",
        "outputId": "2441295e-6378-4e4c-d434-9c5b3aec97bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "path = \"/content/drive/MyDrive/Deep Learning/A6/saved_model/\"\n",
        "model.save(path + \"PA6_task3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTTm0Dbx1d0-",
        "outputId": "08788c5e-7883-4b3f-e5c0-28087f5c43f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Deep Learning/A6/saved_model/PA6_task3/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Deep Learning/A6/saved_model/PA6_task3/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f7901dc26d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f78fda193d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load model for future use\n",
        "path = \"/content/drive/MyDrive/Deep Learning/A6/saved_model/\"\n",
        "model = models.load_model(\"PA6_task3\")"
      ],
      "metadata": {
        "id": "lncDqUNB8E0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dRe_NYN33DIF"
      },
      "cell_type": "markdown",
      "source": [
        "** Generating results **"
      ]
    },
    {
      "metadata": {
        "id": "1P8rBjbY2N08"
      },
      "cell_type": "markdown",
      "source": [
        "Now that you've trained the network, you need to create two smaller subnetworks so that you can use them indepedantly for predictions:\n",
        "\n",
        "1. An encoder model to give you (encoder_input) -> (model states)\n",
        "2. a decoder model to give you (model_states + start_token) -> (next character)\n",
        "\n",
        "You will have to use these as following: \n",
        "\n",
        "  1. encode input and retrieve initial decoder state\n",
        "  \n",
        "  2. run one step of decoder with this initial state and a \"start of sequence\" token as target.\n",
        "  \n",
        "  Output will be the next target token\n",
        "  \n",
        "  3. Repeat with the current target token and current states"
      ]
    },
    {
      "metadata": {
        "id": "x6GdM2mX3PPS"
      },
      "cell_type": "markdown",
      "source": [
        "The following illustration should help solidify this prediction loop better. "
      ]
    },
    {
      "metadata": {
        "id": "O7WdkUGt3B66"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "![alt text](https://blog.keras.io/img/seq2seq/seq2seq-inference.png)"
      ]
    },
    {
      "metadata": {
        "id": "MYyZCjfH4D-r"
      },
      "cell_type": "code",
      "source": [
        "# Define sampling models\n",
        "\n",
        "encoder_inputs = model.input[0]  # input_1\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_inputs = model.input[1]  # input_2\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm = model.layers[3]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model.layers[4]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, len(target_chars)))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_len_decoder_seq:\n",
        "            stop_condition = True\n",
        "        target_seq = np.zeros((1, 1, len(target_chars)))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate 5 random model outputs from inputs\n",
        "for random in range(5):\n",
        "    input_seq = encoder_input[random : random + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print(\"Input date:\", input_dates[random])\n",
        "    print(\"Decoded date in other format:\", decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mag63hND_Xny",
        "outputId": "ce4684bc-a070-44b6-dea7-67a23d73b112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input date: 14-04-1990\n",
            "Decoded date in other format: 14th of April 1990\n",
            "\n",
            "Input date: 15-04-1990\n",
            "Decoded date in other format: 15th of April 1990\n",
            "\n",
            "Input date: 16-04-1990\n",
            "Decoded date in other format: 16th of April 1990\n",
            "\n",
            "Input date: 17-04-1990\n",
            "Decoded date in other format: 17th of April 1990\n",
            "\n",
            "Input date: 18-04-1990\n",
            "Decoded date in other format: 18th of April 1990\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# date from 1987\n",
        "input_date_test = \"18-04-1987\"\n",
        "\n",
        "encoder_input_test = np.zeros( ( 1, max_len_encoder_seq, len(input_chars)) )\n",
        "print(encoder_input_test.shape)\n",
        "\n",
        "for t, char in enumerate(input_date_test):\n",
        "    encoder_input_test[0, t, input_token_index[char]] = 1.0\n",
        "\n",
        "decoded_sentence = decode_sequence(encoder_input_test)\n",
        "print(\"Input date:\", input_date_test)\n",
        "print(\"Decoded date in other format:\", decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RUf9OVa_k4R",
        "outputId": "50fa8706-fd50-4d86-9648-423fea15bed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 10, 11)\n",
            "Input date: 18-04-1987\n",
            "Decoded date in other format: 18th of April 1998\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# date from 2034\n",
        "input_date_test = \"18-04-2034\"\n",
        "\n",
        "encoder_input_test = np.zeros( ( 1, max_len_encoder_seq, len(input_chars)) )\n",
        "print(encoder_input_test.shape)\n",
        "\n",
        "for t, char in enumerate(input_date_test):\n",
        "    encoder_input_test[0, t, input_token_index[char]] = 1.0\n",
        "\n",
        "decoded_sentence = decode_sequence(encoder_input_test)\n",
        "print(\"Input date:\", input_date_test)\n",
        "print(\"Decoded date in other format:\", decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWLSH9MxBk7V",
        "outputId": "39fab484-03e0-4944-8ce6-776dd6310226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 10, 11)\n",
            "Input date: 18-04-2034\n",
            "Decoded date in other format: 18th of April 2004\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# date from 2134\n",
        "input_date_test = \"18-04-2134\"\n",
        "\n",
        "encoder_input_test = np.zeros( ( 1, max_len_encoder_seq, len(input_chars)) )\n",
        "print(encoder_input_test.shape)\n",
        "\n",
        "for t, char in enumerate(input_date_test):\n",
        "    encoder_input_test[0, t, input_token_index[char]] = 1.0\n",
        "\n",
        "decoded_sentence = decode_sequence(encoder_input_test)\n",
        "print(\"Input date:\", input_date_test)\n",
        "print(\"Decoded date in other format:\", decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0v4c4rsrBwft",
        "outputId": "c6aea639-575a-4b86-9a9c-e30a14a91d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 10, 11)\n",
            "Input date: 18-04-2134\n",
            "Decoded date in other format: 18th of April 2004\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datetime(1990, 4, 14)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4E-r8SspCRQI",
        "outputId": "e8a248c6-a7d0-4f98-faee-7c61e93f1db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.datetime(1990, 4, 14, 0, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "metadata": {
        "id": "UWuykLrK3VSc"
      },
      "cell_type": "markdown",
      "source": [
        "** Part 3 - Improving result ** \n",
        "\n",
        "Now that you've got a working model, answer the following questions. \n",
        "\n",
        "1. What does the model return for a date from 1987? Why?\n",
        "2. What about a date from 2034?\n",
        "3. Now try the same date but in year 2134, what does the model return? Why is this so?\n",
        "4. How do we fix this problem?\n",
        "\n",
        "\n",
        "Answers:\n",
        "\n",
        "1. Model returns same date from 1998\n",
        "2. Model returns same date from 2004\n",
        "3. Model returns same date from 2004\n",
        "4. In make dataset() function, dates = pd.date_range(datetime(1990, 4, 14), periods=n, normalize=True) generates the dates. datetime(1990,4,14) gives the start. So start date is 14-04-1990. when n = 10000, dataset was created such that end date was 10,000 days ahead ie 29-08-2017. Model has not seen dates beyond 29-08-2017 and before 14-04-1990. Essentially, model has not seen dates before year 1990 and after year 2017. In order so solve this problem, we can create a more inclusive dataset that covers a greater range of dates.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "D58EBaw24MKK"
      },
      "cell_type": "code",
      "source": [
        "# improve the 'generate dataset' function to overcome the limitations you've highlighted in the previous part, use your answer to (4) for this\n",
        "# code this function below"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AJXiJ7x-4Vdn"
      },
      "cell_type": "code",
      "source": [
        "def make_dataset(n):\n",
        "    dates = pd.date_range(datetime(1900, 4, 14), periods=n, freq=\"W\", normalize=True)\n",
        "    \n",
        "    x = dates.map(make_short_date).values\n",
        "    y = dates.map(make_long_date).values\n",
        "    \n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TM8bOo4d4Vam"
      },
      "cell_type": "code",
      "source": [
        "x,y = make_dataset(10000) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4zymmdFHNA-",
        "outputId": "aa09ead4-69e4-4156-b228-c185fc3efc5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['15-04-1900', '22-04-1900', '29-04-1900', ..., '18-11-2091',\n",
              "       '25-11-2091', '02-12-2091'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "metadata": {
        "id": "BnoyHGq84JZc"
      },
      "cell_type": "markdown",
      "source": [
        "What did you change in this new version of the function?\n",
        "\n",
        "How will it help improve model results for the specific data points we mentioned earlier that our model had trouble with?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# as can be seen from the data array of x, the new dataset has dates stretching \n",
        "# from 1990 till 2091."
      ],
      "metadata": {
        "id": "CpSi-s85wjpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This improvement has been achieved by using the freq option in pd.date_range\n",
        "# weekley frequency has been set by using \"W\" option."
      ],
      "metadata": {
        "id": "J4fd62EGwkC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrain the model"
      ],
      "metadata": {
        "id": "Za93GO7AgB9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Data Function"
      ],
      "metadata": {
        "id": "OD5Xtfe8g_02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_short_date(dt):\n",
        "    return dt.strftime('%d-%m-%Y')\n",
        "\n",
        "def make_long_date(dt):\n",
        "    date = dt.strftime('%d')\n",
        "    if date[-1] == '1':\n",
        "        suffix = 'st'\n",
        "    elif date[-1] == '2':\n",
        "        suffix = 'nd'\n",
        "    elif date[-1] == '3':\n",
        "        suffix = 'rd'\n",
        "    else:\n",
        "        suffix = 'th'\n",
        "    month = dt.strftime('%B')\n",
        "    year = dt.strftime('%Y')\n",
        "    \n",
        "    return date + suffix + ' of ' + month + ' ' + year\n",
        "\n",
        "def make_dataset(n):\n",
        "    dates = pd.date_range(datetime(1900, 4, 14), periods=n, freq=\"W\", normalize=True)\n",
        "    \n",
        "    x = dates.map(make_short_date).values\n",
        "    y = dates.map(make_long_date).values\n",
        "    \n",
        "    return x, y"
      ],
      "metadata": {
        "id": "1Be1opjdwkFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Dataset"
      ],
      "metadata": {
        "id": "W01dSrouhCa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 40  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 15000  # Number of samples to train on.\n",
        "dataset = make_dataset(num_samples)"
      ],
      "metadata": {
        "id": "5h8ds2Z5FCRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzk7Kl7mjFe2",
        "outputId": "600cb203-af2a-4756-e0bf-67e867e41604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['15-04-1900', '22-04-1900', '29-04-1900', ..., '16-09-2187',\n",
              "        '23-09-2187', '30-09-2187'], dtype=object),\n",
              " array(['15th of April 1900', '22th of April 1900', '29th of April 1900',\n",
              "        ..., '16th of September 2187', '23th of September 2187',\n",
              "        '30th of September 2187'], dtype=object))"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BOW character implementarion data representation"
      ],
      "metadata": {
        "id": "dQbMtvnrhD7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code help: https://keras.io/examples/nlp/lstm_seq2seq/\n",
        "# create list to contain input dates and target format of dates\n",
        "input_dates = []\n",
        "target_dates = []\n",
        "# create set (as mentioned in instructions) of individual characters\n",
        "input_chars = set()\n",
        "target_chars = set()\n",
        "\n",
        "for i in range(len(dataset[0])):\n",
        "\n",
        "    # populate input_dates list\n",
        "    input_date = dataset[0][i]\n",
        "    input_dates.append(input_date)\n",
        "\n",
        "    # populate target_dates list\n",
        "    target_date = \"\\t\" + dataset[1][i] + \"\\n\"\n",
        "    target_dates.append(target_date)\n",
        "    \n",
        "    # populate input_chars set\n",
        "    for char in input_date:\n",
        "        if char not in input_chars:\n",
        "            input_chars.add(char)\n",
        "\n",
        "    # populate target_chars set\n",
        "    for char in target_date:\n",
        "        if char not in target_chars:\n",
        "            target_chars.add(char)\n",
        "\n",
        "# make set to list and sort\n",
        "input_chars = sorted(list(input_chars))\n",
        "target_chars = sorted(list(target_chars))\n",
        "\n",
        "max_len_encoder_seq = max([len(txt) for txt in input_dates])\n",
        "max_len_decoder_seq = max([len(txt) for txt in target_dates])\n",
        "\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_chars)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_chars)])\n",
        "\n",
        "encoder_input = np.zeros( (len(input_dates), max_len_encoder_seq, len(input_chars)))\n",
        "decoder_input = np.zeros( (len(input_dates), max_len_decoder_seq, len(target_chars)))\n",
        "decoder_target = np.zeros( (len(input_dates), max_len_decoder_seq, len(target_chars)))\n",
        "\n",
        "for batch in range(len(input_dates)):\n",
        "    for t, char in enumerate(input_dates[batch]):\n",
        "        encoder_input[batch, t, input_token_index[char]] = 1.0\n",
        "    \n",
        "    for t, char in enumerate(target_dates[batch]):\n",
        "        # decoder_target is ahead of decoder_input by one timestep\n",
        "        decoder_input[batch, t, target_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            decoder_target[batch, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_input[batch, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "    decoder_target[batch, t:, target_token_index[\" \"]] = 1.0"
      ],
      "metadata": {
        "id": "HSpDUo8ZgUwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Definition"
      ],
      "metadata": {
        "id": "o65xUl9-g8c5"
      }
    },
    {
      "metadata": {
        "id": "O1zL_LlBgfZr"
      },
      "cell_type": "code",
      "source": [
        "encoder_inputs = Input(shape=(None, len(input_chars)))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "encoder_states = [state_h, state_c]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JBy34JUdgfZr"
      },
      "cell_type": "code",
      "source": [
        "# decoder\n",
        "decoder_inputs = Input(shape=(None, len(target_chars)))\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(target_chars), activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1JGKOO0_gfZr"
      },
      "cell_type": "code",
      "source": [
        "# Model definition\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit([encoder_input, decoder_input], decoder_target, batch_size=batch_size, epochs=epochs, validation_split=0.2,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_USjgaItggib",
        "outputId": "c943f489-1eb3-4950-c9c9-294a923ca7e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "188/188 [==============================] - 41s 187ms/step - loss: 0.4999 - accuracy: 0.8091 - val_loss: 0.4986 - val_accuracy: 0.7985\n",
            "Epoch 2/40\n",
            "188/188 [==============================] - 34s 182ms/step - loss: 0.4019 - accuracy: 0.8380 - val_loss: 0.4626 - val_accuracy: 0.8321\n",
            "Epoch 3/40\n",
            "188/188 [==============================] - 35s 184ms/step - loss: 0.3455 - accuracy: 0.8554 - val_loss: 0.4891 - val_accuracy: 0.8340\n",
            "Epoch 4/40\n",
            "188/188 [==============================] - 35s 184ms/step - loss: 0.3251 - accuracy: 0.8599 - val_loss: 0.4610 - val_accuracy: 0.8319\n",
            "Epoch 5/40\n",
            "188/188 [==============================] - 35s 187ms/step - loss: 0.3003 - accuracy: 0.8705 - val_loss: 0.4489 - val_accuracy: 0.8128\n",
            "Epoch 6/40\n",
            "188/188 [==============================] - 36s 189ms/step - loss: 0.2623 - accuracy: 0.8924 - val_loss: 0.4385 - val_accuracy: 0.8517\n",
            "Epoch 7/40\n",
            "188/188 [==============================] - 36s 191ms/step - loss: 0.2268 - accuracy: 0.9123 - val_loss: 0.4688 - val_accuracy: 0.8764\n",
            "Epoch 8/40\n",
            "188/188 [==============================] - 37s 194ms/step - loss: 0.1842 - accuracy: 0.9325 - val_loss: 0.3667 - val_accuracy: 0.8943\n",
            "Epoch 9/40\n",
            "188/188 [==============================] - 36s 191ms/step - loss: 0.1333 - accuracy: 0.9525 - val_loss: 0.3504 - val_accuracy: 0.8928\n",
            "Epoch 10/40\n",
            "188/188 [==============================] - 36s 189ms/step - loss: 0.0842 - accuracy: 0.9712 - val_loss: 0.3565 - val_accuracy: 0.9133\n",
            "Epoch 11/40\n",
            "188/188 [==============================] - 36s 191ms/step - loss: 0.0472 - accuracy: 0.9853 - val_loss: 0.2814 - val_accuracy: 0.9310\n",
            "Epoch 12/40\n",
            "188/188 [==============================] - 36s 191ms/step - loss: 0.0245 - accuracy: 0.9932 - val_loss: 0.2078 - val_accuracy: 0.9386\n",
            "Epoch 13/40\n",
            "188/188 [==============================] - 36s 190ms/step - loss: 0.0135 - accuracy: 0.9966 - val_loss: 0.2173 - val_accuracy: 0.9410\n",
            "Epoch 14/40\n",
            "188/188 [==============================] - 36s 191ms/step - loss: 0.0094 - accuracy: 0.9977 - val_loss: 0.2448 - val_accuracy: 0.9354\n",
            "Epoch 15/40\n",
            "188/188 [==============================] - 36s 191ms/step - loss: 0.0065 - accuracy: 0.9984 - val_loss: 0.4081 - val_accuracy: 0.9129\n",
            "Epoch 16/40\n",
            "188/188 [==============================] - 36s 193ms/step - loss: 0.0056 - accuracy: 0.9986 - val_loss: 0.2711 - val_accuracy: 0.9358\n",
            "Epoch 17/40\n",
            "188/188 [==============================] - 36s 192ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.2275 - val_accuracy: 0.9456\n",
            "Epoch 18/40\n",
            "188/188 [==============================] - 36s 190ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.2053 - val_accuracy: 0.9515\n",
            "Epoch 19/40\n",
            "188/188 [==============================] - 36s 190ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.2856 - val_accuracy: 0.9365\n",
            "Epoch 20/40\n",
            "188/188 [==============================] - 36s 190ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.2396 - val_accuracy: 0.9455\n",
            "Epoch 21/40\n",
            "188/188 [==============================] - 36s 191ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.2236 - val_accuracy: 0.9445\n",
            "Epoch 22/40\n",
            "188/188 [==============================] - 35s 188ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.2600 - val_accuracy: 0.9428\n",
            "Epoch 23/40\n",
            "188/188 [==============================] - 36s 190ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.2424 - val_accuracy: 0.9497\n",
            "Epoch 24/40\n",
            "188/188 [==============================] - 36s 191ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.2523 - val_accuracy: 0.9466\n",
            "Epoch 25/40\n",
            "188/188 [==============================] - 36s 192ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.2450 - val_accuracy: 0.9524\n",
            "Epoch 26/40\n",
            "188/188 [==============================] - 36s 193ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.2595 - val_accuracy: 0.9487\n",
            "Epoch 27/40\n",
            "188/188 [==============================] - 36s 190ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.4186 - val_accuracy: 0.9273\n",
            "Epoch 28/40\n",
            "188/188 [==============================] - 36s 190ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.2364 - val_accuracy: 0.9523\n",
            "Epoch 29/40\n",
            "188/188 [==============================] - 36s 189ms/step - loss: 4.6438e-04 - accuracy: 0.9999 - val_loss: 0.2532 - val_accuracy: 0.9505\n",
            "Epoch 30/40\n",
            "188/188 [==============================] - 36s 192ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.3248 - val_accuracy: 0.9476\n",
            "Epoch 31/40\n",
            "188/188 [==============================] - 36s 189ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.2488 - val_accuracy: 0.9499\n",
            "Epoch 32/40\n",
            "188/188 [==============================] - 36s 190ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.3385 - val_accuracy: 0.9466\n",
            "Epoch 33/40\n",
            "188/188 [==============================] - 36s 189ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.2917 - val_accuracy: 0.9483\n",
            "Epoch 34/40\n",
            "188/188 [==============================] - 35s 188ms/step - loss: 9.5686e-04 - accuracy: 0.9998 - val_loss: 0.2995 - val_accuracy: 0.9471\n",
            "Epoch 35/40\n",
            "188/188 [==============================] - 36s 191ms/step - loss: 5.1496e-04 - accuracy: 0.9999 - val_loss: 0.2786 - val_accuracy: 0.9495\n",
            "Epoch 36/40\n",
            "188/188 [==============================] - 36s 190ms/step - loss: 5.4642e-04 - accuracy: 0.9999 - val_loss: 0.2904 - val_accuracy: 0.9505\n",
            "Epoch 37/40\n",
            "188/188 [==============================] - 36s 191ms/step - loss: 4.1200e-04 - accuracy: 0.9999 - val_loss: 0.2816 - val_accuracy: 0.9521\n",
            "Epoch 38/40\n",
            "188/188 [==============================] - 35s 189ms/step - loss: 2.4527e-07 - accuracy: 1.0000 - val_loss: 0.2937 - val_accuracy: 0.9519\n",
            "Epoch 39/40\n",
            "188/188 [==============================] - 35s 188ms/step - loss: 1.1022e-07 - accuracy: 1.0000 - val_loss: 0.2957 - val_accuracy: 0.9521\n",
            "Epoch 40/40\n",
            "188/188 [==============================] - 35s 188ms/step - loss: 7.5661e-08 - accuracy: 1.0000 - val_loss: 0.2925 - val_accuracy: 0.9528\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f78fcc15650>"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate model on unknown data:"
      ],
      "metadata": {
        "id": "Z4skK3aPhggC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sampling models\n",
        "\n",
        "encoder_inputs = model.input[0]  # input_1\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_inputs = model.input[1]  # input_2\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm = model.layers[3]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model.layers[4]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, len(target_chars)))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_len_decoder_seq:\n",
        "            stop_condition = True\n",
        "        target_seq = np.zeros((1, 1, len(target_chars)))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence\n"
      ],
      "metadata": {
        "id": "4mGT4-rIiiky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_date_test = [\"10-12-1987\", \"18-04-2134\", \"05-04-2034\"] \n",
        "\n",
        "for i, input_date in enumerate(input_date_test):\n",
        "    encoder_input_test = np.zeros( ( 1, max_len_encoder_seq, len(input_chars)) )\n",
        "\n",
        "    for t, char in enumerate(input_date):\n",
        "        encoder_input_test[0, t, input_token_index[char]] = 1.0\n",
        "\n",
        "    decoded_sentence = decode_sequence(encoder_input_test)\n",
        "    print(\"Input date:\", input_date)\n",
        "    print(\"Decoded date in other format:\", decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wx1hCzOZgm28",
        "outputId": "07e7f6d9-fd85-4b66-84c6-df54edd9b18d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input date: 10-12-1987\n",
            "Decoded date in other format: 10th of December 1987\n",
            "\n",
            "Input date: 18-04-2134\n",
            "Decoded date in other format: 18th of April 2134\n",
            "\n",
            "Input date: 05-04-2034\n",
            "Decoded date in other format: 05th of April 2034\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### As seen from the output, there is no apparent problem with the retrained model. 1987, 2134, and 2034 dates have been accurately converted from one date-format to the other."
      ],
      "metadata": {
        "id": "d5fs7fwlpgS9"
      }
    }
  ]
}